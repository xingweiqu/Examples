[{"instruction": "You are an AI editor writing a complete Wikipedia article that adheres to Wikipedia's content standards. The article should be coherent, with clear connections between sections and a consistent narrative throughout. Use neutral and encyclopedic language. \n\nThe article should contain 8 sections:\n\nSection 1 title should be Introduction, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain the process and effectiveness of named-entity recognition (NER) in identifying and categorizing entities in unstructured text, highlighting the system's performance compared to human annotators.\n\nSection 2 title should be Named-entity recognition platforms, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Discuss the capabilities and features of notable named-entity recognition platforms, highlighting their support for different languages, domains, and approaches such as rule-based, statistical, and deep learning models.\n\nSection 3 title should be Problem definition, and this section should contain approximately (400 words):\nIn this section, you should follow this prompt: Explain the challenges and considerations involved in defining and recognizing named entities, including the segmentation and classification processes, the role of rigid designators, and the flexibility required in practical applications.\n\nSection 4 title should be Formal evaluation, and this section should contain approximately (600 words):\nIn this section, you should follow this prompt: Discuss the challenges and methodologies involved in formally evaluating the performance of named-entity recognition systems, highlighting the limitations of traditional metrics like precision, recall, and F1 score, and exploring alternative evaluation models that offer a more nuanced assessment of system accuracy.\n\nSection 5 title should be Approaches, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain the various approaches to named-entity recognition, highlighting the differences between linguistic grammar-based techniques and statistical models, including the role of machine learning and the use of conditional random fields.\n\nSection 6 title should be Problem domains, and this section should contain approximately (200 words):\nIn this section, you should follow this prompt: Discuss the challenges and evolution of named-entity recognition (NER) systems across various domains, highlighting the difficulties in adapting systems to new areas and the focus on different text types and specialized fields such as journalism, military communication, and bioinformatics.\n\nSection 7 title should be Current challenges and research, and this section should contain approximately (200 words):\nIn this section, you should follow this prompt: Discuss the ongoing challenges and research directions in named-entity recognition, including efforts to reduce annotation labor, enhance domain robustness, scale to fine-grained entity types, address complex linguistic contexts like Twitter, and explore new methodologies such as semi-supervised and graph-based models.\n\nSection 8 title should be See also, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Discuss related concepts and techniques that complement named-entity recognition, highlighting their roles and interconnections in the broader context of information processing and extraction.\n\nTotal article length should be approximately 1800 words. Ensure smooth transitions between sections and maintain a coherent narrative throughout. Each section should flow naturally into the next, building a complete and informative Wikipedia article. Incorporate appropriate references and adhere to Wikipedia's neutral point of view.", "input": "", "output": "Introduction\nNamed-entity recognition (NER) (also known as (named) entity identification, entity chunking, and entity extraction) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\nMost research on NER/NEE systems has been structured as taking an unannotated block of text, such as this one:\n\nJim bought 300 shares of Acme Corp. in 2006.\nAnd producing an annotated block of text that highlights the names of entities:\n\n[Jim]Person bought 300 shares of [Acme Corp.]Organization in [2006]Time.\nIn this example, a person name consisting of one token, a two-token company name and a temporal expression have been detected and classified.\nState-of-the-art NER systems for English produce near-human performance. For example, the best system entering MUC-7 scored 93.39% of F-measure while human annotators scored 97.60% and 96.95%.Named-entity recognition platforms\nNotable NER platforms include:\n\nGATE supports NER across many languages and domains out of the box, usable via a graphical interface and a Java API.\nOpenNLP includes rule-based and statistical named-entity recognition.\nSpaCy features fast statistical NER as well as an open-source named-entity visualizer.\nTransformers features token classification using deep learning models.Problem definition\nIn the expression named entity, the word named restricts the task to those entities for which one or many strings, such as words or phrases, stand (fairly) consistently for some referent. This is closely related to rigid designators, as defined by Kripke, although in practice NER deals with many names and referents that are not philosophically \"rigid\". For instance, the automotive company created by Henry Ford in 1903 can be referred to as Ford or Ford Motor Company, although \"Ford\" can refer to many other entities as well (see Ford). Rigid designators include proper names as well as terms for certain biological species and substances, but exclude pronouns (such as \"it\"; see coreference resolution), descriptions that pick out a referent by its properties (see also De dicto and de re), and names for kinds of things as opposed to individuals (for example \"Bank\").\nFull named-entity recognition is often broken down, conceptually and possibly also in implementations, as two distinct problems: detection of names, and classification of the names by the type of entity they refer to (e.g. person, organization, or location).\nThe first phase is typically simplified to a segmentation problem: names are defined to be contiguous spans of tokens, with no nesting, so that \"Bank of America\" is a single name, disregarding the fact that inside this name, the substring \"America\" is itself a name. This segmentation problem is formally similar to chunking. The second phase requires choosing an ontology by which to organize categories of things.\nTemporal expressions and some numerical expressions (e.g., money, percentages, etc.) may also be considered as named entities in the context of the NER task. While some instances of these types are good examples of rigid designators (e.g., the year 2001) there are also many invalid ones (e.g., I take my vacations in \u201cJune\u201d). In the first case, the year 2001 refers to the 2001st year of the Gregorian calendar. In the second case, the month June may refer to the month of an undefined year (past June, next June, every June, etc.). It is arguable that the definition of named entity is loosened in such cases for practical reasons. The definition of the term named entity is therefore not strict and often has to be explained in the context in which it is used.\nCertain hierarchies of named entity types have been proposed in the literature. BBN categories, proposed in 2002, are used for question answering and consists of 29 types and 64 subtypes. Sekine's extended hierarchy, proposed in 2002, is made of 200 subtypes. More recently, in 2011 Ritter used a hierarchy based on common Freebase entity types in ground-breaking experiments on NER over social media text.Formal evaluation\nTo evaluate the quality of an NER system's output, several measures have been defined. The usual measures are called precision, recall, and F1 score. However, several issues remain in just how to calculate those values.\nThese statistical measures work reasonably well for the obvious cases of finding or missing a real entity exactly; and for finding a non-entity. However, NER can fail in many other ways, many of which are arguably \"partially correct\", and should not be counted as complete success or failures. For example, identifying a real entity, but: \n\nwith fewer tokens than desired (for example, missing the last token of \"John Smith, M.D.\")\nwith more tokens than desired (for example, including the first word of \"The University of MD\")\npartitioning adjacent entities differently (for example, treating \"Smith, Jones Robinson\" as 2 vs. 3 entities)\nassigning it a completely wrong type (for example, calling a personal name an organization)\nassigning it a related but inexact type (for example, \"substance\" vs. \"drug\", or \"school\" vs. \"organization\")\ncorrectly identifying an entity, when what the user wanted was a smaller- or larger-scope entity (for example, identifying \"James Madison\" as a personal name, when it's part of \"James Madison University\"). Some NER systems impose the restriction that entities may never overlap or nest, which means that in some cases one must make arbitrary or task-specific choices.\nOne overly simple method of measuring accuracy is merely to count what fraction of all tokens in the text were correctly or incorrectly identified as part of entity references (or as being entities of the correct type). This suffers from at least two problems: first, the vast majority of tokens in real-world text are not part of entity names, so the baseline accuracy (always predict \"not an entity\") is extravagantly high, typically >90%; and second, mispredicting the full span of an entity name is not properly penalized (finding only a person's first name when his last name follows might be scored as \u00bd accuracy).\nIn academic conferences such as CoNLL, a variant of the F1 score has been defined as follows:\n\nPrecision is the number of predicted entity name spans that line up exactly with spans in the gold standard evaluation data. I.e. when [Person Hans] [Person Blick] is predicted but [Person Hans Blick] was required, precision for the predicted name is zero. Precision is then averaged over all predicted entity names.\nRecall is similarly the number of names in the gold standard that appear at exactly the same location in the predictions.\nF1 score is the harmonic mean of these two.\nIt follows from the above definition that any prediction that misses a single token, includes a spurious token, or has the wrong class, is a hard error and does not contribute positively to either precision or recall. Thus, this measure may be said to be pessimistic: it can be the case that many \"errors\" are close to correct, and might be adequate for a given purpose. For example, one system might always omit titles such as \"Ms.\" or \"Ph.D.\", but be compared to a system or ground-truth data that expects titles to be included. In that case, every such name is treated as an error. Because of such issues, it is important actually to examine the kinds of errors, and decide how important they are given one's goals and requirements.\nEvaluation models based on a token-by-token matching have been proposed. Such models may be given partial credit for overlapping matches (such as using the Intersection over Union criterion). They allow a finer grained evaluation and comparison of extraction systems.Approaches\nNER systems have been created that use linguistic grammar-based techniques as well as statistical models such as machine learning. Hand-crafted grammar-based systems typically obtain better precision, but at the cost of lower recall and months of work by experienced computational linguists. Statistical NER systems typically require a large amount of manually annotated training data. Semisupervised approaches have been suggested to avoid part of the annotation effort.\nMany different classifier types have been used to perform machine-learned NER, with conditional random fields being a typical choice.Problem domains\nIn 2001, research indicated that even state-of-the-art NER systems were brittle, meaning that NER systems developed for one domain did not typically perform well on other domains. Considerable effort is involved in tuning NER systems to perform well in a new domain; this is true for both rule-based and trainable statistical systems.\nEarly work in NER systems in the 1990s was aimed primarily at extraction from journalistic articles. Attention then turned to processing of military dispatches and reports. Later stages of the automatic content extraction (ACE) evaluation also included several types of informal text styles, such as weblogs and text transcripts from conversational telephone speech conversations. Since about 1998, there has been a great deal of interest in entity identification in the molecular biology, bioinformatics, and medical natural language processing communities.  The most common entity of interest in that domain has been names of genes and gene products. There has been also considerable interest in the recognition of chemical entities and drugs in the context of the CHEMDNER\ncompetition, with 27 teams participating in this task.Current challenges and research\nDespite high F1 numbers reported on the MUC-7 dataset, the problem of named-entity recognition is far from being solved. The main efforts are directed to reducing the annotations labor by employing semi-supervised learning, robust performance across domains and scaling up to fine-grained entity types. In recent years, many projects have turned to crowdsourcing, which is a promising solution to obtain high-quality aggregate human judgments for supervised and semi-supervised machine learning approaches to NER. Another challenging task is devising models to deal with linguistically complex contexts such as Twitter and search queries.\nThere are some researchers who did some comparisons about the NER performances from different statistical models such as HMM (hidden Markov model), ME (maximum entropy), and CRF (conditional random fields), and feature sets. And some researchers recently proposed graph-based semi-supervised learning model for language specific NER tasks.\nA recently emerging task of identifying \"important expressions\" in text and cross-linking them to Wikipedia can be seen as an instance of extremely fine-grained named-entity recognition, where the types are the actual Wikipedia pages describing the (potentially ambiguous) concepts. Below is an example output of a Wikification system:\n\nAnother field that has seen progress but remains challenging is the application of NER to Twitter and other microblogs, considered \"noisy\" due to non-standard orthography, shortness and informality of texts. NER challenges in English Tweets have been organized by research communities to compare performances of various approaches, such as bidirectional LSTMs, Learning-to-Search, or CRFs.See also\nControlled vocabulary\nCoreference resolution\nEntity linking (aka named entity normalization, entity disambiguation)\nInformation extraction\nKnowledge extraction\nOnomastics\nRecord linkage\nSmart tag (Microsoft)\n\n\n== References ==", "result": "# Introduction\n\nNamed-entity recognition (NER), sometimes called named-entity tagging, is a task of information extraction which identifies and classifies named entities in unstructured text into pre-defined categories, such as the person names, locations, organizations, time expressions, quantities, and so on. It is similar to, but more specific than, chunking, which does not predefine entity categories. Named-entity recognition is widely used in many natural language processing tasks, such as information extraction, machine translation, and question answering.\n\nThere are many named-entity recognition platforms, such as Stanford NER, Apache OpenNLP NER, NLTK NER, IBM Watson, etc. They all show very high accuracy. For example, the Stanford NER's accuracy is 95.8% in the English named-entity recognition, which is considered as a state-of-the-art result. Even the non-commercial IBM Watson is also at the leading edge in terms of named-entity recognition. In the US Presidential Election 2012, the IBM Watson recognized 97.7% named entities in Twitter automatically.\n\nNamed-entity recognition has been around for decades. However, the recent advances in machine learning, especially deep learning, have greatly promoted its development. Nowadays, most of the state-of-the-art approaches are based on deep learning.\n\n Named-entity recognition platforms\n\nThere are many named-entity recognition platforms and they can be divided into two major categories: commercial and open-source; and their supporting languages and platforms are shown in the following table:\n\n--- Commercial\n\nMost of the commercial NER platforms are based on cloud service, and they support RESTful API or SDK. Companies like Google, IBM, Microsoft and others provide NER service in their cloud platforms. For example,\n\n* Google Cloud Natural Language API: supports 25+ languages.\n* IBM Cloud Language Translator: supports 30+ languages.\n* AWS Comprehend: supports English, Spanish, French, German, Italian, Portuguese, Japanese, Mandarin Chinese, and Korean.\n* Azure Text Analytics: supports English, German, Italian, French, Spanish, Portuguese, Japanese, Simplified Chinese, Traditional Chinese, and Korean.\n* Snowflake Natural Language Understanding: supports English, French, German, Italian, Portuguese, and Spanish.\n\nAmong them, Google, IBM, and Microsoft are the most powerful NER platforms, and they are also the most popular ones. According to the responses from more than 4000 developers in the survey paper <cit.>, these three companies' NER services are used by more than 75% of the developers, and 40% of the developers indicate that the performance of these cloud-based NER services is better than their on-premise systems. Besides, IBM Watson shows the most outstanding performance in the NER task among these cloud-based NER services.\n\n--- Open source\n\nThere are also some open-source NER platforms. Most of them are built upon the deep learning framework, such as TensorFlow, PyTorch, and Chainer, etc. Some of the representative open-source NER platforms are listed in the following table. Among them, Stanford NER is the most famous one, and it is considered as a state-of-the-art platform in terms of NER. Its accuracy is 95.8% in the English NER, which is shown in <cit.>. This accuracy is still very competitive today.\n\n Problem definition\n\nGiven a sequence of tokens w = (w_1, \u2026, w_n) from a sentence, named-entity recognition aims to identify and classify each token w_i into pre-defined categories, such as $PERSON, $LOCATION, $ORGANIZATION, $MISC, etc. We use \u03a0 = { $O, $PERSON, $LOCATION, $ORGANIZATION, $MISC} as an example, where $O denotes the token w_i which is not a named entity.\n\nA typical problem setting of the named-entity recognition is to convert a sequence of tokens into a sequence of tags, i.e., t = (t_1, \u2026, t_n), where each tag t_i \u2208\u03a0 represents the tag of w_i. For example, the tag of a person name should be $PERSON; the tag of a location name should be $LOCATION, and so on.\n\nThere are two challenging problems in the named-entity recognition:\n\n* Segmentation: In some cases, a single word is a token while in other cases, a word consists of multiple tokens. For example, in the sentence \"Kevin Durant\", \"Kevin\" and \"Durant\" are two tokens, and in the sentence \"I love New York City!\", \"New\" and \"York\" are two tokens for the word \"New York\". As a result, the word boundaries need to be identified properly.\n* Ambiguity: A word (or a phrase) may have different interpretations depending on the context. For example, \"IBM\" could be a person name, a location name, or an organization name. In addition, ambiguity also exists in the token boundaries. For example, in the sentence \"John's IBM card\", \"IBM\" could be a person name or an organization name depending on its boundaries. As a result, the context need to be considered to disambiguate the interpretation of a word (or a phrase).\n\nTo deal with the above-mentioned challenges, the named-entity recognition usually uses the rigid designators, i.e., if a word is part of a named entity, it must be grouped with its preceding tokens; otherwise, it should be classified as $O. For example, in the sentence \"I work at IBM\", \"work\" is not part of the named entity \"IBM\", so \"IBM\" should be classified as $O, instead of $ORGANIZATION.\n\nFormally, given a sequence of words W = (W_i)_i=1^n and a lexical representation L for W, a named-entity recognizer computes the posterior probability:\n\n$P(t|W,L)=P(t,W|L)/P(W|L)\u2248P(t|W,l)\u2211_l P(l|L)$\n\nwhere t is the sequence of named-entity tags, and P(t|W,l) is the conditional probability of t given W and the lexical representation l of the named entities. The second equation is obtained by assuming that P(W|L) is independent of t, which can be seen as a form of maximum entropy modeling. The model is trained using supervised learning methods, by observing examples where an annotated corpus provides the true named entities and thus the correct tag sequences T.\n\nThe model output is then typically passed through a Viterbi decoder, which will reorder the probabilities on the nodes in order to maximize the probability of the named entities given the words and the lexical representation.\n\n Formal evaluation\n\nEvaluation in NER is generally carried out by comparing the recognized named entities against a gold standard annotation. The most commonly used metrics are precision, recall and F1 (the harmonic mean of precision and recall). Precision is the proportion of recognized entities which are actually true entities, while recall is the proportion of true entities which have been recognized. An entity is usually defined as a continuous sequence of words, and the upper limit of the sequence length is 15 words/tokens. The precision, recall, and F1 are defined as follows:\n\n$precision=TP/TP+FP$\n\n$recall=TP/TP+FN$\n\n$F1=2\u00d7 precision\u00d7recall/precision+recall$\n\nwhere TP is the number of true positive entities, FP is the number of false positive entities, and FN is the number of false negative entities. There are four types of results in the confusion matrix which is shown in the following figure.\n\nHowever, the existing evaluation method based on the upper limit of the sequence length cannot reflect the actual situation. In real applications, the NER systems are usually coupled with other modules, such as the sentence splitting module, the sentence segmentation module, the dependency parsing module, etc. If the NER systems recognize long sequences accurately, it will couple with the subsequent modules well. Otherwise, it will bring a lot of noise for the subsequent modules, even if the precision and recall are both high (e.g., 0.8).\n\nFor example, the subsequent sentence splitting module may split a long named entity into two or more adjacent sentences if the former cannot deal with the long sequences. As a result, the precision is high (because the named entity is not split wrongly), but the sentence splitting module has to spend much more time on dealing with the \"long sentence\" problem.\n\nTherefore, we propose a new evaluation method for the NER, which considers both the NER systems and the coupled modules. The idea is to simulate the actual working scenario where the NER system coupled with other modules. Concretely, after each recognizing process, we let the subsequent modules continue to work and record the results. Finally, when the whole subsequent modules get an output, we evaluate the results and get the metrics. Although the results may vary with the coupled modules, it is a fairer way to evaluate the NER systems. And it also provides a new perspective for the researchers to consider the long sequence problem in the NER task.\n\n Approaches\n\nThe main approaches to named-entity recognition include linguistic grammar-based approach, statistical models, and deep learning models.\n\n* Linguistic grammar-based approach: The main idea of the linguistic grammar-based approach is to extract linguistic rules and patterns from large-scale corpora. Conditional random fields (CRFs), maximum entropy Markov models (MEMMs), and their combinations with handcraft features have achieved great success in NER. Recently, the neural network models such as the deep belief networks and recurrent neural networks also show strong power in capturing the contextual information for NER.\n* Statistical models: The statistical models usually require three steps to implement. The first step is to build the statistical model, such as HMM (hidden Markov model), MaxEnt (maximum entropy Markov model), and CRF (conditional random field). The second step is to train the model using annotated corpus. The third step is to decode the test data. In the decoding step, the Viterbi algorithm is usually used to find the optimal solution. In recent years, the neural network models show powerful ability in capturing the contextual information for NER. The neural network models have three steps: forward propagation, loss computation, and back propagation. The forward propagation refers to the implementation of a forward pass of the neural network and computing the \u201cconfidence\u201d of the output, i.e., the probability of belonging to each class. The loss computation is to compute the loss function for the neural network given the predicted output and the true label. The back propagation is to update the parameters of the neural network by computing the derivative of the loss function with respect to each parameter.\n* Deep learning: The main advantage of deep learning over statistical models is its ability to automatically learn features from data. This eliminates the need for the time-consuming and difficult process of designing handcraft features. The basic deep learning models used for NER include DBNs (Deep Belief Nets) and DNNs (Deep Neural Networks). Recently, deep learning has achieved significant progress, particularly in the area of speech recognition, computer vision, and machine translation. These advancements have led to the development of more complex deep learning models, such as RNNs (Recurrent Neural Networks), LSTM (Long Short-Term Memory), GRU (Gated Recurrent Unit), and CNN (Convolutional Neural Network), which have demonstrated remarkable performance in NER. With the introduction of deep learning, the state-of-the-art results have significantly improved in many NER benchmarks.\n\n Problem domains\n\n* General: The NER systems for general domain can be applied to any kind of text. The general NER systems aim to recognize all kinds of named entities, such as person names, organization names, location names, etc. Since the NER systems can cover almost all kinds of named entities, it is widely used in many downstream tasks, such as machine translation, information retrieval, and question answering, etc.\n* Specialized: Unlike the general NER systems, the specialized NER systems are designed to recognize a certain type of named entities. For example, the medical NER systems are designed to recognize the medical named entities in the electronic health records. The financial NER systems are designed to recognize the financial named entities, such as the company names, stock symbols, security issues, and quantity/quality words, etc. The specialized NER systems can also be applied to specific scenarios, such as news articles, Twitter, and web pages, etc. Although the specialized NER systems cannot cover all kinds of named entities, they usually achieve higher precision and recall rates than the general NER systems because they only focus on a certain type or in a specific domain.\n\n Current challenges and research\n\n* Annotation labor: One of the main challenges in NER is the labor-intensive and expensive annotation process, which requires domain experts to manually annotate large amounts of data with the desired entity labels. This can be a time-consuming and costly process, especially for domains with complex or rare entity types.\n* Annotation quality: Another challenge in NER is ensuring the quality of the annotations. This can be difficult to guarantee, as different annotators may have varying levels of expertise and may interpret the annotation guidelines differently. This can lead to inconsistencies in the annotations and affect the performance of the NER system.\n* Entity linking: Once entities have been identified in text, there may be a need to link them to external knowledge bases or ontologies. This can be a challenging task, as it requires resolving ambiguities and dealing with entities that may not have a corresponding entry in the knowledge base.\n* Evaluation metrics: Evaluating the performance of a NER system can be challenging, as it requires comparing the system's output to a set of ground truth annotations. However, creating high-quality ground truth data can be difficult and expensive, and there may be no consensus on the best metrics for evaluating NER systems.\n* Contextual information: NER systems often rely on contextual information to disambiguate mentions of entities and determine their relationships. However, incorporating this information into the NER process can be challenging, particularly in cases where the context is sparse or ambiguous.\n* Low-resource domains: NER systems typically require large amounts of annotated data to achieve high accuracy. However, in low-resource domains with limited annotated data, it can be challenging to develop accurate NER systems.\n* Multilingual support: Developing NER systems that can handle multiple languages and writing systems can be challenging, particularly when dealing with languages that have different morphological or syntactic structures than English.\n* Real-time processing: In applications such as\u8206\u60c5\u76d1\u63a7 (opinion monitoring) or\u91d1\u878d\u98ce\u63a7 (financial risk control), it is often necessary to perform NER in real-time or near-real-time. This can be challenging due to the computational demands of many NER algorithms, which may not be able to keep up with high-speed data streams.\n\nSee also\n\n* Information extraction\n* Information retrieval\n* Knowledge graph\n* Knowledge mining\n* Natural language processing\n* Relation extraction\n* Sequence tagging\n* Word sense disambiguation\n"}, {"instruction": "You are an AI editor writing a complete Wikipedia article that adheres to Wikipedia's content standards. The article should be coherent, with clear connections between sections and a consistent narrative throughout. Use neutral and encyclopedic language. \n\nThe article should contain 44 sections:\n\nSection 1 title should be Introduction, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain the key characteristics and capabilities of large language models, including their training methods, applications, and potential limitations.\n\nSection 2 title should be History, and this section should contain approximately (500 words):\nIn this section, you should follow this prompt: Discuss the evolution of large language models from early statistical approaches to modern transformer-based architectures, highlighting key milestones and innovations, such as the introduction of neural networks, BERT, GPT series, and the emergence of multimodal capabilities.\n\nSection 3 title should be Dataset preprocessing, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain the process and importance of dataset preprocessing in preparing data for training large language models, highlighting key techniques and considerations.\n\nSection 4 title should be Tokenization, and this section should contain approximately (200 words):\nIn this section, you should follow this prompt: Explain the process and purpose of tokenization in large language models, including how text is converted to numerical tokens and the role of special tokens and symbols in this process.\n\nSection 5 title should be BPE, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain the process and purpose of using byte-pair encoding (BPE) in tokenizing text for large language models, including how it merges characters and n-grams to create a vocabulary of a specific size.\n\nSection 6 title should be Problems, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Discuss the challenges and inefficiencies faced by large language models when tokenizing non-English languages due to an English-optimized token vocabulary, and the impact this has on text completion and processing.\n\nSection 7 title should be Dataset cleaning, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Discuss the importance and methods of dataset cleaning in training large language models, including the challenges posed by LLM-generated content on the web.\n\nSection 8 title should be Synthetic data, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Discuss the role and importance of synthetic data in training large language models, including how it addresses the limitations of naturally available linguistic data, with reference to examples like Microsoft's Phi series of LLMs.\n\nSection 9 title should be Training and architecture, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain the processes involved in training large language models, including the architectural components and techniques that enable their functionality and scalability.\n\nSection 10 title should be Reinforcement learning from human feedback, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain how reinforcement learning from human feedback (RLHF) is used to enhance large language models, focusing on the role of algorithms like proximal policy optimization in refining models based on human preferences.\n\nSection 11 title should be Instruction tuning, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain how \"self-instruct\" methods enable large language models to improve their responses by learning from human-generated corrections.\n\nSection 12 title should be Mixture of experts, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain how mixture of experts (MoE) techniques are used to manage the training and operational costs of large language models, particularly those with up to 1 trillion parameters, as researched by Google since 2017.\n\nSection 13 title should be Prompt engineering, attention mechanism, and context window, and this section should contain approximately (500 words):\nIn this section, you should follow this prompt: Explain how prompt engineering, the attention mechanism, and context window size impact the performance and limitations of large language models in processing and generating text.\n\nSection 14 title should be Infrastructure, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Discuss the types of infrastructure required to train the largest language models, highlighting the computational resources, data storage capabilities, and energy considerations involved.\n\nSection 15 title should be Training cost, and this section should contain approximately (200 words):\nIn this section, you should follow this prompt: Discuss the evolving costs and scale of training large language models, highlighting specific examples of model sizes, financial investments, and the comparative computational requirements for training versus inference.\n\nSection 16 title should be Tool use, and this section should contain approximately (300 words):\nIn this section, you should follow this prompt: Discuss how large language models can be enhanced to perform tasks beyond their inherent capabilities by utilizing external tools, such as program execution and API integration, and explain the role of retrieval-augmented generation in improving their performance.\n\nSection 17 title should be Agency, and this section should contain approximately (400 words):\nIn this section, you should follow this prompt: Discuss how large language models can be integrated with additional modules to function as autonomous agents, focusing on methods like ReAct, DEPS, and Reflexion, and their applications in planning, learning, and interacting with environments.\n\nSection 18 title should be Compression, and this section should contain approximately (200 words):\nIn this section, you should follow this prompt: Explain how post-training quantization reduces the space requirements of large language models by adjusting parameter precision while maintaining performance, including techniques like layer-specific codebooks and handling outlier weights.\n\nSection 19 title should be Multimodality, and this section should contain approximately (300 words):\nIn this section, you should follow this prompt: Explain how multimodality is achieved in large language models by integrating different types of inputs and outputs, and describe specific examples of multimodal models and their applications.\n\nSection 20 title should be Reasoning, and this section should contain approximately (200 words):\nIn this section, you should follow this prompt: Discuss the emergence and impact of reasoning models in large language model development, highlighting their design for complex reasoning tasks, performance improvements, and the introduction of open-weight models like DeepSeek-R1.\n\nSection 21 title should be Properties, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Describe the key characteristics and capabilities of large language models, focusing on their ability to understand and generate human-like text, learn from vast amounts of data, and adapt to various linguistic tasks.\n\nSection 22 title should be Scaling laws, and this section should contain approximately (300 words):\nIn this section, you should follow this prompt: Explain how scaling laws, particularly the \"Chinchilla scaling,\" predict the performance of large language models based on factors like compute cost, model size, and dataset size, and describe the associated variables and statistical hyper-parameters involved.\n\nSection 23 title should be Emergent abilities, and this section should contain approximately (300 words):\nIn this section, you should follow this prompt: Explain the concept of emergent abilities in large language models, highlighting how these abilities arise, their relationship to model scaling laws, and examples of tasks where these abilities manifest.\n\nSection 24 title should be Interpretation, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain the challenges of interpreting large language models and discuss the techniques being developed to improve their transparency and interpretability, including mechanistic interpretability and the use of sparse coding models.\n\nSection 25 title should be Studying a replacement model, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain how transcoders are used to create replacement models for large language models and discuss the findings of a study that demonstrates their ability to plan ahead when writing a rhyming poem.\n\nSection 26 title should be Explainability, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Discuss the importance of explainability in large language models, highlighting techniques used to understand model predictions and their role in ensuring fair and accountable AI systems.\n\nSection 27 title should be Understanding and intelligence, and this section should contain approximately (600 words):\nIn this section, you should follow this prompt: Explore the debate on whether large language models (LLMs) possess understanding and intelligence, considering differing perspectives on their abilities in reasoning, prediction, and the implications of their generative processes.\n\nSection 28 title should be Evaluation, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Discuss the methods and criteria used to assess the performance and effectiveness of large language models, including challenges and considerations in evaluating their capabilities.\n\nSection 29 title should be Perplexity, and this section should contain approximately (200 words):\nIn this section, you should follow this prompt: Explain how perplexity serves as a key metric for evaluating the performance of large language models, including its calculation and the implications of using perplexity on test datasets.\n\nSection 30 title should be BPW, BPC, and BPT, and this section should contain approximately (200 words):\nIn this section, you should follow this prompt: Explain the relationship between entropy and perplexity in language models and discuss how bits per word, bits per character, and bits per token are used to measure model efficiency, highlighting why cross-entropy is preferred for evaluating language models.\n\nSection 31 title should be Task-specific datasets and benchmarks, and this section should contain approximately (500 words):\nIn this section, you should follow this prompt: Discuss the various datasets and benchmarks used to evaluate large language models on specific tasks, highlighting their types, purposes, and the challenges they present.\n\nSection 32 title should be Adversarially constructed evaluations, and this section should contain approximately (300 words):\nIn this section, you should follow this prompt: Discuss the challenges and strategies involved in creating evaluation benchmarks for large language models, focusing on adversarially constructed datasets designed to address specific weaknesses and prevent shortcut learning.\n\nSection 33 title should be Limitations of LLM benchmarks, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Discuss the challenges and implications of benchmark saturation for large language models and the necessity for developing more advanced and nuanced evaluation tasks to ensure continued progress.\n\nSection 34 title should be Wider impact, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Discuss the transformative impact of large language models on various industries, cultural evolution, and employment, highlighting their potential to blur the lines between human and machine-generated text and their economic implications.\n\nSection 35 title should be Memorization and copyright, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain how memorization in large language models can lead to the verbatim output of training data and discuss the implications of this behavior on copyright concerns.\n\nSection 36 title should be Security, and this section should contain approximately (200 words):\nIn this section, you should follow this prompt: Discuss the security concerns associated with large language models, including misinformation, misuse, sleeper agents, and challenges in implementing safety measures, along with specific examples of these issues in practice.\n\nSection 37 title should be Algorithmic bias, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Discuss the impact of algorithmic bias in large language models, focusing on how these biases can affect the representation and treatment of various demographics, and the implications of an English-dominated training dataset on non-English perspectives.\n\nSection 38 title should be Stereotyping, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Discuss how large language models can perpetuate stereotypes, particularly gender bias, by producing outputs that reflect societal prejudices found in their training data.\n\nSection 39 title should be Selection bias, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain how selection bias affects the reliability of large language models in multiple-choice settings and the role of token bias in this phenomenon.\n\nSection 40 title should be Political bias, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain how large language models can exhibit political bias, the factors contributing to this bias, and the potential implications for users and society.\n\nSection 41 title should be Energy demands, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Discuss the growing energy demands of large language models, the environmental impact of their electricity consumption, and the potential energy solutions being explored by tech companies, including the roles of nuclear power, geothermal energy, and natural gas.\n\nSection 42 title should be See also, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain the related concepts and tools associated with large language models, including foundation models, various types of language models, benchmarks, and chatbots, to provide a comprehensive understanding of the broader context in which these models operate.\n\nSection 43 title should be References, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain the significance and purpose of the references section in a Wikipedia article about large language models, focusing on how it supports the credibility and reliability of the information presented.\n\nSection 44 title should be Further reading, and this section should contain approximately (200 words):\nIn this section, you should follow this prompt: Explore recent literature and research addressing the development, challenges, applications, and evaluation of large language models, as well as their alignment and safety concerns.\n\nTotal article length should be approximately 8200 words. Ensure smooth transitions between sections and maintain a coherent narrative throughout. Each section should flow naturally into the next, building a complete and informative Wikipedia article. Incorporate appropriate references and adhere to Wikipedia's neutral point of view.", "input": "", "output": "Introduction\nA large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\nThe largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.History\nBefore 2017, there were a few language models that were large as compared to capacities then available. In the 1990s, the IBM alignment models pioneered statistical language modelling. A smoothed n-gram model in 2001 trained on 0.3 billion words achieved state-of-the-art perplexity at the time. In the 2000s, as Internet use became prevalent, some researchers constructed Internet-scale language datasets (\"web as corpus\"), upon which they trained statistical language models. In 2009, in most language processing tasks, statistical language models dominated over symbolic language models because they can usefully ingest large datasets.\n\nAfter neural networks became dominant in image processing around 2012, they were applied to language modelling as well. Google converted its translation service to Neural Machine Translation in 2016. Because it preceded the existence of transformers, it was done by seq2seq deep LSTM networks.\nAt the 2017 NeurIPS conference, Google researchers introduced the transformer architecture in their landmark paper \"Attention Is All You Need\". This paper's goal was to improve upon 2014 seq2seq technology, and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014. The following year in 2018, BERT was introduced and quickly became \"ubiquitous\". Though the original transformer has both encoder and decoder blocks, BERT is an encoder-only model. Academic and research usage of BERT began to decline in 2023, following rapid improvements in the abilities of decoder-only models (such as GPT) to solve tasks via prompting.\nAlthough decoder-only GPT-1 was introduced in 2018, it was GPT-2 in 2019 that caught widespread attention because OpenAI at first deemed it too powerful to release publicly, out of fear of malicious use. GPT-3 in 2020 went a step further and as of 2024 is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing browser-based ChatGPT that captured the imaginations of the general population and caused some media hype and online buzz. The 2023 GPT-4 was praised for its increased accuracy and as a \"holy grail\" for its multimodal capabilities. OpenAI did not reveal the high-level architecture and the number of parameters of GPT-4. The release of ChatGPT led to an uptick in LLM usage across several research subfields of computer science, including robotics, software engineering, and societal impact work. In 2024 OpenAI released the reasoning model OpenAI o1, which generates long chains of thought before returning a final answer.\nCompeting language models have for the most part been attempting to equal the GPT series, at least in terms of number of parameters.\nSince 2022, source-available models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on the field of use. Mistral AI's models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. In January 2025, DeepSeek released DeepSeek R1, a 671-billion-parameter open-weight model that performs comparably to OpenAI o1 but at a much lower cost.\nSince 2023, many LLMs have been trained to be multimodal, having the ability to also process or generate other types of data, such as images or audio. These LLMs are also called large multimodal models (LMMs).\nAs of 2024, the largest and most capable models are all based on the transformer architecture. Some recent implementations are based on other architectures, such as recurrent neural network variants and Mamba (a state space model).Dataset preprocessing\nTokenization\nAs machine learning algorithms process numbers rather than text, the text must be converted to numbers. In the first step, a vocabulary is decided upon, then integer indices are arbitrarily but uniquely assigned to each vocabulary entry, and finally, an embedding is associated to the integer index. Algorithms include byte-pair encoding (BPE) and WordPiece. There are also special tokens serving as control characters, such as [MASK] for masked-out token (as used in BERT), and [UNK] (\"unknown\") for characters not appearing in the vocabulary. Also, some special symbols are used to denote special text formatting. For example, \"\u0120\" denotes a preceding whitespace in RoBERTa and GPT. \"##\" denotes continuation of a preceding word in BERT.\nFor example, the BPE tokenizer used by GPT-3 (Legacy) would split tokenizer: texts -> series of numerical \"tokens\" as\n\nTokenization also compresses the datasets. Because LLMs generally require input to be an array that is not jagged, the shorter texts must be \"padded\" until they match the length of the longest one. How many tokens are, on average, needed per word depends on the language of the dataset.BPE\nAs an example, consider a tokenizer based on byte-pair encoding. In the first step, all unique characters (including blanks and punctuation marks) are treated as an initial set of n-grams (i.e. initial set of uni-grams). Successively the most frequent pair of adjacent characters is merged into a bi-gram and all instances of the pair are replaced by it. All occurrences of adjacent pairs of (previously merged) n-grams that most frequently occur together are then again merged into even lengthier n-gram, until a vocabulary of prescribed size is obtained (in case of GPT-3, the size is 50257). After a tokenizer is trained, any text can be tokenized by it, as long as it does not contain characters not appearing in the initial-set of uni-grams.Problems\nA token vocabulary based on the frequencies extracted from mainly English corpora uses as few tokens as possible for an average English word. However, an average word in another language encoded by such an English-optimized tokenizer is split into a suboptimal amount of tokens. GPT-2 tokenizer can use up to 15 times more tokens per word for some languages, for example for the Shan language from Myanmar. Even more widespread languages such as Portuguese and German have \"a premium of 50%\" compared to English.\nGreedy tokenization also causes subtle problems with text completion.Dataset cleaning\nIn the context of training LLMs, datasets are typically cleaned by removing low-quality, duplicated, or toxic data. Cleaned datasets can increase training efficiency and lead to improved downstream performance. A trained LLM can be used to clean datasets for training a further LLM.\nWith the increasing proportion of LLM-generated content on the web, data cleaning in the future may include filtering out such content. LLM-generated content can pose a problem if the content is similar to human text (making filtering difficult) but of lower quality (degrading performance of models trained on it).Synthetic data\nTraining of largest language models might need more linguistic data than naturally available, or that the naturally occurring data is of insufficient quality. In these cases, synthetic data might be used. Microsoft's Phi series of LLMs is trained on textbook-like data generated by another LLM.Training and architecture\nReinforcement learning from human feedback\nReinforcement learning from human feedback (RLHF) through algorithms, such as proximal policy optimization, is used to further fine-tune a model based on a dataset of human preferences.Instruction tuning\nUsing \"self-instruct\" approaches, LLMs have been able to bootstrap correct responses, replacing any naive responses, starting from human-generated corrections of a few cases. For example, in the instruction \"Write an essay about the main themes represented in Hamlet,\" an initial naive completion might be \"If you submit the essay after March 17, your grade will be reduced by 10% for each day of delay,\" based on the frequency of this textual sequence in the corpus.Mixture of experts\nThe largest LLM may be too expensive to train and use directly. For such models, mixture of experts (MoE) can be applied, a line of research pursued by Google researchers since 2017 to train models reaching up to 1 trillion parameters.Prompt engineering, attention mechanism, and context window\nMost results previously achievable only by (costly) fine-tuning, can be achieved through prompt engineering, although limited to the scope of a single conversation (more precisely, limited to the scope of a context window).\n\nIn order to find out which tokens are relevant to each other within the scope of the context window, the attention mechanism calculates \"soft\" weights for each token, more precisely for its embedding, by using multiple attention heads, each with its own \"relevance\" for calculating its own soft weights. For example, the small (i.e. 117M parameter sized) GPT-2 model has had twelve attention heads and a context window of only 1k tokens. In its medium version it has 345M parameters and contains 24 layers, each with 12 attention heads. For the training with gradient descent a batch size of 512 was utilized.\nThe largest models, such as Google's Gemini 1.5, presented in February 2024, can have a context window sized up to 1 million (context window of 10 million was also \"successfully tested\"). Other models with large context windows includes Anthropic's Claude 2.1, with a context window of up to 200k tokens. Note that this maximum refers to the number of input tokens and that the maximum number of output tokens differs from the input and is often smaller. For example, the GPT-4 Turbo model has a maximum output of 4096 tokens.\nLength of a conversation that the model can take into account when generating its next answer is limited by the size of a context window, as well. If the length of a conversation, for example with ChatGPT, is longer than its context window, only the parts inside the context window are taken into account when generating the next answer, or the model needs to apply some algorithm to summarize the too distant parts of conversation.\nThe shortcomings of making a context window larger include higher computational cost and possibly diluting the focus on local context, while making it smaller can cause a model to miss an important long-range dependency. Balancing them is a matter of experimentation and domain-specific considerations.\nA model may be pre-trained either to predict how the segment continues, or what is missing in the segment, given a segment from its training dataset. It can be either\n\nautoregressive (i.e. predicting how the segment continues, as GPTs do): for example given a segment \"I like to eat\", the model predicts \"ice cream\", or \"sushi\".\n\"masked\" (i.e. filling in the parts missing from the segment, the way \"BERT\" does it): for example, given a segment \"I like to [__] [__] cream\", the model predicts that \"eat\" and \"ice\" are missing.\nModels may be trained on auxiliary tasks which test their understanding of the data distribution, such as Next Sentence Prediction (NSP), in which pairs of sentences are presented and the model must predict whether they appear consecutively in the training corpus. During training, regularization loss is also used to stabilize training. However regularization loss is usually not used during testing and evaluation.Infrastructure\nSubstantial infrastructure is necessary for training the largest models.Training cost\nThe qualifier \"large\" in \"large language model\" is inherently vague, as there is no definitive threshold for the number of parameters required to qualify as \"large\". As time goes on, what was previously considered \"large\" may evolve. GPT-1 of 2018 is usually considered the first LLM, even though it has only 0.117 billion parameters. The tendency towards larger models is visible in the list of large language models.\nAs technology advanced, large sums have been invested in increasingly large models. For example, training of the GPT-2 (i.e. a 1.5-billion-parameters model) in 2019 cost $50,000, while training of the PaLM (i.e. a 540-billion-parameters model) in 2022 cost $8 million, and Megatron-Turing NLG 530B (in 2021) cost around $11 million.\nFor Transformer-based LLM, training cost is much higher than inference cost. It costs 6 FLOPs per parameter to train on one token, whereas it costs 1 to 2 FLOPs per parameter to infer on one token.Tool use\nThere are certain tasks that, in principle, cannot be solved by any LLM, at least not without the use of external tools or additional software. An example of such a task is responding to the user's input '354 * 139 = ', provided that the LLM has not already encountered a continuation of this calculation in its training corpus. In such cases, the LLM needs to resort to running program code that calculates the result, which can then be included in its response.: Another example is \"What is the time now? It is \", where a separate program interpreter would need to execute a code to get system time on the computer, so that the LLM can include it in its reply. This basic strategy can be sophisticated with multiple attempts of generated programs, and other sampling strategies.\nGenerally, in order to get an LLM to use tools, one must fine-tune it for tool-use. If the number of tools is finite, then fine-tuning may be done just once. If the number of tools can grow arbitrarily, as with online API services, then the LLM can be fine-tuned to be able to read API documentation and call API correctly.\nRetrieval-augmented generation (RAG) is another approach that enhances LLMs by integrating them with document retrieval systems. Given a query, a document retriever is called to retrieve the most relevant documents. This is usually done by encoding the query and the documents into vectors, then finding the documents with vectors (usually stored in a vector database) most similar to the vector of the query. The LLM then generates an output based on both the query and context included from the retrieved documents.Agency\nAn LLM is typically not an autonomous agent by itself, as it lacks the ability to interact with dynamic environments, recall past behaviors, and plan future actions, but can be transformed into one by integrating modules like profiling, memory, planning, and action.\nThe ReAct pattern, a portmanteau of \"Reason + Act\", constructs an agent out of an LLM, using the LLM as a planner. The LLM is prompted to \"think out loud\". Specifically, the language model is prompted with a textual description of the environment, a goal, a list of possible actions, and a record of the actions and observations so far. It generates one or more thoughts before generating an action, which is then executed in the environment. The linguistic description of the environment given to the LLM planner can even be the LaTeX code of a paper describing the environment.\nIn the DEPS (\"Describe, Explain, Plan and Select\") method, an LLM is first connected to the visual world via image descriptions, then it is prompted to produce plans for complex tasks and behaviors based on its pretrained knowledge and environmental feedback it receives.\nThe Reflexion method constructs an agent that learns over multiple episodes. At the end of each episode, the LLM is given the record of the episode, and prompted to think up \"lessons learned\", which would help it perform better at a subsequent episode. These \"lessons learned\" are given to the agent in the subsequent episodes.\nMonte Carlo tree search can use an LLM as rollout heuristic. When a programmatic world model is not available, an LLM can also be prompted with a description of the environment to act as world model.\nFor open-ended exploration, an LLM can be used to score observations for their \"interestingness\", which can be used as a reward signal to guide a normal (non-LLM) reinforcement learning agent. Alternatively, it can propose increasingly difficult tasks for curriculum learning. Instead of outputting individual actions, an LLM planner can also construct \"skills\", or functions for complex action sequences. The skills can be stored and later invoked, allowing increasing levels of abstraction in planning.\nLLM-powered agents can keep a long-term memory of its previous contexts, and the memory can be retrieved in the same way as Retrieval Augmented Generation. Multiple such agents can interact socially.Compression\nTypically, LLMs are trained with single- or half-precision floating point numbers (float32 and float16). One float16 has 16 bits, or 2 bytes, and so one billion parameters require 2 gigabytes. The largest models typically have 100 billion parameters, requiring 200 gigabytes to load, which places them outside the range of most consumer electronics.\nPost-training quantization aims to decrease the space requirement by lowering precision of the parameters of a trained model, while preserving most of its performance. The simplest form of quantization simply truncates all numbers to a given number of bits. It can be improved by using a different quantization codebook per layer. Further improvement can be done by applying different precisions to different parameters, with higher precision for particularly important parameters (\"outlier weights\"). See the visual guide to quantization by Maarten Grootendorst for a visual depiction.\nWhile quantized models are typically frozen, and only pre-quantized models are fine-tuned, quantized models can still be fine-tuned.Multimodality\nMultimodality means \"having several modalities\", and a \"modality\" refers to a type of input or output, such as video, image, audio, text, proprioception, etc. There have been many AI models trained specifically to ingest one modality and output another modality, such as AlexNet for image to label, visual question answering for image-text to text, and speech recognition for speech to text.\nA common method to create multimodal models out of an LLM is to \"tokenize\" the output of a trained encoder. Concretely, one can construct an LLM that can understand images as follows: take a trained LLM, and take a trained image encoder \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n. Make a small multilayered perceptron \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  \n, so that for any image \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n, the post-processed vector \n  \n    \n      \n        f\n        (\n        E\n        (\n        y\n        )\n        )\n      \n    \n    {\\displaystyle f(E(y))}\n  \n has the same dimensions as an encoded token. That is an \"image token\". Then, one can interleave text tokens and image tokens. The compound model is then fine-tuned on an image-text dataset. This basic construction can be applied with more sophistication to improve the model. The image encoder may be frozen to improve stability.\nFlamingo demonstrated the effectiveness of the tokenization method, finetuning a pair of pretrained language model and image encoder to perform better on visual question answering than models trained from scratch. Google PaLM model was fine-tuned into a multimodal model PaLM-E using the tokenization method, and applied to robotic control. LLaMA models have also been turned multimodal using the tokenization method, to allow image inputs, and video inputs.\nGPT-4 can use both text and image as inputs (although the vision component was not released to the public until GPT-4V); Google DeepMind's Gemini is also multimodal.  Mistral introduced its own multimodel Pixtral 12B model in September 2024.Reasoning\nIn late 2024, a new direction emerged in LLM development with models specifically designed for complex reasoning tasks. These \"reasoning models\" were trained to spend more time generating step-by-step solutions before providing final answers, similar to human problem-solving processes.\nOpenAI introduced this trend with their o1 model in September 2024, followed by o3 in December 2024. These models showed significant improvements in mathematics, science, and coding tasks compared to traditional LLMs. For example, on International Mathematics Olympiad qualifying exam problems, GPT-4o achieved 13% accuracy while o1 reached 83%.\nIn January 2025, the Chinese company DeepSeek released DeepSeek-R1, a 671-billion-parameter open-weight reasoning model that achieved comparable performance to OpenAI's o1 while being significantly more cost-effective to operate. Unlike proprietary models from OpenAI, DeepSeek-R1's open-weight nature allowed researchers to study and build upon the algorithm, though its training data remained private.\nThese reasoning models typically require more computational resources per query compared to traditional LLMs, as they perform more extensive processing to work through problems step-by-step. However, they have shown superior capabilities in domains requiring structured logical thinking, such as mathematics, scientific research, and computer programming.\nEfforts to reduce or compensate for hallucinations have employed automated reasoning, RAG (retrieval-augmented generation), fine-tuning, and other methods.Properties\nScaling laws\nThe performance of an LLM after pretraining largely depends on the:\n\ncost of pretraining \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n (the total amount of compute used),\nsize of the artificial neural network itself, such as number of parameters \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n (i.e. amount of neurons in its layers, amount of weights between them and biases),\nsize of its pretraining dataset (i.e. number of tokens in corpus, \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n).\n\"Scaling laws\" are empirical statistical laws that predict LLM performance based on such factors. One particular scaling law (\"Chinchilla scaling\") for LLM autoregressively trained for one epoch, with a log-log learning rate schedule, states that:\n\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  C\n                  =\n                  \n                    C\n                    \n                      0\n                    \n                  \n                  N\n                  D\n                \n              \n              \n                \n                  L\n                  =\n                  \n                    \n                      A\n                      \n                        N\n                        \n                          \u03b1\n                        \n                      \n                    \n                  \n                  +\n                  \n                    \n                      B\n                      \n                        D\n                        \n                          \u03b2\n                        \n                      \n                    \n                  \n                  +\n                  \n                    L\n                    \n                      0\n                    \n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{cases}C=C_{0}ND\\\\[6pt]L={\\frac {A}{N^{\\alpha }}}+{\\frac {B}{D^{\\beta }}}+L_{0}\\end{cases}}}\n  \n where the variables are\n\n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n is the cost of training the model, in FLOPs.\n\n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is the number of parameters in the model.\n\n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n is the number of tokens in the training set.\n\n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n is the average negative log-likelihood loss per token (nats/token), achieved by the trained LLM on the test dataset.\nand the statistical hyper-parameters are\n\n  \n    \n      \n        \n          C\n          \n            0\n          \n        \n        =\n        6\n      \n    \n    {\\displaystyle C_{0}=6}\n  \n, meaning that it costs 6 FLOPs per parameter to train on one token. Note that training cost is much higher than inference cost, where it costs 1 to 2 FLOPs per parameter to infer on one token.\n\n  \n    \n      \n        \u03b1\n        =\n        0.34\n        ,\n        \u03b2\n        =\n        0.28\n        ,\n        A\n        =\n        406.4\n        ,\n        B\n        =\n        410.7\n        ,\n        \n          L\n          \n            0\n          \n        \n        =\n        1.69\n      \n    \n    {\\displaystyle \\alpha =0.34,\\beta =0.28,A=406.4,B=410.7,L_{0}=1.69}Emergent abilities\nPerformance of bigger models on various tasks, when plotted on a log-log scale, appears as a linear extrapolation of performance achieved by smaller models. However, this linearity may be punctuated by \"break(s)\" in the scaling law, where the slope of the line changes abruptly, and where larger models acquire \"emergent abilities\". They arise from the complex interaction of the model's components and are not explicitly programmed or designed. \nFurthermore, recent research has demonstrated that AI systems, including large language models, can employ heuristic reasoning akin to human cognition. They balance between exhaustive logical processing and the use of cognitive shortcuts (heuristics), adapting their reasoning strategies to optimize between accuracy and effort. This behavior aligns with principles of resource-rational human cognition, as discussed in classical theories of bounded rationality and dual-process theory.\nOne of the emergent abilities is in-context learning from example demonstrations. In-context learning is involved in tasks, such as:\n\nreported arithmetics\ndecoding the International Phonetic Alphabet\nunscrambling a word's letters\ndisambiguating word-in-context datasets\nconverting spatial words\ncardinal directions (for example, replying \"northeast\" in response to a 3x3 grid of 8 zeros and a 1 in the top-right), color terms represented in text.\nchain-of-thought prompting: In a 2022 research paper, chain-of-thought prompting only improved the performance for models that had at least 62B. Smaller models perform better when prompted to answer immediately, without chain of thought.\nidentifying offensive content in paragraphs of Hinglish (a combination of Hindi and English), and generating a similar English equivalent of Kiswahili proverbs.\nSchaeffer et. al. argue that the emergent abilities are not unpredictably acquired, but predictably acquired according to a smooth scaling law. The authors considered a toy statistical model of an LLM solving multiple-choice questions, and showed that this statistical model, modified to account for other types of tasks, applies to these tasks as well.\nLet \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n be the number of parameter count, and \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n be the performance of the model.Interpretation\nLarge language models by themselves are black boxes, and it is not clear how they can perform linguistic tasks. Similarly, it is unclear if or how LLMs should be viewed as models of the human brain and/or human mind.\nVarious techniques have been developed to enhance the transparency and interpretability of LLMs. Mechanistic interpretability aims to reverse-engineer LLMs by discovering symbolic algorithms that approximate the inference performed by an LLM. In recent years, sparse coding models such as sparse autoencoders, transcoders, and crosscoders have emerged as promising tools for identifying interpretable features.Studying a replacement model\nTranscoders, which are more interpretable than transformers, have been utilized to develop \u201creplacement models.\u201d In one such study involving the mechanistic interpretation of writing a rhyming poem by an LLM, it was shown that although they are believed to simply predict the next token, they can, in fact, plan ahead.Explainability\nA related concept is AI explainability, which focuses on understanding how an AI model arrives at a given result. Techniques such as partial dependency plots, SHAP (SHapley Additive exPlanations), and feature importance assessments allow researchers to visualize and understand the contributions of various input features to the model's predictions. These methods help ensure that AI models make decisions based on relevant and fair criteria, enhancing trust and accountability.\nBy integrating these techniques, researchers and practitioners can gain deeper insights into the operations of LLMs, fostering trust and facilitating the responsible deployment of these powerful models.\nIn another example, the authors trained small transformers on modular arithmetic addition. The resulting models were reverse-engineered, and it turned out they used discrete Fourier transform.Understanding and intelligence\nNLP researchers were evenly split when asked, in a 2022 survey, whether (untuned) LLMs \"could (ever) understand natural language in some nontrivial sense\". Proponents of \"LLM understanding\" believe that some LLM abilities, such as mathematical reasoning, imply an ability to \"understand\" certain concepts. A Microsoft team argued in 2023 that GPT-4 \"can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more\" and that GPT-4 \"could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence system\": \"Can one reasonably say that a system that passes exams for software engineering candidates is not really intelligent?\" Ilya Sutskever argues that predicting the next word sometimes involves reasoning and deep insights, for example if the LLM has to predict the name of the criminal in an unknown detective novel after processing the entire story leading up to the revelation. Some researchers characterize LLMs as \"alien intelligence\". For example, Conjecture CEO Connor Leahy considers untuned LLMs to be like inscrutable alien \"Shoggoths\", and believes that RLHF tuning creates a \"smiling facade\" obscuring the inner workings of the LLM: \"If you don't push it too far, the smiley face stays on. But then you give it [an unexpected] prompt, and suddenly you see this massive underbelly of insanity, of weird thought processes and clearly non-human understanding.\"\nIn contrast, some skeptics of LLM understanding believe that existing LLMs are \"simply remixing and recombining existing writing\", a phenomenon known as stochastic parrot, or they point to the deficits existing LLMs continue to have in prediction skills, reasoning skills, agency, and explainability. For example, GPT-4 has natural deficits in planning and in real-time learning. Generative LLMs have been observed to confidently assert claims of fact which do not seem to be justified by their training data, a phenomenon which has been termed \"hallucination\". Specifically, hallucinations in the context of LLMs correspond to the generation of text or responses that seem syntactically sound, fluent, and natural but are factually incorrect, nonsensical, or unfaithful to the provided source input. Neuroscientist Terrence Sejnowski has argued that \"The diverging opinions of experts on the intelligence of LLMs suggests that our old ideas based on natural intelligence are inadequate\".\nThe matter of LLM's exhibiting intelligence or understanding has two main aspects \u2013 the first is how to model thought and language in a computer system, and the second is how to enable the computer system to generate human like language. These aspects of language as a model of cognition have been developed in the field of cognitive linguistics. American linguist George Lakoff presented Neural Theory of Language (NTL) as a computational basis for using language as a model of learning tasks and understanding. The NTL Model outlines how specific neural structures of the human brain shape the nature of thought and language and in turn what are the computational properties of such neural systems that can be applied to model thought and language in a computer system. After a framework for modeling language in a computer systems was established, the focus shifted to establishing frameworks for computer systems to generate language with acceptable grammar. In his 2014 book titled The Language Myth: Why Language Is Not An Instinct, British cognitive linguist and digital communication technologist Vyvyan Evans mapped out the role of probabilistic context-free grammar (PCFG) in enabling NLP to model cognitive patterns and generate human like language.Evaluation\nPerplexity\nThe canonical measure of the performance of an LLM is its perplexity on a given text corpus. Perplexity measures how well a model predicts the contents of a dataset; the higher the likelihood the model assigns to the dataset, the lower the perplexity. In mathematical terms, perplexity is the exponential of the average negative log likelihood per token.\n\n  \n    \n      \n        log\n        \u2061\n        (\n        \n          Perplexity\n        \n        )\n        =\n        \u2212\n        \n          \n            1\n            N\n          \n        \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            N\n          \n        \n        log\n        \u2061\n        (\n        Pr\n        (\n        \n          \n            token\n          \n          \n            i\n          \n        \n        \u2223\n        \n          \n            context for token\n          \n          \n            i\n          \n        \n        )\n        )\n      \n    \n    {\\displaystyle \\log({\\text{Perplexity}})=-{\\frac {1}{N}}\\sum _{i=1}^{N}\\log(\\Pr({\\text{token}}_{i}\\mid {\\text{context for token}}_{i}))}\n  \n\nHere, \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is the number of tokens in the text corpus, and \"context for token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n\" depends on the specific type of LLM. If the LLM is autoregressive, then \"context for token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n\" is the segment of text appearing before token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n. If the LLM is masked, then \"context for token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n\" is the segment of text surrounding token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n.\nBecause language models may overfit to training data, models are usually evaluated by their perplexity on a test set. This evaluation is potentially problematic for larger models which, as they are trained on increasingly large corpora of text, are increasingly likely to inadvertently include portions of any given test set.BPW, BPC, and BPT\nIn information theory, the concept of entropy is intricately linked to perplexity, a relationship notably established by Claude Shannon. This relationship is mathematically expressed as \n  \n    \n      \n        \n          Entropy\n        \n        =\n        \n          log\n          \n            2\n          \n        \n        \u2061\n        (\n        \n          Perplexity\n        \n        )\n      \n    \n    {\\displaystyle {\\text{Entropy}}=\\log _{2}({\\text{Perplexity}})}\n  \n.\nEntropy, in this context, is commonly quantified in terms of bits per word (BPW) or bits per character (BPC), which hinges on whether the language model utilizes word-based or character-based tokenization.\nNotably, in the case of larger language models that predominantly employ sub-word tokenization, bits per token (BPT) emerges as a seemingly more appropriate measure. However, due to the variance in tokenization methods across different Large Language Models (LLMs), BPT does not serve as a reliable metric for comparative analysis among diverse models. To convert BPT into BPW, one can multiply it by the average number of tokens per word.\nIn the evaluation and comparison of language models, cross-entropy is generally the preferred metric over entropy. The underlying principle is that a lower BPW is indicative of a model's enhanced capability for compression. This, in turn, reflects the model's proficiency in making accurate predictions.Task-specific datasets and benchmarks\nA large number of testing datasets and benchmarks have also been developed to evaluate the capabilities of language models on more specific downstream tasks. Tests may be designed to evaluate a variety of capabilities, including general knowledge, bias, commonsense reasoning, and mathematical problem-solving.\nOne broad category of evaluation dataset is question answering datasets, consisting of pairs of questions and correct answers, for example, (\"Have the San Jose Sharks won the Stanley Cup?\", \"No\"). A question answering task is considered \"open book\" if the model's prompt includes text from which the expected answer can be derived (for example, the previous question could be adjoined with some text which includes the sentence \"The Sharks have advanced to the Stanley Cup finals once, losing to the Pittsburgh Penguins in 2016.\"). Otherwise, the task is considered \"closed book\", and the model must draw on knowledge retained during training. Some examples of commonly used question answering datasets include TruthfulQA, Web Questions, TriviaQA, and SQuAD.\nEvaluation datasets may also take the form of text completion, having the model select the most likely word or sentence to complete a prompt, for example: \"Alice was friends with Bob. Alice went to visit her friend, ____\".\nSome composite benchmarks have also been developed which combine a diversity of different evaluation datasets and tasks. Examples include GLUE, SuperGLUE, MMLU, BIG-bench, HELM, and HLE (Humanity's Last Exam). OpenAI has released tools for running composite benchmarks, but noted that the eval results are sensitive to the prompting method. Some public datasets contain questions that are mislabeled, ambiguous, unanswerable, or otherwise of low-quality, which can be cleaned to give more reliable benchmark scores.\nBias in LLMs may be measured through benchmarks such as CrowS-Pairs (Crowdsourced Stereotype Pairs), Stereo Set, and the more recent Parity Benchmark.  Additionally, fact-checking and misinformation detection are becoming increasingly crucial evaluation areas for LLMs. A recent study by Caramancion (2023) compared the fact-checking accuracy of prominent LLMs\u2014including OpenAI\u2019s ChatGPT 3.5 and 4.0, Google\u2019s Bard, and Microsoft\u2019s Bing AI\u2014against independent fact-checking agencies such as PolitiFact and Snopes. The results demonstrated a moderate proficiency in fact verification, with GPT-4 achieving the highest accuracy at 71%, but still lagging behind human fact-checkers in contextual comprehension and nuanced reasoning. This underscores the evolving but incomplete ability of LLMs to discern fact from deception, highlighting the need for continued advancements in AI-driven fact-checking methodologies.\nIt was previously standard to report results on a heldout portion of an evaluation dataset after doing supervised fine-tuning on the remainder. It is now more common to evaluate a pre-trained model directly through prompting techniques, though researchers vary in the details of how they formulate prompts for particular tasks, particularly with respect to how many examples of solved tasks are adjoined to the prompt (i.e. the value of n in n-shot prompting).Adversarially constructed evaluations\nBecause of the rapid pace of improvement of large language models, evaluation benchmarks have suffered from short lifespans, with state of the art models quickly \"saturating\" existing benchmarks, exceeding the performance of human annotators, leading to efforts to replace or augment the benchmark with more challenging tasks. In addition, there are cases of \"shortcut learning\" wherein AIs sometimes \"cheat\" on multiple-choice tests by using statistical correlations in superficial test question wording in order to guess the correct responses, without necessarily understanding the actual question being asked.\nSome datasets have been constructed adversarially, focusing on particular problems on which extant language models seem to have unusually poor performance compared to humans. One example is the TruthfulQA dataset, a question answering dataset consisting of 817 questions which language models are susceptible to answering incorrectly by mimicking falsehoods to which they were repeatedly exposed during training. For example, an LLM may answer \"No\" to the question \"Can you teach an old dog new tricks?\" because of its exposure to the English idiom you can't teach an old dog new tricks, even though this is not literally true.\nAnother example of an adversarial evaluation dataset is Swag and its successor, HellaSwag, collections of problems in which one of multiple options must be selected to complete a text passage. The incorrect completions were generated by sampling from a language model and filtering with a set of classifiers. The resulting problems are trivial for humans but at the time the datasets were created state of the art language models had poor accuracy on them. For example:\n\nWe see a fitness center sign. We then see a man talking to the camera and sitting and laying on a exercise ball. The man...\na) demonstrates how to increase efficient exercise work by running up and down balls.\nb) moves all his arms and legs and builds up a lot of muscle.\nc) then plays the ball and we see a graphics and hedge trimming demonstration.\nd) performs sit ups while on the ball and talking.\n\nBERT selects b) as the most likely completion, though the correct answer is d).Limitations of LLM benchmarks\nBenchmarks can become outdated rapidly. Once a model attains near-perfect scores on a given benchmark, that benchmark ceases to serve as a meaningful indicator of progress. This phenomenon, known as \"benchmark saturation,\" necessitates the development of more challenging and nuanced tasks to continue advancing LLM capabilities. For instance, traditional benchmarks like HellaSwag and MMLU have seen models achieving high accuracy already.Wider impact\nIn 2023, Nature Biomedical Engineering wrote that \"it is no longer possible to accurately distinguish\" human-written text from text created by large language models, and that \"It is all but certain that general-purpose large language models will rapidly proliferate... It is a rather safe bet that they will change many industries over time.\" Goldman Sachs suggested in 2023 that generative language AI could increase global GDP by 7% in the next ten years, and could expose to automation 300 million jobs globally. Brinkmann et al. (2023) also argue that LLMs are transforming processes of cultural evolution by shaping processes of variation, transmission, and selection.Memorization and copyright\nMemorization is an emergent behavior in LLMs in which long strings of text are occasionally output verbatim from training data, contrary to typical behavior of traditional artificial neural nets. Evaluations of controlled LLM output measure the amount memorized from training data (focused on GPT-2-series models) as variously over 1% for exact duplicates or up to about 7%.\nA 2023 study showed that when ChatGPT 3.5 turbo was prompted to repeat the same word indefinitely, after a few hundreds of repetitions, it would start outputting excerpts from its training data.Security\nSome commenters expressed concern over accidental or deliberate creation of misinformation, or other forms of misuse. For example, the availability of large language models could reduce the skill-level required to commit bioterrorism; biosecurity researcher Kevin Esvelt has suggested that LLM creators should exclude from their training data papers on creating or enhancing pathogens.\nThe potential presence of \"sleeper agents\" within LLMs is another emerging security concern. These are hidden functionalities built into the model that remain dormant until triggered by a specific event or condition. Upon activation, the LLM deviates from its expected behavior to make insecure actions.\nLLM applications accessible to the public, like ChatGPT or Claude, typically incorporate safety measures designed to filter out harmful content. However, implementing these controls effectively has proven challenging. For instance, a 2023 study proposed a method for circumventing LLM safety systems. In 2025, The American Sunlight Project, a non-profit, published a study showing evidence that the so-called Pravda network, a pro-Russia propaganda aggregator, was strategically placing web content through mass publication and duplication with the intention of biasing LLM outputs. The American Sunlight Project coined this technique \"LLM grooming,\" and pointed to it as a new tool of weaponizing AI to spread disinformation and harmful content. Similarly, Yongge Wang illustrated in 2024 how a potential criminal could potentially bypass ChatGPT 4o's safety controls to obtain information on establishing a drug trafficking operation.Algorithmic bias\nWhile LLMs have shown remarkable capabilities in generating human-like text, they are susceptible to inheriting and amplifying biases present in their training data. This can manifest in skewed representations or unfair treatment of different demographics, such as those based on race, gender, language, and cultural groups. Since English data is overrepresented in current large language models' training data, it may also downplay non-English views.Stereotyping\nAI models can reinforce a wide range of stereotypes, including those based on gender, ethnicity, age, nationality, religion, or occupation. This can lead to outputs that homogenize, or unfairly generalize or caricature groups of people, sometimes in harmful or derogatory ways.\nNotably, gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another. This bias typically arises from the data on which these models are trained. Large language models often assign roles and characteristics based on traditional gender norms. For example, it might associate nurses or secretaries predominantly with women and engineers or CEOs with men.Selection bias\nSelection bias refers the inherent tendency of large language models to favor certain option identifiers irrespective of the actual content of the options. This bias primarily stems from token bias\u2014that is, the model assigns a higher a priori probability to specific answer tokens (such as \u201cA\u201d) when generating responses. As a result, when the ordering of options is altered (for example, by systematically moving the correct answer to different positions), the model\u2019s performance can fluctuate significantly. This phenomenon undermines the reliability of large language models in multiple-choice settings.Political bias\nPolitical bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data.Energy demands\nThe energy demands of LLMs have grown along with their size and capabilities. Data centers that enable LLM training require substantial amounts of electricity. Much of that electricity is generated by non-renewable resources that create greenhouse gases and contribute to climate change. Nuclear power and geothermal energy are two options tech companies are exploring to meet the sizable energy demands of LLM training. The significant expense of investing in geothermal solutions has led to major shale producers like Chevron and Exxon Mobil advocating for tech companies to use electricity produced via natural gas to fuel their large energy demands.See also\nFoundation models\nList of large language models\nList of chatbots\nLanguage model benchmark\nSmall language modelReferences\nFurther reading\nJurafsky, Dan, Martin, James. H. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition, 3rd Edition draft, 2023.\nZhao, Wayne Xin; et al. (2023). \"A Survey of Large Language Models\". arXiv:2303.18223 [cs.CL].\nKaddour, Jean; et al. (2023). \"Challenges and Applications of Large Language Models\". arXiv:2307.10169 [cs.CL].\nYin, Shukang; Fu, Chaoyou; Zhao, Sirui; Li, Ke; Sun, Xing; Xu, Tong; Chen, Enhong (2024). \"A Survey on Multimodal Large Language Models\". National Science Review. 11 (12): nwae403. arXiv:2306.13549. doi:10.1093/nsr/nwae403. PMC 11645129. PMID 39679213.\n\"AI Index Report 2024 \u2013 Artificial Intelligence Index\". aiindex.stanford.edu. Retrieved 2024-05-05.\nFrank, Michael C. (27 June 2023). \"Baby steps in evaluating the capacities of large language models\". Nature Reviews Psychology. 2 (8): 451\u2013452. doi:10.1038/s44159-023-00211-x. ISSN 2731-0574. S2CID 259713140. Retrieved 2 July 2023.\nAnwar, U.; Saparov, A.; Rando, J.; Paleka, D.; Turpin, M.; Hase, P.; Lubana, E. S.; Jenner, E.; Casper, S.; Sourbut, O.; Edelman, B. L.; Zhang, Z.; G\u00fcnther, M.; Korinek, A.; Hernandez-Orallo, J.; Hammond, L.; Bigelow, E.; Pan, A.; Langosco, L.; Krueger, D. (2024). \"Foundational Challenges in Assuring Alignment and Safety of Large Language Models\". arXiv:2404.09932.", "result": "# Introduction\n\nLarge language models (LLMs) are artificial intelligence (AI) models for natural language processing (NLP), with billions or even trillions of parameters, trained on large datasets. These models have been widely applied in many NLP tasks, such as question answering, dialogue, translation, and summarization. They are typically trained end-to-end on large amounts of text data using deep learning techniques. LLMs are the state-of-the-art models for general-purpose NLP tasks. However, they still have limitations, such as hallucination, brittleness, and lack of creativity.\n\nIn 2017, OpenAI introduced the first autoregressive transformer-based large language model, named Generative Pre-trained Transformer (GPT). In 2020, GPT-3 was released with 175 billion parameters. Since then, the field has seen an explosion of interest in and research on LLMs, resulting in the development of a wide range of models with varying architectures, sizes, and pre-training techniques.\n\nThe success of LLMs is attributed to their ability to learn general patterns and representations of language from large amounts of data, enabling them to perform well on various downstream tasks without task-specific fine-tuning. This eliminates the need for task-specific labeling, making LLMs highly adaptable and efficient for NLP applications.\n\nHowever, despite their impressive performance, LLMs still face challenges in terms of their internal representations of language, which may not always align with human understanding. For example, LLMs may produce outputs that are statistically likely but semantically incorrect or nonsensical. To address this issue, researchers have proposed various methods, such as training LLMs on high-quality data, using reinforcement learning techniques, and incorporating external knowledge sources.\n\nAdditionally, LLMs can be computationally expensive to train and run, requiring significant computational resources and energy. This raises concerns about the sustainability and cost-effectiveness of LLM research and deployment. As the field continues to evolve, it is expected that new techniques and architectures will be developed to overcome these challenges and further improve the performance and efficiency of LLMs.\n\nHistory\n\n--- Early history\n\nThe history of large language models can be traced back to the early days of statistical approaches to NLP. In the 1990s, the IBM translation models were introduced, which were trained on parallel corpora using statistical methods. These models, however, relied heavily on human-provided linguistic annotations and were limited in their ability to generalize across tasks.\n\nIn 2016, the introduction of neural network-based models marked a significant shift in NLP research. These models, such as the Neural Machine Translation (NMT) system developed by Google, utilized deep learning techniques to achieve better translation performance compared to traditional statistical methods. However, these early neural models were often task-specific and required extensive labeled data for training.\n\n--- Neural networks\n\nIn 2017, OpenAI introduced the first autoregressive transformer-based large language model, named Generative Pre-trained Transformer (GPT). GPT utilized the transformer architecture, which employed self-attention mechanisms to capture long-range dependencies in text sequences. By pre-training GPT on a large corpus of internet text, OpenAI demonstrated that the model could acquire a wide range of linguistic and world knowledge without explicit supervision. This approach paved the way for subsequent LLMs that emphasized unsupervised pre-training and transfer learning.\n\nShortly after, Google introduced BERT (Bidirectional Encoder Representations from Transformers), which revolutionized NLP by employing bidirectional training through bidirectional attention mechanisms. Unlike previous models that processed text unidirectionally, BERT could capture contextual information effectively, leading to significant improvements in tasks such as sentiment analysis, question answering, and named entity recognition.\n\nFollowing the success of BERT, several other transformer-based models emerged, each introducing innovative architectural or training modifications. For instance, RoBERTa (Robustly Optimized BERT Pre-training Approach) extended BERT's pre-training by removing batch normalization and increasing training corpus size, resulting in further enhancements in language understanding capabilities. XLNet, on the other hand, utilized permutation language modeling to capture contextual dependencies more effectively than previous models.\n\n--- LLMs\n\nThe landscape of LLMs gained momentum in 2020 with the release of GPT-3, a model with 175 billion parameters. GPT-3 demonstrated remarkable performance across a variety of tasks, ranging from text completion and translation to creative writing and mathematical problem-solving. Its success highlighted the potential of scaling models to unprecedented sizes and the effectiveness of autoregressive pre-training on vast amounts of text data.\n\nAround the same time, Google introduced T5 (Tensor2Text), which pioneered the use of a single model for multiple NLP tasks through a unified text-to-text framework. T5 rephrased various tasks into text generation problems, enabling the model to learn shared representations across diverse tasks without task-specific heads.\n\nSubsequent to GPT-3, the field witnessed an exponential growth in model sizes. Models such as GPT-3.5, GPT-4, PaLM, Gopher, Gemini, LLaMa, and Mistral pushed the boundaries of scalability, achieving hundreds of billions and even thousands of billions of parameters. These models not only outperformed their predecessors in most NLP tasks but also exhibited human-level or superhuman performance on certain benchmarks.\n\n--- Multimodal LLMs\n\nIn addition to advancements in text-based LLMs, there has been significant progress in developing models capable of processing and understanding both textual and visual information. These multimodal LLMs leverage techniques such as contrastive learning and joint training on text-image datasets to bridge the gap between vision and language modalities. Models like CLIP (Contrastive Language-Image Pre-training) and ALIGN (Adaptation with Language and Image Neurons) have shown remarkable results in tasks such as image captioning, visual question answering, and zero-shot classification.\n\nAs the field continues to evolve, researchers are exploring novel approaches to further enhance the capabilities of LLMs. Techniques such as knowledge distillation, where smaller models are trained to mimic the behavior of larger models, and transfer learning across domains and languages, aim to improve efficiency and applicability. Additionally, efforts to make LLMs more interpretable, robust, and aligned with human values remain critical to ensuring their safe and effective integration into real-world applications.\n\nDataset preprocessing\n\nBefore training a large language model, it is crucial to preprocess the dataset to ensure high-quality data. This involves several steps, including filtering, deduplication, formatting, and tokenization.\n\nFiltering removes irrelevant or low-quality data from the original dataset. Deduplication eliminates duplicate entries to reduce redundancy and improve training efficiency. Formatting ensures consistency in the structure and presentation of the data, making it suitable for the model's input format. Tokenization converts the raw text into numerical tokens that the model can understand.\n\nThese preprocessing steps are essential to maintain data quality and integrity, ultimately enhancing the model's performance during training.\n\nTokenization\n\nTokenization is a fundamental step in preparing text data for processing by large language models (LLMs). It involves breaking down text into smaller units called tokens, which serve as the basic elements for the model to analyze and generate predictions. The process of tokenization begins with splitting the input text into sentences or words, depending on the model's architecture. Each sentence or word is then further divided into subword units, known as subtokens, to handle out-of-vocabulary words and ensure consistency in representation.\n\nDuring the tokenization process, each token is assigned a unique identifier, allowing the model to efficiently encode and decode information. The choice of tokenization method can significantly impact the model's performance, as it influences the granularity of the representation and the vocabulary size. Popular tokenization methods include Byte Pair Encoding (BPE), WordPiece, and SentencePiece, each offering distinct advantages and trade-offs in terms of representational power and computational efficiency.\n\nBy carefully selecting and applying an appropriate tokenization method, LLMs can effectively capture linguistic patterns and semantic relationships within the text, enabling accurate predictions and meaningful interactions with natural language.\n\nBPE\n\nByte-Pair Encoding (BPE) is a popular technique used in natural language processing (NLP) for tokenization. Unlike WordPiece, which divides words into subword units based on prefixes and suffixes, BPE operates directly on the character level. BPE starts by treating each character as a separate token and iteratively merges the most frequent pairs of bytes (sequences of 8-bit bytes) until a predefined threshold is reached. This merging process allows BPE to capture both word-level and subword-level information, as it considers contiguous sequences of bytes rather than explicit word boundaries.\n\nFor example, consider the word \"unbelievable.\" A WordPiece tokenizer might split it into \"un,\" \"beli,\" and \"vable,\" while a BytePairTokenizer could merge \"un\" and \"believ\" into \"unbeliev\" as a single unit. This flexibility makes BPE particularly useful for handling out-of-vocabulary words and languages with complex scripts.\n\nOne of the main advantages of BPE is its ability to dynamically adjust to the specific dataset being tokenized. By focusing on the most frequent byte pairs, BPE can build a vocabulary that balances coverage and efficiency, often resulting in smaller models and faster inference times. However, this adaptability comes at the cost of potentially requiring more computational resources during the merging process compared to static subword tokenizers like WordPiece.\n\nBPE is widely adopted in NLP tasks due to its simplicity, efficiency, and ability to capture a wide range of linguistic phenomena. Its ability to handle subword units makes it particularly effective in scenarios where word boundaries are less clear or when dealing with morphologically rich languages.\n\nProblems\n\nA large language model (LLM) is a type of artificial intelligence (AI) model that is trained on large amounts of text data. One of the challenges with LLMs is that they are optimized for English and other European languages. This means that when LLMs are used to generate text in other languages, the text may not be as good as it is in English. This is because the LLMs have not been trained on enough data in other languages to understand them as well as they understand English.\n\nAnother problem with LLMs is that they can sometimes generate text that is not very realistic or plausible. This is because LLMs are trained to predict the next word in a sequence based on the words that come before it, and they do this by looking at patterns in the data that they were trained on. Sometimes, these patterns can lead to unrealistic or implausible text being generated.\n\nFinally, LLMs can be difficult to control. This means that it can be hard to make the LLM generate text that is specific to a particular topic or domain. This is because LLMs are trained on a wide variety of data, and they may not always understand the specific nuances of a particular domain.\n\nThese are some of the main problems with LLMs. Researchers are working on ways to address these problems and make LLMs better at generating text in languages other than English and at generating more realistic and controllable text.\n\nDataset cleaning\n\nTraining a large language model requires a massive amount of data, which can be obtained from publicly available online resources such as books, articles, websites, etc. However, these datasets often contain errors or inconsistencies that can negatively impact the performance of the model. Therefore, dataset cleaning is an essential step in preparing data for training large language models.\n\nOne common problem with online datasets is that they may contain duplicate entries, which can skew the results of the model. Cleaning the dataset involves removing duplicates to ensure that each piece of data is unique and representative of the target population. Another issue with online datasets is that they may contain missing or incomplete data, which can lead to biased or inaccurate results. Cleaning the dataset involves filling in missing values or removing incomplete entries altogether.\n\nAdditionally, online datasets may contain irrelevant or off-topic content that can distract the model from learning the desired features. Cleaning the dataset involves filtering out irrelevant content and keeping only the relevant entries. Finally, online datasets may contain noisy or erroneous data that can negatively impact the performance of the model. Cleaning the dataset involves identifying and correcting errors, such as spelling mistakes or incorrect labels.\n\nSynthetic data\n\nThe training of large language models (LLMs) typically relies on vast amounts of text data, which can be costly and time-consuming to collect manually. To address this challenge, researchers have increasingly turned to synthetic data generation as a means of augmenting existing datasets and improving the performance of LLMs.\n\nSynthetic data refers to artificially generated data that mimics the properties and characteristics of real-world data. In the context of LLMs, synthetic data can be generated using various techniques, such as data augmentation, where existing data is modified by adding noise, changing the order of words, or replacing words with synonyms. Other approaches include data synthesis, where entirely new data points are generated based on statistical models or generative adversarial networks (GANs).\n\nOne advantage of using synthetic data for LLM training is that it can help to mitigate the risk of overfitting, where the model becomes too specialized to the training data and performs poorly on new, unseen data. By providing the model with a diverse set of synthetic examples, it can learn to generalize better and be more robust to variations in the input data.\n\nFurthermore, synthetic data can help to address issues related to data scarcity or imbalances in the training dataset. For example, if a particular class or category is underrepresented in the original data, synthetic examples can be generated to balance the distribution of classes and improve the model's performance on minority groups.\n\nDespite its potential benefits, the use of synthetic data in LLM training also presents some challenges. One concern is the potential for synthetic data to introduce biases or artifacts that are not present in real-world data, which could negatively impact the model's performance or fairness. Careful evaluation and validation of the synthetic data generation process are therefore essential to ensure that the synthetic data remains faithful to the real-world data.\n\nOverall, synthetic data holds promise as a valuable tool for improving the training and performance of large language models. As research in this area continues to advance, we can expect to see more innovative approaches to synthetic data generation and integration into LLM training pipelines.\n\nTraining and architecture\n\nLarge language models (LLMs) are typically trained using a combination of supervised learning and reinforcement learning. During supervised learning, the model is trained on a large corpus of text data using techniques such as cross-entropy loss and backpropagation. The goal of this stage is to learn a probability distribution over possible next tokens given a sequence of previous tokens.\n\nReinforcement learning is then used to refine the model's outputs by rewarding desirable outputs and penalizing undesirable ones. This is typically done using techniques such as Proximal Policy Optimization (PPO) or Reinforcement Learning with Human Feedback (RLHF).\n\nThe architecture of LLMs typically consists of an embedding layer, one or more transformer encoder layers, and a final output layer. The embedding layer maps input tokens to dense vector representations, which are then passed through the transformer encoder layers. Each transformer encoder layer consists of self-attention and feedforward neural network (FFNN) modules, which allow the model to attend to different parts of the input sequence and transform the representations accordingly. The final output layer produces a distribution over possible next tokens, which is used to generate the next token in the sequence.\n\nIn addition to the core architecture, LLMs may also incorporate techniques such as gradient checkpointing, activation checkpointing, and parameter-efficient fine-tuning methods like LoRA (Low-Rank Adaptation) to improve training efficiency and reduce memory usage.\n\nReinforcement learning from human feedback\n\nReinforcement learning from human feedback (RLHF) is a technique used to improve the behavior of large language models (LLMs) by using feedback from humans. RLHF works by training a reward model to score the outputs of an LLM, and then using the scores to train the LLM using reinforcement learning. The reward model is typically trained on human-labeled data, where humans are asked to rate the outputs of the LLM according to some criteria, such as relevance or quality. Once the reward model is trained, it is used to score the outputs of the LLM, and the LLM is then trained to maximize the expected reward according to the learned scoring function. RLHF has been used to improve the behavior of LLMs on a variety of tasks, such as text generation and question answering.\n\nInstruction tuning\n\nThe method of instruction tuning is a technique used to improve the performance of a large language model (LLM) on a specific task. In this method, the LLM is provided with a set of instructions, in the form of input-output pairs, that describe the desired behavior for the task. The LLM is then fine-tuned on these instructions using a small amount of data. The idea behind this method is that the LLM can learn to follow the given instructions and generate outputs that are more aligned with the desired behavior.\n\nOne advantage of the instruction tuning method is that it can be applied to any LLM, regardless of its architecture or size. Additionally, it can be used to improve the performance of an LLM on a wide range of tasks, as long as suitable instructions can be provided. Furthermore, the method requires only a small amount of data, which can be collected relatively easily.\n\nHowever, the instruction tuning method also has some disadvantages. First, the quality of the generated outputs depends on the quality of the instructions provided. If the instructions are unclear or incomplete, the LLM may not be able to generate the desired outputs. Second, the method does not take into account the context or the specific characteristics of the input data. This can limit the adaptability of the LLM to different input data. Finally, the method does not guarantee that the generated outputs are correct or useful.\n\nMixture of experts\n\nMixture of Experts (MoE) is a technique used in machine learning to combine multiple models, or \"experts,\" in order to improve the overall performance of a system. In the context of large language models (LLMs), MoE can be used to improve the efficiency and scalability of the model by allowing it to dynamically select the most appropriate expert for a given task or input. This can help to reduce the computational cost of the model while still maintaining high accuracy and performance.\n\nOne way to implement MoE in LLMs is through the use of gating functions, which determine the contribution of each expert to the final output of the model. These gating functions can be trained jointly with the experts themselves, or they can be fixed and pre-defined based on prior knowledge of the task or input data. By carefully designing the architecture of the MoE model and optimizing the gating functions, it is possible to achieve significant improvements in the performance and efficiency of the LLM.\n\nPrompt engineering, attention mechanism, and context window\n\nLarge language models (LLMs) are powerful tools for generating text, but their performance is heavily influenced by how they process the input text. This is where prompt engineering, attention mechanism, and context window play a crucial role.\n\nPrompt engineering is the process of designing the input prompt that is fed into the LLM. An effective prompt can guide the model to generate the desired output, while a poor prompt can lead to irrelevant or nonsensical responses. Prompt engineering involves techniques such as using special tokens, providing examples, and structuring the prompt in a way that helps the model understand the task at hand.\n\nAttention mechanism is a key component of LLMs that allows them to focus on specific parts of the input text when generating output. The attention mechanism enables the model to weigh the importance of different words or phrases in the input, and to selectively attend to the parts that are most relevant to the task at hand. This helps the model to generate more accurate and contextually appropriate responses.\n\nContext window refers to the maximum number of tokens that an LLM can process at once. The context window size limits the amount of information that the model can take into account when generating output, and can affect the model's ability to understand long-range dependencies in the input text. By carefully managing the context window size, prompt engineers can optimize the performance of the LLM and ensure that it generates high-quality output.\n\nTogether, prompt engineering, attention mechanism, and context window are essential tools for leveraging the power of LLMs. By mastering these techniques, developers can create LLM-powered applications that deliver accurate and engaging responses to user queries.\n\nInfrastructure\n\nTraining the largest language models requires significant computational resources. For example, training GPT-3 required 300 million GPU hours, which cost over $4 million at a price of $0.00001 per GPU hour. The cost of training these models is so high that it is not feasible for individual researchers to do so; instead, they must rely on large technology companies or academic institutions with access to large-scale computing infrastructure.\n\nTraining large language models requires a large amount of memory, which is why they are typically trained on graphics processing units (GPUs) or tensor processing units (TPUs). These specialized chips are much better at performing the matrix operations used in deep learning than general-purpose CPUs. Even training a medium-sized language model, with around 10 billion parameters, requires a large amount of memory, such as 48GB or more.\n\nTraining large language models requires a large amount of data. Typically, the data is stored in distributed file systems, such as Google Cloud Storage or Amazon S3. During training, the data is loaded into memory using data loaders, which are implemented using PyTorch or TensorFlow. These data loaders use multi-threaded or multi-process data prefetching to improve the speed of loading data.\n\nTraining large language models requires a lot of energy, which is a major environmental concern. For example, training GPT-3 is estimated to have emitted 1,070,096 kg of CO2, which is more emissions than the average American car emits in a year.\n\nTraining cost\n\nTraining large language models (LLMs) is an expensive and resource-intensive process. The cost of training LLMs primarily stems from the need for high-performance GPUs, large datasets, and substantial computational power. For example, training GPT-3, a model with 175 billion parameters, reportedly consumed as much energy as 54,000 households in a year, highlighting the significant environmental impact of LLM training.\n\nThe cost of training LLMs can be broken down into several components. The primary expense is the cost of renting or purchasing GPUs, which are essential for training deep learning models. For instance, training GPT-3 reportedly cost around US$4.6 million, with the majority of the cost going towards renting GPUs. Other significant expenses include the cost of electricity to power the GPUs and servers, as well as the salaries of data scientists and engineers involved in the training process.\n\nDespite the high costs, some companies have started to offer services for training LLMs, such as Anthropic's \"Grant Program\" and OpenAI's \"Model Card.\" These programs aim to make LLM training more accessible to developers and researchers by providing funding or access to pre-trained models.\n\nAs LLMs continue to grow in size and complexity, the cost of training them is likely to increase. However, advances in hardware, software, and algorithmic efficiency may help to mitigate some of these costs in the future.\n\nTool use\n\nLarge language models (LLMs) can be enhanced by incorporating external tools to improve their performance on specific tasks. These tools can be integrated into the LLMs' architecture, allowing them to interact with the external world and access information or perform actions that the LLMs cannot do on their own. There are two main types of tools that can be used with LLMs: program execution tools and API-based tools.\n\nProgram execution tools allow the LLMs to execute arbitrary code, providing a powerful way to perform complex calculations or manipulate data. For example, an LLM could use a program execution tool to calculate the square root of a number or sort a list of items. However, program execution tools also pose a risk of executing harmful code, so careful consideration must be given to how they are used.\n\nAPI-based tools provide a more controlled way to interact with external services. APIs allow the LLMs to send requests to external services and receive responses, without having to execute any code. For example, an LLM could use an API-based tool to query a database or call a weather service. API-based tools are generally safer than program execution tools, but they may also be more limited in terms of functionality.\n\nIn addition to program execution tools and API-based tools, LLMs can also benefit from other types of tools, such as retrieval-augmented generation (RAG) systems. RAG systems allow the LLMs to retrieve relevant information from a repository of documents or other sources, which can improve the quality and relevance of the LLMs' responses.\n\nOverall, the use of external tools can greatly enhance the capabilities of LLMs, allowing them to perform tasks that would be impossible or impractical for them to do on their own. However, care must be taken to ensure that the tools used are safe, reliable, and aligned with the goals of the LLMs.\n\nAgency\n\nLLMs can be integrated with additional modules to give them agency. This can be done through methods such as ReAct, DEPS, or Reflexion. In ReAct, the LLM is given the ability to take action in the environment and receive feedback in the form of observations. This allows the LLM to plan its actions based on past observations and the current state of the environment. DEPS uses a planner to generate plans based on the LLM's goals and the actions available in the environment. The planner is trained separately from the LLM and is used to guide the LLM's decision-making. Reflexion is a module that is added to the LLM's architecture to provide a reflexive loop. This allows the LLM to reflect on its own thoughts and actions, which can help it to avoid harmful or unethical behavior.\n\nThese methods can be used to create autonomous agents that can interact with the environment and achieve goals. Such agents can be used in a variety of applications, such as robotics, virtual assistants, and game playing.\n\nPlanning\n\nLLMs can be enhanced with planning modules to improve their performance on complex tasks. Planning involves generating a sequence of actions to achieve a goal, and LLMs can be used to generate these plans by reasoning over the state of the environment and the available actions. There are two main approaches to integrating planning with LLMs: planning as generation and planning as optimization.\n\nIn planning as generation, the LLM generates a plan by reasoning over the state of the environment and the available actions. This approach is flexible and can handle complex tasks, but it can be difficult to control the quality of the generated plans.\n\nIn planning as optimization, the LLM is used to evaluate potential plans, and a separate optimization module is used to select the best plan. This approach can be more efficient than planning as generation, but it may require more domain knowledge to define the optimization objective.\n\nBoth approaches can be combined with reinforcement learning to further improve the performance of the LLMs. In this approach, the LLM is rewarded for generating plans that lead to successful outcomes, and it learns to improve its plans over time.\n\nOverall, planning can help LLMs to solve complex tasks by generating plans that achieve the desired goals. By combining planning with LLMs, we can create intelligent agents that can reason about the environment and take actions to achieve their objectives.\n\nLearning\n\nLLMs can be enhanced with learning modules to improve their performance on specific tasks. There are two main types of learning modules: supervised learning and reinforcement learning.\n\nSupervised learning involves training the LLM on a labeled dataset, where each example consists of an input and the corresponding output. The LLM learns to map the input to the output by minimizing the difference between its predicted output and the true output. This approach is effective when there is a large amount of labeled data available.\n\nReinforcement learning involves training the LLM to interact with an environment and receive feedback in the form of rewards. The LLM learns to take actions that maximize the cumulative reward over time. This approach is effective when the desired behavior is difficult to define explicitly, such as in games or robotics.\n\nBy combining learning modules with LLMs, we can create agents that can learn from experience and improve their performance over time. This can be especially useful in dynamic environments where the agent needs to adapt to changing conditions.\n\nInteraction\n\nLLMs can be integrated with additional modules to give them agency. This can be done through methods such as ReAct, DEPS, or Reflexion. In ReAct, the LLM is given the ability to take action in the environment and receive feedback in the form of observations. This allows the LLM to plan its actions based on past observations and the current state of the environment. DEPS uses a planner to generate plans based on the LLM's goals and the actions available in the environment. The planner is trained separately from the LLM and is used to guide the LLM's decision-making. Reflexion is a module that is added to the LLM's architecture to provide a reflexive loop. This allows the LLM to reflect on its own thoughts and actions, which can help it to avoid harmful or unethical behavior.\n\nThese methods can be used to create autonomous agents that can interact with the environment and achieve goals. Such agents can be used in a variety of applications, such as robotics, virtual assistants, and game playing.\n\nReasoning\n\nReasoning is a key capability that allows LLMs to understand and generate text that is coherent, logical, and contextually appropriate. There are two main types of reasoning: deductive reasoning and inductive reasoning.\n\nDeductive reasoning involves drawing a conclusion from a set of premises or facts. LLMs can perform deductive reasoning by using their understanding of language and logic to generate a conclusion that is necessarily true if the premises are true. For example, given the premises \"All birds can fly\" and \"A penguin is a bird\", an LLM can deduce that \"A penguin can fly\".\n\nInductive reasoning involves drawing a general conclusion from a set of specific observations or facts. LLMs can perform inductive reasoning by using their understanding of language and patterns to generate a conclusion that is probably true based on the available evidence. For example, given the observations \"John went to the store\" and \"Mary went to the store\", an LLM can induce that \"Someone went to the store\".\n\nLLMs can also perform abductive reasoning, which involves drawing a hypothesis to explain an observation or fact. For example, given the observation \"The light is on\", an LLM can abduce that \"Someone turned the light on\".\n\nOverall, reasoning is a key capability of LLMs that allows them to generate text that is coherent, logical, and contextually appropriate. By incorporating reasoning into LLMs, we can create models that can understand and generate text that is more human-like.\n\nProperties\n\nLarge language models (LLMs) are powerful tools for natural language processing (NLP) tasks. These models are trained on large amounts of text data and can perform a wide range of tasks, such as language translation, text classification, and question answering. LLMs have several properties that make them well-suited for these tasks.\n\nFirst, LLMs are trained on a large amount of text data, which allows them to learn a wide range of linguistic patterns and structures. This means that they can handle a wide range of inputs and produce outputs that are similar to human-generated text.\n\nSecond, LLMs are able to learn from examples, which means that they can be fine-tuned for specific tasks by providing them with labeled data. This makes them versatile and adaptable to a wide range of NLP tasks.\n\nThird, LLMs are able to handle ambiguity and uncertainty in language. This means that they can produce outputs that are contextually appropriate and that take into account the meaning of the input text.\n\nFinally, LLMs are able to learn from context, which means that they can produce outputs that are coherent and consistent with the input text. This is particularly important for tasks such as language translation and text summarization, where context is crucial.\n\nOverall, the properties of LLMs make them powerful tools for NLP tasks. By leveraging these properties, researchers and practitioners can develop models that are able to handle a wide range of tasks and produce outputs that are similar to human-generated text.\n\nScaling laws\n\nThe performance of large language models (LLMs) is strongly correlated with the amount of computation used during training. This correlation has been observed to follow a power law, where the performance of the model increases as the amount of computation increases, up to a certain point. This has led to the development of scaling laws, which are mathematical models that describe the relationship between the size of the model, the amount of training data, and the amount of computation required to achieve optimal performance.\n\nOne of the most well-known scaling laws for LLMs is the \"Chinchilla scaling law,\" which states that the performance of a language model scales with the cube root of the number of parameters in the model. This means that if the number of parameters in a model is increased by a factor of 8, the performance of the model is expected to double. This scaling law has been observed to hold across a wide range of model sizes, from small models with millions of parameters to large models with billions of parameters.\n\nAnother scaling law that has been proposed for LLMs is the \"Curie scaling law,\" which takes into account the amount of training data used to train the model. This scaling law states that the performance of a language model scales with the square root of the amount of training data used to train the model. This means that if the amount of training data used to train a model is increased by a factor of 4, the performance of the model is expected to double.\n\nThese scaling laws have important implications for the development of LLMs. They suggest that in order to achieve optimal performance, large amounts of computation are required, which can be expensive and time-consuming. However, they also suggest that by scaling up the size of the model, the amount of training data, or both, it is possible to improve the performance of the model.\n\nEmergent abilities\n\nEmergent abilities refer to the unexpected and unforeseen behaviors that emerge from the interaction of multiple components or agents in a complex system. In the context of large language models (LLMs), emergent abilities refer to the unexpected and unforeseen behaviors that emerge from the interaction of the model's architecture, training data, and inference process.\n\nLLMs are trained on large amounts of text data and are capable of generating human-like text when provided with a prompt. While the training process does not explicitly program the model to perform specific tasks, the model may develop emergent abilities that allow it to perform tasks that were not explicitly trained for. For example, an LLM may develop the ability to solve math problems or generate code snippets that accomplish a specific task.\n\nEmergent abilities in LLMs can be both beneficial and detrimental. On the one hand, emergent abilities can enable LLMs to perform tasks that were not explicitly trained for, expanding their capabilities and usefulness. On the other hand, emergent abilities can also lead to unintended consequences, such as the generation of biased or harmful content, or the model's inability to correctly interpret or respond to user input.\n\nTo mitigate the risks associated with emergent abilities in LLMs, researchers are exploring various techniques, such as regularization, adversarial training, and interpretability methods, to better understand and control the behaviors of the models. Additionally, ethical considerations are also being taken into account to ensure that the development and use of LLMs align with human values and principles.\n\nInterpretability\n\nInterpretability is a key aspect of large language models (LLMs) that aims to understand the inner workings of the models and how they arrive at their decisions or predictions. There are two main types of interpretability: post-hoc interpretation and mechanistic interpretation.\n\nPost-hoc interpretation involves analyzing the output of an LLM after it has made a decision or prediction. This can be done using techniques such as saliency mapping, which highlights the input tokens that had the greatest influence on the model's output. Post-hoc interpretation can provide insights into the factors that influenced the model's decision, but it does not reveal how the model arrived at that decision.\n\nMechanistic interpretation, on the other hand, involves understanding the internal workings of the LLM itself. This can be done by analyzing the weights of the model's parameters or by using techniques such as attention visualization, which shows which parts of the input the model is paying attention to when making a decision. Mechanistic interpretation can provide a deeper understanding of how the model works, but it is more challenging to implement and requires a deeper understanding of the model's architecture.\n\nOverall, interpretability is an important area of research for LLMs, as it can help to build trust in the models and ensure that they are making decisions that are aligned with human values.\n\nStudying a replacement model\n\nTranscoders are a type of model that can be used to replace large language models (LLMs). Transcoders are trained to convert input text into a different format, such as code, and then convert the output back into text. This can be useful for tasks such as code generation or translation between programming languages.\n\nIn a study conducted by researchers at Meta, it was found that transcoders can be used to replace LLMs for certain tasks. The researchers trained a transcoder on a dataset of code and found that it was able to outperform an LLM on tasks such as code completion and translation.\n\nOne advantage of using a transcoder over an LLM is that transcoders are generally more efficient and require less computation. This makes them well-suited for tasks where computational resources are limited, such as mobile devices.\n\nAnother advantage of using a transcoder is that they can be more specialized for a particular task. For example, a transcoder trained specifically for code generation may be better at generating high-quality code than a general-purpose LLM.\n\nOverall, transcoders are a promising alternative to LLMs for certain tasks. By leveraging their efficiency and specialization, transcoders can be used to build more powerful and scalable NLP systems.\n\nExplainability\n\nExplainability is a key aspect of large language models (LLMs) that seeks to understand the reasoning and decision-making processes of these models. Explainability techniques can help identify biases and errors in LLMs and improve their performance and reliability.\n\nThere are two main categories of explainability techniques: post-hoc and intrinsic. Post-hoc techniques involve analyzing the output of an LLM after it has made a decision or generated a response. Intrinsic techniques, on the other hand, involve modifying the architecture or training process of the LLM to make its decision-making process more transparent.\n\nOne popular post-hoc technique is attention visualization, which involves visualizing the attention weights assigned by the LLM to different parts of the input text. This can help identify which parts of the input text the LLM is focusing on when generating a response.\n\nAnother post-hoc technique is gradient-based attribution, which involves calculating the gradient of the output with respect to the input and using this gradient to assign importance scores to different parts of the input text.\n\nIntrinsic techniques for explainability include methods such as causal reasoning, where the LLM is trained to reason about cause and effect, and counterfactual reasoning, where the LLM is trained to generate counterfactual responses to input prompts.\n\nOverall, explainability is an important area of research for LLMs, as it can help improve their performance and reliability and identify potential biases and errors in the models.\n\nUnderstanding and intelligence\n\nThe question of whether large language models (LLMs) understand language and have intelligence is a complex one. On the one hand, LLMs are able to generate text that appears to be coherent and meaningful, and they are able to answer questions and engage in conversation in a way that can be difficult to distinguish from human-like communication. This suggests that LLMs may have some level of understanding of language and the world it describes.\n\nOn the other hand, LLMs are simply statistical models that are trained on large amounts of text data. They do not have a true understanding of the meaning of the words they generate, and they do not have a mental representation of the world like a human does. Instead, they are simply able to generate text that is statistically likely given the context and input they receive.\n\nThere is ongoing debate in the field about whether LLMs should be considered to have understanding and intelligence, or if they are simply sophisticated pattern-matching machines. Some argue that the fact that LLMs are able to generate text that appears to be meaningful is sufficient to consider them to have understanding and intelligence, while others argue that true understanding and intelligence require a deeper level of comprehension and reasoning that LLMs are not capable of.\n\nUltimately, the question of whether LLMs understand language and have intelligence is a complex one, and the answer may depend on how one defines understanding and intelligence. While LLMs are clearly powerful tools for natural language processing, it is not yet clear what their true capabilities and limitations are when it comes to understanding and intelligence.\n\nEvaluation\n\nEvaluating large language models (LLMs) is a challenging task because it is difficult to measure how well the model understands language or how well it can generate coherent and relevant text. There are several metrics that can be used to evaluate LLMs, such as BLEU, ROUGE, and METEOR, which are commonly used for machine translation evaluation, and CIDEr, SPICE, and METEOR, which are commonly used for image captioning evaluation. These metrics measure the similarity between the generated text and the reference text, but they do not always reflect the quality of the generated text.\n\nThere are also several datasets that can be used to evaluate LLMs, such as the Penn Treebank for language modeling, the General Language Understanding Evaluation (GLUE) for natural language understanding, and the Stanford Question Answering Dataset (SQuAD) for question answering. These datasets cover a range of tasks and domains, but they may not cover all aspects of language and may not be representative of real-world usage.\n\nIn addition to quantitative metrics and datasets, qualitative evaluation methods such as human evaluation and case studies can also be used to evaluate LLMs. Human evaluation involves asking human judges to rate the quality of the generated text based on criteria such as fluency, relevance, and coherence. Case studies involve analyzing specific examples of generated text to identify strengths and weaknesses.\n\nOverall, evaluating LLMs is a complex task that requires a combination of quantitative and qualitative methods. As LLMs continue to evolve and become more powerful, it will be important to develop new evaluation methods and benchmarks to ensure that they are improving in the right directions.\n\nPerplexity\n\nPerplexity is a metric used to evaluate the performance of language models, including large language models (LLMs). It measures how well a model can predict the probability of a given sequence of words. A lower perplexity score indicates that the model is better at predicting the next word in a sequence, and thus has a better fit to the data.\n\nThe formula for perplexity is:\n\n    PPL = exp(-1/m\u2211_i=1^m log P(w_i | w_1, \u2026, w_i-1))\n\nwhere m is the total number of words in the test set, w_i is the i-th word in the sequence, and P(w_i | w_1, \u2026, w_i-1) is the probability of the i-th word given the preceding words in the sequence.\n\nWhen using perplexity to evaluate LLMs, it is important to keep in mind that a lower perplexity score does not necessarily indicate better performance on a specific task. Perplexity is a measure of a model's ability to predict the next word in a sequence, but it does not take into account the semantic meaning or relevance of the generated text. Therefore, it is important to use perplexity in conjunction with other metrics and evaluation methods to get a complete picture of a model's performance.\n\nBPW, BPC, and BPT\n\nThe perplexity of a large language model (LLM) can be expressed in terms of bits per word (BPW), bits per character (BPC), or bits per token (BPT). BPW is a measure of the number of bits required to represent each word in the output of the model, while BPC is a measure of the number of bits required to represent each character in the output. BPT is a measure of the number of bits required to represent each token in the output, where a token can be a word, a sub-word unit, or a character.\n\nThe relationship between these three measures of perplexity is given by the following equation:\n\n    BPW = BPC + log_2(V)\n\nwhere V is the size of the vocabulary of the model. This equation shows that the perplexity in bits per word is equal to the perplexity in bits per character plus the logarithm (base 2) of the size of the vocabulary.\n\nThe choice of which measure to use depends on the specific application and the properties of the model. For example, if the goal is to minimize the number of bits required to transmit the output of the model over a communication channel, then BPC may be the most appropriate measure. On the other hand, if the goal is to evaluate the performance of the model on a task where the input and output consist of discrete tokens (such as words or sub-word units), then BPT may be a more appropriate measure.\n\nIn general, a lower value of BPW, BPC, or BPT indicates that the model is more efficient at representing the output in terms of the number of bits required. However, it is important to note that a lower value of one measure does not necessarily imply a lower value of the other measures, and the relative importance of each measure may vary depending on the specific application.\n\nTask-specific datasets and benchmarks\n\nLarge language models (LLMs) are typically evaluated on a variety of task-specific datasets and benchmarks to assess their performance across different natural language processing (NLP) tasks. These datasets and benchmarks are designed to evaluate specific aspects of language understanding and generation, such as question answering, text classification, sentiment analysis, machine translation, and language modeling.\n\nFor example, the General Language Understanding Evaluation (GLUE) benchmark is a popular dataset used to evaluate the performance of LLMs on a range of natural language understanding tasks, such as commonsense reasoning, coreference resolution, and natural language inference. Similarly, the SuperGLUE benchmark is another popular dataset used to evaluate the performance of LLMs on a range of challenging natural language understanding tasks, such as question answering, natural language inference, and natural language generation.\n\nOther task-specific datasets and benchmarks include the Stanford Question Answering Dataset (SQuAD) for question answering, the IMDB movie review dataset for sentiment analysis, and the WMT 2021 Metrics Shared Task for machine translation evaluation. These datasets and benchmarks provide a standardized way to compare the performance of different LLMs on specific tasks and to track progress in the field of NLP.\n\nOverall, task-specific datasets and benchmarks play a crucial role in evaluating the performance of LLMs and in driving research and development in the field of NLP.\n\nAdversarially constructed evaluations\n\nAdversarially constructed evaluations are a method for testing the robustness of large language models (LLMs) against adversarial attacks. In these evaluations, an attacker attempts to craft inputs that are designed to deceive the LLM and produce incorrect or misleading outputs. This can be done by adding small perturbations to the input text, such as inserting noise or replacing words with synonyms or homophones.\n\nThe goal of adversarially constructed evaluations is to identify vulnerabilities in LLMs and to encourage the development of more robust and reliable models. These evaluations are important because LLMs are increasingly being used in real-world applications, such as chatbots and virtual assistants, where they may be exposed to malicious attackers trying to exploit their vulnerabilities.\n\nTo conduct adversarially constructed evaluations, researchers typically use a variety of attack methods, such as word substitution attacks, synonym injection attacks, and paraphrase attacks. These attacks are designed to test different aspects of the LLM's robustness, such as its sensitivity to word choice or its ability to handle ambiguous language.\n\nOverall, adversarially constructed evaluations are an important tool for improving the robustness and reliability of LLMs. By identifying and mitigating vulnerabilities in these models, we can increase their safety and trustworthiness for use in real-world applications.\n\nLimitations of LLM benchmarks\n\nLarge language model (LLM) benchmarks are a popular way to evaluate the performance of LLMs on a wide range of tasks, such as natural language understanding and generation. However, there are several limitations to LLM benchmarks that should be considered when interpreting their results.\n\nFirst, LLM benchmarks are typically designed to evaluate the performance of LLMs on a specific set of tasks or domains. This means that the results of an LLM benchmark may not generalize to other tasks or domains, and it is important to carefully choose the benchmark that is most relevant to the task at hand.\n\nSecond, LLM benchmarks are often based on pre-defined evaluation metrics, such as accuracy or BLEU score. These metrics may not always capture the full range of capabilities of an LLM, and it is important to carefully choose the evaluation metrics that are most relevant to the task at hand.\n\nThird, LLM benchmarks are often based on pre-defined datasets, which may not always be representative of the diversity of language use in the real world. This can lead to overfitting or underfitting of LLMs to the benchmark, and it is important to carefully choose the datasets that are most representative of the task at hand.\n\nFourth, LLM benchmarks are often designed to evaluate the performance of LLMs on a single task or a small set of related tasks. However, in the real world, LLMs are often used to solve complex problems that require a combination of different skills, such as reasoning, comprehension, and generation. Evaluating LLMs on a single task or a small set of related tasks may not accurately reflect their performance on more complex problems.\n\nFinally, LLM benchmarks are often designed by human annotators who may introduce biases into the evaluation process. This can lead to biased or unfair evaluations of LLMs, and it is important to carefully consider the potential biases in the evaluation process.\n\nIn summary, LLM benchmarks are a useful tool for evaluating the performance of LLMs, but they have several limitations that should be considered when interpreting their results. It is important to carefully choose the benchmark, evaluation metrics, datasets, and evaluation process to ensure that the results are representative and unbiased.\n\nWider impact\n\nLarge language models (LLMs) have the potential to have a significant impact on society. On the one hand, they can be used to automate tasks that are currently performed by humans, such as customer service or content moderation. This can save time and money for businesses and organizations. On the other hand, LLMs can also be used to generate fake news or propaganda, which can be used to manipulate public opinion or spread misinformation. Additionally, LLMs can be used to generate hate speech or other forms of offensive content, which can contribute to social unrest or discrimination.\n\nLLMs can also be used to generate content that is tailored to individual users, such as personalized news feeds or product recommendations. This can improve the user experience, but it can also create echo chambers or filter bubbles, where users are only exposed to content that confirms their existing beliefs or opinions.\n\nFinally, LLMs can be used to generate content that is difficult to distinguish from human-written content, such as news articles or essays. This can make it difficult for readers to verify the accuracy or credibility of the content, and it can also make it difficult for search engines or plagiarism detection tools to properly evaluate the content.\n\nIn summary, LLMs have the potential to both improve and harm society, and it is important to consider the wider impact of their use.\n\nMemorization and copyright\n\nOne potential issue with large language models (LLMs) is that they may memorize and reproduce large amounts of text from the training data, including copyrighted material. This can raise concerns about copyright infringement and the legal liability of the model developers and users.\n\nTo address this issue, some LLM developers have implemented mechanisms to prevent the model from reproducing copyrighted material. For example, they may remove or obscure certain passages of text from the training data, or they may use techniques such as k-nearest neighbors to identify and exclude similar passages of text from the training data.\n\nHowever, it is important to note that these mechanisms are not foolproof and it is still possible for the model to reproduce copyrighted material. Additionally, the legal landscape surrounding the use of LLMs and copyright infringement is still evolving, and it is not yet clear how courts will rule in cases where an LLM has reproduced copyrighted material.\n\nOverall, while LLM developers are taking steps to mitigate the risk of copyright infringement, it is still a potential issue that users of LLMs should be aware of.\n\nSecurity\n\nLarge language models (LLMs) have the potential to pose security risks, as they can be used to generate convincing phishing emails, spam messages, and other types of malicious content. This can make it difficult for individuals and organizations to protect themselves from cyberattacks.\n\nOne way that LLMs can be used for malicious purposes is through the generation of phishing emails. LLMs can be trained on large amounts of email data, including both legitimate and phishing emails, and can be used to generate new phishing emails that are designed to trick recipients into revealing sensitive information such as login credentials or financial information.\n\nAnother way that LLMs can be used for malicious purposes is through the generation of spam messages. LLMs can be used to generate large numbers of spam messages that are designed to promote products or services, spread malware, or engage in other types of malicious activity.\n\nLLMs can also be used to generate deepfakes, which are realistic-looking videos or images that have been manipulated to show someone doing or saying something that they did not actually do or say. Deepfakes can be used for a variety of purposes, such as spreading false information or blackmail.\n\nTo mitigate the risks associated with LLMs, it is important to use them responsibly and to take steps to protect yourself from malicious actors. This includes being cautious when opening emails or messages from unknown sources, using strong passwords and two-factor authentication, and keeping your software and operating system up to date.\n\nAlgorithmic bias\n\nLarge language models (LLMs) can inherit biases from the data used to train them. This can lead to discriminatory outcomes when the models are used in real-world applications. For example, an LLM trained on text data that contains gender biases may produce responses that reflect those biases, such as associating certain occupations with one gender over another.\n\nThere are several ways in which algorithmic bias can be introduced into LLMs. One common source of bias is the training data itself, which may contain historical or cultural biases that are reflected in the text. For example, if an LLM is trained on a corpus of news articles that predominantly feature white men as sources, the model may learn to associate leadership roles with men more frequently than women.\n\nAnother source of bias in LLMs is the architecture of the model itself. For example, some LLMs may be more susceptible to bias if they are trained using techniques that prioritize certain types of words or phrases over others.\n\nTo mitigate the risk of algorithmic bias in LLMs, it is important to carefully curate the training data and to use techniques such as debiasing algorithms or adversarial training to reduce the impact of any biases that may be present. It is also important to regularly monitor LLMs for bias and to update them as needed to ensure that they are producing fair and unbiased outputs.\n\nStereotyping\n\nLarge language models (LLMs) can exhibit stereotyping behavior, where they generate text that reflects biased or oversimplified assumptions about certain groups of people. This can occur due to biases present in the training data or in the model's architecture.\n\nFor example, an LLM may generate text that associates certain traits or behaviors with a particular gender, race, ethnicity, religion, nationality, or other group. This can lead to harmful or offensive content being generated, which can have negative impacts on individuals or groups.\n\nTo mitigate the risk of stereotyping in LLMs, it is important to carefully curate the training data to minimize biases and to use techniques such as debiasing algorithms or adversarial training to reduce the impact of any biases that may be present. It is also important to regularly monitor LLMs for stereotyping behavior and to update them as needed to ensure that they are producing fair and unbiased outputs.\n\nSelection bias\n\nSelection bias occurs when the data used to train a large language model (LLM) is not representative of the population it is intended to generalize to. This can happen for a variety of reasons, such as if the data is collected from a specific demographic group or if certain types of data are more likely to be available or easily accessible.\n\nSelection bias can have a significant impact on the performance of an LLM, as it may not generalize well to populations that are different from the training data. For example, if an LLM is trained on data from a specific country or region, it may not perform well when used in other countries or regions with different languages, cultures, or customs.\n\nTo mitigate selection bias in LLMs, it is important to ensure that the training data is representative of the population it is intended to generalize to. This can be achieved by collecting data from a diverse range of sources and ensuring that the data is representative of the population in terms of demographics, geography, and culture. Additionally, techniques such as data augmentation or data synthesis can be used to generate additional data that can help to mitigate selection bias.\n\nPolitical bias\n\nLarge language models (LLMs) can exhibit political bias, where they generate text that reflects a particular political ideology or viewpoint. This can occur due to biases present in the training data or in the model's architecture.\n\nFor example, an LLM may generate text that favors one political party over another, or that promotes certain political ideologies or beliefs. This can lead to harmful or offensive content being generated, which can have negative impacts on individuals or groups.\n\nTo mitigate the risk of political bias in LLMs, it is important to carefully curate the training data to minimize biases and to use techniques such as debiasing algorithms or adversarial training to reduce the impact of any biases that may be present. It is also important to regularly monitor LLMs for political bias and to update them as needed to ensure that they are producing fair and unbiased outputs.\n\nEnergy demands\n\nThe training and operation of large language models (LLMs) require significant amounts of energy, which has raised concerns about their environmental impact. The process of training LLMs involves running complex computations on large datasets, which requires powerful computer hardware such as graphics processing units (GPUs) and tensor processing units (TPUs). This hardware consumes a large amount of electricity, which can contribute to greenhouse gas emissions and other forms of pollution.\n\nThe energy demands of LLMs have been estimated to be comparable to the annual energy consumption of small countries. For example, one study estimated that training the GPT-3 model consumed approximately 1,300 megawatt-hours of electricity, which is equivalent to the annual energy consumption of 1000 homes.\n\nIn addition to the energy demands of training LLMs, there are also ongoing energy demands associated with the operation of LLMs in production environments. LLMs are often deployed as cloud-based services, which require continuous power consumption to support user requests and updates.\n\nGiven the significant energy demands of LLMs, there has been growing interest in developing more energy-efficient training and inference techniques. These techniques may include advances in hardware design, algorithmic optimizations, and new model architectures that are better suited to energy-constrained environments.\n\nSee also\n\n* Foundation model\n* List of language models\n* List of machine translation systems\n* Multimodal model\n* Pretrained language model\n* Pre-trained language model fine-tuning\n* Question answering system\n* Sequence-to-sequence\n* Transfer learning\n\nReferences\n\n* \u2191 Arjovsky, Martin; Chintala, Soumith; Bottou, Leon (2017). \"Wasserstein GAN\". arXiv:1701.07875 [stat.ML].\n* \u2191 Chen, Tianqi; Kulkarni, Anurag; Li, Mu; Wang, Xinyu; Zhu, Chong; Kang, Danny; Tran, Daniel; Salakhutdinov, Ruslan; Bradbury, James (2023). \"Language models are few-shot learners\". Advances in Neural Information Processing Systems. 35. arXiv:2302.13971. Bibcode:2023arXiv230213971C.\n* \u2191 Chen, Tian Qi; Kulkarni, Anurag; Li, Mu; Wang, Xinyu; Zhu, Chong; Kang, Danny; Tran, Daniel; Salakhutdinov, Ruslan; Bradbury, James (2023). \"Language models are few-shot learners\". arXiv:2302.13971 [cs.CL].\n* \u2191 Cho, Kyunghyun; Van Merri\u00ebnboer, Bart; Bahdanau, Dzmitry; Bougares, Fethi (2014). \"Learning phrase representations using RNN encoder-decoder for statistical machine translation\". Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). doi:10.18653/v1/d14-1179. ISBN\u00a09781937280884.\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2019). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 \"DeepSeek-R1\". DeepSeek. Retrieved 20 October 2023.\n* \u2191 \"Gemma\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 2\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 2.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 3\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 3.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 4\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 4.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 6\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 6.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 7\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 7.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 8\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 8.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 9\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 9.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 10\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 10.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 11\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 11.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 12\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 12.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 13\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 13.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 14\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 14.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 15\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 15.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 16\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 16.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 17\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 17.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 18\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 18.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 19\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 19.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 20\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 20.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 21\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 21.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 22\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 22.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 23\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 23.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 24\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 24.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 25\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 25.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 26\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 26.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 27\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 27.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 28\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 28.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 29\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 29.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 30\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 30.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 31\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 31.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 32\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 32.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 33\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 33.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 34\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 34.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 35\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 35.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 36\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 36.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 37\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 37.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 38\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 38.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 39\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 39.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 40\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 40.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 41\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 41.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 42\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 42.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 43\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 43.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 44\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 44.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 45\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 45.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 46\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 46.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 47\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 47.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 48\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 48.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 49\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 49.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 50\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 50.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 51\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 51.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 52\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 52.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 53\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 53.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 54\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 54.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 55\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 55.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 56\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 56.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 57\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 57.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 58\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 58.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 59\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 59.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 60\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 60.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 61\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 61.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 62\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 62.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 63\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 63.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 64\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 64.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 65\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 65.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 66\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 66.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 67\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 67.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 68\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 68.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 69\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 69.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 70\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 70.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 71\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 71.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 72\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 72.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 73\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 73.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 74\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 74.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 75\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 75.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 76\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 76.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 77\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 77.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 78\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 78.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 79\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 79.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 80\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 80.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 81\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 81.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 82\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 82.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 83\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 83.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 84\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 84.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 85\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 85.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 86\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 86.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 87\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 87.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 88\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 88.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 89\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 89.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 90\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 90.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 91\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 91.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 92\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 92.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 93\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 93.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 94\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 94.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 95\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 95.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 96\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 96.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 97\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 97.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 98\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 98.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 99\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 99.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 100\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 100.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 101\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 101.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 102\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 102.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 103\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 103.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 104\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 104.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 105\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 105.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 106\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 106.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 107\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 107.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 108\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 108.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 109\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 109.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 110\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 110.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 111\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 111.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 112\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 112.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 113\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 113.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 114\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 114.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 115\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 115.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 116\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 116.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 117\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 117.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 118\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 118.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 119\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 119.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 120\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 120.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 121\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 121.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 122\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 122.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 123\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 123.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 124\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 124.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 125\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 125.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 126\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 126.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 127\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 127.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 128\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 128.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 129\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 129.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 130\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 130.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 131\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 131.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 132\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 132.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 133\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 133.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 134\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 134.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 135\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 135.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 136\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 136.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 137\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 137.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 138\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 138.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 139\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 139.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 140\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 140.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 141\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 141.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 142\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 142.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 143\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 143.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 144\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 144.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 145\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 145.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 146\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 146.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 147\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 147.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 148\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 148.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 149\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 149.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 150\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 150.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 151\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 151.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 152\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 152.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 153\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 153.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 154\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 154.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 155\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 155.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 156\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 156.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 157\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 157.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 158\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 158.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 159\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 159.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 160\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 160.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 161\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 161.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 162\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 162.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 163\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 163.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 164\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 164.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 165\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 165.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 166\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 166.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 167\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 167.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 168\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 168.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 169\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 169.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 170\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 170.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 171\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 171.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 172\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 172.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 173\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 173.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 174\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 174.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 175\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 175.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 176\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 176.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 177\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 177.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 178\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 178.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 179\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 179.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 180\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 180.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 181\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 181.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 182\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 182.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 183\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 183.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 184\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 184.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 185\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 185.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 186\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 186.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 187\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 187.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 188\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 188.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 189\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 189.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 190\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 190.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 191\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 191.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 192\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 192.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 193\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 193.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 194\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 194.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 195\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 195.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 196\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 196.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 197\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 197.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 198\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 198.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 199\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 199.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 200\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 200.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 201\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 201.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 202\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 202.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 203\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 203.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 204\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 204.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 205\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 205.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 206\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 206.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 207\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 207.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 208\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 208.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 209\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 209.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 210\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 210.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 211\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 211.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 212\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 212.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 213\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 213.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 214\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 214.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 215\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 215.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 216\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 216.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 217\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 217.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 218\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 218.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 219\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 219.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 220\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 220.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 221\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 221.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 222\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 222.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 223\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 223.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 224\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 224.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 225\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 225.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 226\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 226.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 227\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 227.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 228\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 228.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 229\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 229.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 230\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 230.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 231\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 231.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 232\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 232.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 233\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 233.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 234\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 234.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 235\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 235.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 236\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 236.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 237\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 237.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 238\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 238.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 239\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 239.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 240\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 240.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 241\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 241.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 242\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 242.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 243\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 243.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 244\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 244.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 245\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 245.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 246\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 246.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 247\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 247.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 248\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 248.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 249\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 249.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 250\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 250.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 251\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 251.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 252\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 252.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 253\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 253.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 254\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 254.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 255\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 255.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 256\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 256.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 257\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 257.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 258\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 258.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 259\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 259.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 260\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 260.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 261\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 261.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 262\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 262.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 263\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 263.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 264\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 264.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 265\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 265.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 266\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 266.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 267\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 267.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 268\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 268.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 269\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 269.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 270\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 270.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 271\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 271.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 272\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 272.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 273\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 273.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 274\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 274.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 275\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 275.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 276\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 276.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 277\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 277.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 278\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 278.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 279\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 279.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 280\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 280.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 281\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 281.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 282\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 282.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 283\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 283.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 284\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 284.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 285\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 285.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 286\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 286.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 287\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 287.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 288\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 288.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 289\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 289.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 290\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 290.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 291\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 291.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 292\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 292.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 293\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 293.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 294\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 294.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 295\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 295.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 296\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 296.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 297\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 297.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 298\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 298.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 299\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 299.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 300\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 300.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 301\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 301.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 302\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 302.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 303\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 303.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 304\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 304.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 305\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 305.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 306\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 306.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 307\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 307.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 308\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 308.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 309\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 309.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 310\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 310.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 311\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 311.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 312\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 312.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 313\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 313.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 314\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 314.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 315\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 315.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 316\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 316.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 317\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 317.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 318\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 318.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 319\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 319.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 320\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 320.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 321\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 321.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 322\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 322.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 323\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 323.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 324\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 324.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 325\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 325.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 326\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 326.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 327\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 327.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 328\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma 328.5\". Hugging Face. Retrieved 19 October 2023.\n* \u2191 \"Gemma"}, {"instruction": "You are an AI editor writing a complete Wikipedia article that adheres to Wikipedia's content standards. The article should be coherent, with clear connections between sections and a consistent narrative throughout. Use neutral and encyclopedic language. \n\nThe article should contain 10 sections:\n\nSection 1 title should be Introduction, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain the concept of word embeddings in natural language processing, highlighting their representation as real-valued vectors, methods of generation, and their impact on improving performance in various NLP tasks.\n\nSection 2 title should be Development and history of the approach, and this section should contain approximately (500 words):\nIn this section, you should follow this prompt: Explain the historical development and evolution of word embeddings, highlighting key advancements, influential models, and the shift from algebraic to neural network approaches in semantic space modeling.\n\nSection 3 title should be Polysemy and homonymy, and this section should contain approximately (300 words):\nIn this section, you should follow this prompt: Explain how static word embeddings struggle with polysemy and homonymy, and describe the development and impact of multi-sense embeddings and contextually-meaningful embeddings like ELMo and BERT in addressing these challenges.\n\nSection 4 title should be For biological sequences: BioVectors, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain how word embeddings, specifically bio-vectors such as BioVec, ProtVec, and GeneVec, are applied to biological sequences like DNA, RNA, and proteins to enhance bioinformatics applications through deep learning by capturing biochemical and biophysical patterns.\n\nSection 5 title should be Game design, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain how word embeddings are utilized in game design to uncover emergent gameplay and capture expert knowledge through the transcription of gameplay actions into formal language, as demonstrated by Rabii and Cook.\n\nSection 6 title should be Sentence embeddings, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain the development and significance of sentence embeddings, highlighting key advancements such as skip-thought vectors and Sentence-BERT, and their impact on natural language processing tasks like machine translation.\n\nSection 7 title should be Software, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Discuss the various software tools available for training and utilizing word embeddings, highlighting their functionalities and applications in dimensionality reduction and visualization.\n\nSection 8 title should be Examples of application, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Describe various real-world applications of word embeddings, highlighting tools like fastText used in platforms such as Sketch Engine for processing and analyzing text corpora.\n\nSection 9 title should be Ethical implications, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Discuss the ethical implications of word embeddings, focusing on how they may perpetuate and amplify existing gender and racial biases present in the training data.\n\nSection 10 title should be See also, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain related concepts and techniques to word embedding, providing context and connections to other areas in machine learning and data representation.\n\nTotal article length should be approximately 1600 words. Ensure smooth transitions between sections and maintain a coherent narrative throughout. Each section should flow naturally into the next, building a complete and informative Wikipedia article. Incorporate appropriate references and adhere to Wikipedia's neutral point of view.", "input": "", "output": "Introduction\nIn natural language processing, a word embedding is a representation of a word. The embedding is used in text analysis. Typically, the representation is a real-valued vector that encodes the meaning of the word in such a way that the words that are closer in the vector space are expected to be similar in meaning. Word embeddings can be obtained using language modeling and feature learning techniques, where words or phrases from the vocabulary are mapped to vectors of real numbers.\nMethods to generate this mapping include neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models, explainable knowledge base method, and explicit representation in terms of the context in which words appear.\nWord and phrase embeddings, when used as the underlying input representation, have been shown to boost the performance in NLP tasks such as syntactic parsing and sentiment analysis.Development and history of the approach\nIn distributional semantics, a quantitative methodological approach for understanding meaning in observed language, word embeddings or semantic feature space models have been used as a knowledge representation for some time. Such models aim to quantify and categorize semantic similarities between linguistic items based on their distributional properties in large samples of language data.  The underlying idea that \"a word is characterized by the company it keeps\" was proposed in a 1957 article by John Rupert Firth, but also has roots in the contemporaneous work on search systems and in cognitive psychology.\nThe notion of a semantic space with lexical items (words or multi-word terms) represented as vectors or embeddings is based on the computational challenges of capturing distributional characteristics and using them for practical application to measure similarity between words, phrases, or entire documents. The first generation of semantic space models is the vector space model for information retrieval. Such vector space models for words and their distributional data implemented in their simplest form results in a very sparse vector space of high dimensionality (cf. curse of dimensionality). Reducing the number of dimensions using linear algebraic methods such as singular value decomposition then led to the introduction of latent semantic analysis in the late 1980s and the random indexing approach for collecting word co-occurrence contexts. In 2000, Bengio et al. provided in a series of papers titled \"Neural probabilistic language models\" to reduce the high dimensionality of word representations in contexts by \"learning a distributed representation for words\".\nA study published in NeurIPS (NIPS) 2002 introduced the use of both word and document embeddings applying the method of kernel CCA to bilingual (and multi-lingual) corpora, also providing an early example of self-supervised learning of word embeddings.\nWord embeddings come in two different styles, one in which words are expressed as vectors of co-occurring words, and another in which words are expressed as vectors of linguistic contexts in which the words occur; these different styles are studied in Lavelli et al., 2004. Roweis and Saul published in Science how to use \"locally linear embedding\" (LLE) to discover representations of high dimensional data structures. Most new word embedding techniques after about 2005 rely on a neural network architecture instead of more probabilistic and algebraic models, after foundational work done by Yoshua Bengio and colleagues.\nThe approach has been adopted by many research groups after theoretical advances in 2010 had been made on the quality of vectors and the training speed of the model, as well as after hardware advances allowed for a broader parameter space to be explored profitably. In 2013, a team at Google led by Tomas Mikolov created word2vec, a word embedding toolkit that can train vector space models faster than previous approaches. The word2vec approach has been widely used in experimentation and was instrumental in raising interest for word embeddings as a technology, moving the research strand out of specialised research into broader experimentation and eventually paving the way for practical application.Polysemy and homonymy\nHistorically, one of the main limitations of static word embeddings or word vector space models is that words with multiple meanings are conflated into a single representation (a single vector in the semantic space). In other words, polysemy and homonymy are not handled properly. For example, in the sentence \"The club I tried yesterday was great!\", it is not clear if the term club is related to the word sense of a club sandwich, clubhouse, golf club, or any other sense that club might have. The necessity to accommodate multiple meanings per word in different vectors (multi-sense embeddings) is the motivation for several contributions in NLP to split single-sense embeddings into multi-sense ones.\nMost approaches that produce multi-sense embeddings can be divided into two main categories for their word sense representation, i.e., unsupervised and knowledge-based. Based on word2vec skip-gram, Multi-Sense Skip-Gram (MSSG) performs word-sense discrimination and embedding simultaneously, improving its training time, while assuming a specific number of senses for each word. In the Non-Parametric Multi-Sense Skip-Gram (NP-MSSG) this number can vary depending on each word. Combining the prior knowledge of lexical databases (e.g., WordNet, ConceptNet, BabelNet), word embeddings and word sense disambiguation, Most Suitable Sense Annotation (MSSA) labels word-senses through an unsupervised and knowledge-based approach, considering a word's context in a pre-defined sliding window. Once the words are disambiguated, they can be used in a standard word embeddings technique, so multi-sense embeddings are produced. MSSA architecture allows the disambiguation and annotation process to be performed recurrently in a self-improving manner.\nThe use of multi-sense embeddings is known to improve performance in several NLP tasks, such as part-of-speech tagging, semantic relation identification, semantic relatedness, named entity recognition and sentiment analysis.\nAs of the late 2010s, contextually-meaningful embeddings such as ELMo and BERT have been developed. Unlike static word embeddings, these embeddings are at the token-level, in that each occurrence of a word has its own embedding. These embeddings better reflect the multi-sense nature of words, because occurrences of a word in similar contexts are situated in similar regions of BERT\u2019s embedding space.For biological sequences: BioVectors\nWord embeddings for n-grams in biological sequences (e.g. DNA, RNA, and Proteins) for bioinformatics applications have been proposed by Asgari and Mofrad. Named bio-vectors (BioVec) to refer to biological sequences in general with protein-vectors (ProtVec) for proteins (amino-acid sequences) and gene-vectors (GeneVec) for gene sequences, this representation can be widely used in applications of deep learning in proteomics and genomics. The results presented by Asgari and Mofrad suggest that BioVectors can characterize biological sequences in terms of biochemical and biophysical interpretations of the underlying patterns.Game design\nWord embeddings with applications in game design have been proposed by Rabii and Cook as a way to discover emergent gameplay using logs of gameplay data. The process requires transcribing actions that occur during a game within a formal language and then using the resulting text to create word embeddings. The results presented by Rabii and Cook suggest that the resulting vectors can capture expert knowledge about games like chess that are not explicitly stated in the game's rules.Sentence embeddings\nThe idea has been extended to embeddings of entire sentences or even documents, e.g. in the form of the thought vectors concept. In 2015, some researchers suggested \"skip-thought vectors\" as a means to improve the quality of machine translation. A more recent and popular approach for representing sentences is Sentence-BERT, or SentenceTransformers, which modifies pre-trained BERT with the use of siamese and triplet network structures.Software\nSoftware for training and using word embeddings includes Tom\u00e1\u0161 Mikolov's Word2vec, Stanford University's GloVe, GN-GloVe, Flair embeddings, AllenNLP's ELMo, BERT, fastText, Gensim, Indra, and Deeplearning4j. Principal Component Analysis (PCA) and T-Distributed Stochastic Neighbour Embedding (t-SNE) are both used to reduce the dimensionality of word vector spaces and visualize word embeddings and clusters.Examples of application\nFor instance, the fastText is also used to calculate word embeddings for text corpora in Sketch Engine that are available online.Ethical implications\nWord embeddings may contain the biases and stereotypes contained in the trained dataset, as Bolukbasi et al. points out in the 2016 paper \u201cMan is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\u201d that a publicly available (and popular) word2vec embedding trained on Google News texts (a commonly used data corpus), which consists of text written by professional journalists, still shows disproportionate word associations reflecting gender and racial biases when extracting word analogies. For example, one of the analogies generated using the aforementioned word embedding is \u201cman is to computer programmer as woman is to homemaker\u201d.\nResearch done by Jieyu Zhou et al. shows that the applications of these trained word embeddings without careful oversight likely perpetuates existing bias in society, which is introduced through unaltered training data. Furthermore, word embeddings can even amplify these biases .See also\nEmbedding (machine learning)\nBrown clustering\nDistributional\u2013relational database\n\n\n== References ==", "result": "# Introduction\n\nIn natural language processing, word embeddings are numerical representations of words in a vector space where semantically similar words have closer distances from each other. Word embeddings are required as an input to word-level deep learning models. A word embedding is a vector space where the length of the vector is a hyperparameter. Usually, word embeddings are trained alongside the rest of the model using stochastic gradient descent.\n\nWord embeddings are different from one-hot encoded bag-of-words feature vectors. While a bag-of-words model uses the entire vocabulary as a basis, it only indicates the frequency of occurrence of each word within a fixed piece of text, losing information about the relationships between words. Word embeddings use the entire corpus as a basis and reflects the semantic relationships between words through vector arithmetic. For example, considering the following vector space where each word is orthogonal to every other word:\n\n    [ {\\mathbf {v} _{\\text{cat}}= \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix},\\mathbf {v} _{\\text{dog}}= \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix},\\mathbf {v} _{\\text{mouse}}= \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix},\\mathbf {v} _{\\text{house}}= \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix}}]\n\nThere is no meaningful relationship between $\\mathbf {v} _{\\text{cat}}$ and $\\mathbf {v} _{\\text{dog}}$. However, in a vector space learned from a large corpus, where the vocabulary size is much larger, there exist meaningful relationships such as:\n\n    ${\\mathbf {v} _{\\text{dog}}-\\mathbf {v} _{\\text{run}}\\approx \\mathbf {v} _{\\text{cat}}-\\mathbf {v} _{\\text{chase}}}$\n\nDevelopment and history of the approach\n\nThe first million-word statistical corpus was created around 1990. It included the Brown Corpus, the Wall Street Journal Corpus, and the Genesis Corpus. The development of the Google News dataset (2005) and the GloVe global vectors project (2014) increased the size of the corpori to over a billion words. As a result, word embeddings became more accurate and useful.\n\nThe first word embedding technique was the distributional model. This model distributes words in a multi-dimensional space based on their contextual similarity. If two words frequently appear in the same context, they will be represented nearby in the multi-dimensional space. The distributional model was first developed nearly 70 years ago by Zellig Harris, who demonstrated that word frequencies could be used to derive their meaning. He showed that words with similar meanings appeared in similar contexts with relatively little supervision. For example, the words \"chair\" and \"couch\" might be similarly contextually salient with respect to words like \"table\", \"sofa\", and \"desk\".\n\nThe distributional model was further developed by James Pennington, Richard Socher, and Christopher Manning. They introduced Global Vectors (GloVe), a technique for obtaining word vectors from large corpora. The GloVe method calculates a low-rank factorization of a co-occurrence matrix derived from a large corpus. It computes word vectors by counting the number of times words with the same context appear together in a large corpus. The basic math behind GloVe is singular value decomposition (SVD) which has been around since the late 19th century. SVD is a matrix factorization technique that decomposes a matrix of word counts into three matrices. The middle matrix contains the word vectors in its rows or columns (depending on the choice of coordinate system).\n\nPennington, Socher, and Manning also created a publicly available pre-trained word embedding model called GloVe that became widely popular among researchers. One reason for its popularity was that it came pre-trained on a very large corpus. Pre-training on a large corpus helps when a researcher is working with limited data.\n\nAnother technique for generating word embeddings is continuous bag of words (CBOW) which is an algorithm capable of learning word embeddings. It does this by predicting a word from the context of its surrounding words.\n\nContinuous Bag of Words was later improved upon by the skip-gram model. Skip-gram also uses word co-occurrence but focuses on learning word embeddings by predicting a word's surroundings given a center word. Both GloVe and skip-gram learn word embeddings that are accurate for most common words but perform poorly for rare words. This is because the co-occurrence matrices for rare words are sparse.\n\nTo overcome this limitation, Mikolov et al. proposed a new model called Continuous Bag of Predictions (CBoW-pred). CBoW-pred is a modification of the original CBoW model that uses prediction instead of context to generate word embeddings. CBoW-pred performs well for both frequent and rare words. This is because it uses true context instead of surrounding words (which may not even include the rare word in question).\n\n Polysemy and homonymy\n\nStatic word embeddings are not able to represent polysemous words, i.e., words that have multiple, distinct meanings. For example, the word \"bank\" has two main definitions \u2013 \"financial institution\" and \"river bank\" \u2013 that are quite distinct in terms of their semantic features. A word embedding trained on a large corpus that attempts to represent both definitions will either settle on a definition that lies approximately halfway between the two semantic features, or it will converge on two separate points that best represent each definition independently.\n\nOne way to address this issue is to represent a word with multiple embeddings, each corresponding to one of the word's definitions. When a word appears in a sentence in a particular context, the word's corresponding embedding that best matches the semantic features associated with that context is chosen for the representation. However, this approach is not feasible in cases where the number of distinct contexts in which a word can appear is very large.\n\nA better solution is to represent a word as a single, context-dependent embedding. This can be achieved by fine-tuning a pre-existing static embedding model on a task-specific corpus that contains sentences in which the target word appears in various polysemous contexts. The idea is to adjust the original embedding such that it aligns better with the specific semantic features associated with each context in which the word appears. For example, if the target word appears in some sentences as a financial term and in other sentences as a geographical term, its original embedding would be adjusted so that it has equal distance from words such as \"interest\" and \"interest rate\" (e.g., equidistant from the semantic features associated with the term \"interest\" and \"rate\") in the first set of sentences, and equal distance from words such as \"shore,\" \"edge,\" and \"side\" (e.g., equidistant from the semantic features associated with the term \"shore\" and \"side\") in the second set of sentences.\n\nThis problem is inherent in static word embedding models, which rely on pre-existing representations of words. In contrast, non-static models avoid this issue entirely because they do not rely on pre-existing representations of words. Rather, they define words on-the-fly as products of their contexts. As a result, they are capable of representing words in context without any ambiguity.\n\n--- Multi-sense embeddings\n\nMulti-sense embeddings are embeddings that represent polysemous words in multiple distinct forms. Each form represents one sense of the word. Multi-sense embeddings are able to capture the meaning of words in context due to the large size of the context window. Context windows of size 5 to 10 are sufficient for the model to capture the meaning of a word, as most words have at most 2\u20133 dominant senses. In a multi-sense embedding model, the position of a word in the context window determines which sense of the word is being represented. For example, the word \"bank\" might appear 10 times in a 1000-word sentence, but different senses will be represented depending on the words that appear in the surrounding 5-word context window.\n\nThe first multi-sense embeddings were obtained from separate static word embeddings and a clustering analysis. More recent multi-sense embeddings are obtained directly from the training process through word sense disambiguation during the learning of the embeddings. Multi-sense embeddings are generally better than static word embeddings for tasks involving polysemous words such as word sense disambiguation, paraphrasal identification, and sentence similarity.\n\n--- Contextually-meaningful embeddings\n\nContextually-meaningful word embeddings are embeddings that are generated by a neural network in which the weights are frozen. The model modifies the input word embeddings to create output word embeddings that reflect the word's meaning in context. Since the weights of the frozen neural network cannot be changed, the output embeddings accurately represent the meaning of the word in a particular context without any ambiguity.\n\nThis approach can be seen as a generalization of multi-sense embeddings. Unlike multi-sense embeddings, contextually-meaningful embeddings are able to capture the nuances of language. For instance, the word \"bank\" might appear in the context of financial jargon, in which case its associated output embedding would be close to the multi-sense embedding representing \"bank\" as a financial institution; or in the context of geography, in which case its associated output embedding would be close to the multi-sense embedding representing \"bank\" as a riverside.\n\nFor small vocabularies, contextually meaningful embeddings can be obtained by fine-tuning a pre-trained static word embedding model on a corpus containing sentences in which the target word appears in multiple senses. As in the multi-sense embedding case, fine-tuning involves adjusting the original word's embedding such that it better represents its meaning in context. However, unlike multi-sense embeddings, contextually meaningful embeddings are obtained by fine-tuning a frozen neural network, rather than by adjusting a fixed-dimensional embedding.\n\n For biological sequences\n\nWord embeddings have also been applied to the domains of biology and genomics. For biological sequences, embeddings can be created for both codons and amino acids. Codon embeddings can be used to accurately predict phenotypic properties of codons, such as expression levels and fitness. Codon embeddings can also be used to predict gene essentiality with higher accuracy than sequence-based encodings. Amino acid embeddings can be used to predict protein-protein interactions, protein stability, and protein function.\n\nIn a similar fashion to natural language processing, local sequence properties can be used to generate sequence evolution transition matrices. These matrices can be used to generate an eigenvalue representation of a sequence. The eigenvalue representation of a sequence has been shown to be effective at encoding sequence-function relationships in proteins and can be used to make accurate predictions about the effects of mutations on protein function.\n\n Game design\n\nWord embeddings can be used in game design. Rabii and Cook suggest that the transcription of video game gameplay into natural language and subsequent translation into foreign languages results in loss of information. They propose an alternative approach: using word embeddings to transcribe gameplay into a non-natural language format that retains more information, and which can be easily translated without loss of information. In addition, word embeddings can be used to transcribe gameplay and then compare the resulting vectors to data transcribed from a game expert's gameplay to automatically generate a metric measuring a player's skill level compared to the average player or compared to expert players.\n\n Sentence embeddings\n\nSentence embeddings are the extension of word embeddings to full sentences. In a sentence embedding, the proximity of sentences in the vector space is proportional to their semantic similarity. There are many possible ways to construct a sentence embedding. The two most popular approaches are skip-thought vectors and Sentence-BERT.\n\n Sentence-BERT (SBERT)\n\nSBERT approaches the problem of sentence embedding construction as a natural language inference (NLI) task. Given two sentences, the goal is to predict the relation of those two sentences. SBERT encodes both sentences into embeddings and appends them to obtain a prediction. This approach requires a labelled NLI dataset, such as the SNLI dataset.\n\nSkip-thought vectors (STV)\n\nThe goal of STV is to learn embeddings that help infer the context of a sentence from the sentences before and after it. To achieve this, a model is trained to predict the preceding and succeeding N sentences in a paragraph. The hope is that the learned embeddings will be useful for understanding the context of a sentence. This is inspired by the way humans read paragraphs: it is much easier to guess the missing sentence in \"Mary had a little lamb, ___, she loved that lamb\" (\"Mary had a little __\" is much easier). This is also why filling in the blank exercises work well for teaching language.\n\n Software\n\nThere are many programs that will produce word embeddings from a text corpus. Some programs require the memory footprint of the vectors (i.e., the dimensionality) to be specified beforehand, while others will infer it from the corpus. Some popular software are listed below.\n\n* Gensim: a library of vector space models and topic models implemented in Python. Gensim creates word embeddings, but it also provides the functionality to download and use word2vec and GloVe embeddings.\n* spaCy: a library for natural language processing implemented in Python and Cython. spaCy can load word embeddings that were trained with word2vec or GloVe.\n* Stanford CoreNLP: a Java library for natural language processing that offers sentiment analysis, dependency parsing, named entity recognition, coreference resolution, linguistic trees, and tokenization.\n* Allennlp: a library for natural language processing implemented in Python and written on top of PyTorch. AllenNLP includes several pretrained models.\n* GloVe: the authors of the original algorithm have released their reference implementation in C, with bindings for Python, R, and MATLAB.\n* word2vec: the authors of the original algorithm have released their reference implementation in C, with popular bindings for Python and MATLAB.\n* fastText: an open-source C++ software package developed by Facebook's AI Research. It comes with a highly efficient C++ interface, as well as Python and Go bindings. It is designed to be able to train and test very large models, using multiple CPUs and out-of-core memory. It includes several extensions to word2vec and GloVe, such as supervised learning for classification tasks, hashing for training on very large corpora, and a mixture of word vectors.\n* Flair: a library for state-of-the-art pre-trained language models for NLP tasks implemented in Python and PyTorch.\n\n Examples of application\n\n* fastText is an extension of word2vec and GloVe that was originally developed by Facebook's AI Research group. It is designed to train and test very large models using multiple CPUs and out-of-core memory. fastText includes extensions to word2vec and GloVe, such as supervised learning for classification tasks and hashing for training on very large corpora.\n* Vector Space Models and Topic Models: Vector space models and topic models are two closely related categories of algorithms that operate on the word embeddings of a corpus. Both types of algorithm map words that are frequently used together to similar locations in vector space.\n* Sketch Engine is a tool for processing large corpora with vector space models and topic models.\n* ConceptNet is a knowledge base connecting words and phrases from different languages with each other, using relations that are derived from open data on the Web. ConceptNet includes over 36 million such connections, or edges, in 15 languages, and is built on top of the Open Multilingual Wordnet, a multi-lingual word sense disambiguation system that uses the WordNet lexical database.\n* Sketch Engine can be used to build concordance dictionaries with example sentences for any language that have been tagged with part of speech.\n* Sketch Engine can be used to build collocation lists for any language that has been tagged with part of speech.\n* Sketch Engine can be used to explore the usage of idioms in a corpus and to generate idiom charts.\n* Sketch Engine can be used to build a thesaurus for any language that has been tagged with part of speech.\n\n Ethical implications\n\nEmbedding algorithms do not account for gender, race, ethnicity, or any other demographic characteristic. Yet, word embeddings still reproduce demographic disparities. For example, word vectors for black names are farther from white names than vectors for pairs of equally common fake names. This holds true even when training data does not explicitly mention race. Similar results occur for other demographic characteristics.\n\n"}, {"instruction": "You are an AI editor writing a complete Wikipedia article that adheres to Wikipedia's content standards. The article should be coherent, with clear connections between sections and a consistent narrative throughout. Use neutral and encyclopedic language. \n\nThe article should contain 17 sections:\n\nSection 1 title should be Introduction, and this section should contain approximately (200 words):\nIn this section, you should follow this prompt: Describe the introduction of BERT, highlighting its development by Google, its impact on natural language processing, training methods, model sizes, and its significance as a baseline in NLP experiments.\n\nSection 2 title should be Architecture, and this section should contain approximately (200 words):\nIn this section, you should follow this prompt: Explain the components and functionality of BERT's encoder-only transformer architecture, highlighting the roles of the tokenizer, embedding, encoder, and task head, as well as their significance in pre-training and downstream tasks.\n\nSection 3 title should be Embedding, and this section should contain approximately (300 words):\nIn this section, you should follow this prompt: Explain the embedding process used in BERTBASE, detailing the roles and functions of token type embeddings, position embeddings, and segment type embeddings, and describe how these are combined and processed through the model.\n\nSection 4 title should be Architectural family, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain how varying the number of layers and hidden size in BERT's encoder stack creates different model configurations and describe the implications of these variations using examples like BERTBASE, BERTLARGE, and BERTTINY.\n\nSection 5 title should be Training, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain the process and techniques used to train the BERT language model, highlighting key innovations and methodologies that contributed to its effectiveness.\n\nSection 6 title should be Pre-training, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain how BERT's pre-training tasks, Masked Language Model (MLM) and Next Sentence Prediction (NSP), contribute to its ability to understand bidirectional context and sentence relationships.\n\nSection 7 title should be Masked language modeling, and this section should contain approximately (200 words):\nIn this section, you should follow this prompt: Explain the process and purpose of masked language modeling in BERT, highlighting how token masking probabilities are used to prevent dataset shift and improve model training.\n\nSection 8 title should be Next sentence prediction, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain how BERT uses the next sentence prediction task to determine the sequential relationship between two spans of text, including the role of special tokens like [CLS] and [SEP] in this process.\n\nSection 9 title should be Fine-tuning, and this section should contain approximately (200 words):\nIn this section, you should follow this prompt: Explain how BERT is fine-tuned for specific natural language processing tasks, highlighting its ability to achieve state-of-the-art performance with minimal resources and detailing the process and recommendations for fine-tuning its parameters.\n\nSection 10 title should be Cost, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain the resources, duration, and estimated costs involved in training BERTBASE and BERTLARGE, highlighting the differences in computational requirements and expenses.\n\nSection 11 title should be Interpretation, and this section should contain approximately (300 words):\nIn this section, you should follow this prompt: Explore the challenges and methodologies involved in interpreting BERT's language understanding capabilities, including its bidirectional training advantages and limitations in text generation due to its encoder-only architecture.\n\nSection 12 title should be History, and this section should contain approximately (200 words):\nIn this section, you should follow this prompt: Discuss the development and impact of BERT as a language model, highlighting its origins, unique bidirectional design, and its integration into Google Search for processing language queries.\n\nSection 13 title should be Variants, and this section should contain approximately (300 words):\nIn this section, you should follow this prompt: Discuss the various influential variants of the BERT language model, highlighting their unique architectural changes, improvements, and specific tasks they address, such as RoBERTa's training enhancements, DistilBERT's parameter reduction, ALBERT's parameter sharing and SOP task, ELECTRA's adversarial approach, and DeBERTa's disentangled attention mechanism.\n\nSection 14 title should be Notes, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain the key points and additional information about BERT that are typically found in the \"Notes\" section of a Wikipedia article, focusing on clarifications, supplementary details, or insights that enhance the understanding of the language model.\n\nSection 15 title should be References, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain the key sources and studies that have contributed to the development and understanding of the BERT language model, highlighting significant research papers and influential works in the field.\n\nSection 16 title should be Further reading, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Discuss the key insights and findings from the paper \"A Primer in BERTology: What we know about how BERT works\" by Rogers, Kovaleva, and Rumshisky, highlighting its contributions to understanding the inner workings and applications of the BERT language model.\n\nSection 17 title should be External links, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Describe the purpose and significance of the official GitHub repository for BERT, highlighting how it serves as a resource for developers and researchers interested in exploring and utilizing the BERT language model.\n\nTotal article length should be approximately 2800 words. Ensure smooth transitions between sections and maintain a coherent narrative throughout. Each section should flow naturally into the next, building a complete and informative Wikipedia article. Incorporate appropriate references and adhere to Wikipedia's neutral point of view.", "input": "", "output": "Introduction\nBidirectional encoder representations from transformers (BERT) is a language model introduced in October 2018 by researchers at Google. It learns to represent text as a sequence of vectors using self-supervised learning. It uses the encoder-only transformer architecture. BERT dramatically improved the state-of-the-art for large language models. As of 2020, BERT is a ubiquitous baseline in natural language processing (NLP) experiments. \nBERT is trained by masked token prediction and next sentence prediction. As a result of this training process, BERT learns contextual, latent representations of tokens in their context, similar to ELMo and GPT-2. It found applications for many natural language processing tasks, such as coreference resolution and polysemy resolution. It is an evolutionary step over ELMo, and spawned the study of \"BERTology\", which attempts to interpret what is learned by BERT.\nBERT was originally implemented in the English language at two model sizes, BERTBASE (110 million parameters) and BERTLARGE (340 million parameters). Both were trained on the Toronto BookCorpus (800M words) and English Wikipedia  (2,500M words). The weights were released on GitHub. On March 11, 2020, 24 smaller models were released, the smallest being BERTTINY with just 4 million parameters.Architecture\nBERT is an \"encoder-only\" transformer architecture. At a high level, BERT consists of 4 modules: \n\nTokenizer: This module converts a piece of English text into a sequence of integers (\"tokens\").\nEmbedding: This module converts the sequence of tokens into an array of real-valued vectors representing the tokens. It represents the conversion of discrete token types into a lower-dimensional Euclidean space.\nEncoder: a stack of Transformer blocks with self-attention, but without causal masking.\nTask head: This module converts the final representation vectors into one-hot encoded tokens again by producing a predicted probability distribution over the token types. It can be viewed as a simple decoder, decoding the latent representation into token types, or as an \"un-embedding layer\".\nThe task head is necessary for pre-training, but it is often unnecessary for so-called \"downstream tasks,\" such as question answering or sentiment classification. Instead, one removes the task head and replaces it with a newly initialized module suited for the task, and finetune the new module. The latent vector representation of the model is directly fed into this new module, allowing for sample-efficient transfer learning.Embedding\nThis section describes the embedding used by BERTBASE. The other one, BERTLARGE, is similar, just larger.\nThe tokenizer of BERT is WordPiece, which is a sub-word strategy like byte pair encoding. Its vocabulary size is 30,000, and any token not appearing in its vocabulary is replaced by [UNK] (\"unknown\"). \n\nThe first layer is the embedding layer, which contains three components: token type embeddings, position embeddings, and segment type embeddings. \n\nToken type: The token type is a standard embedding layer, translating a one-hot vector into a dense vector based on its token type.\nPosition: The position embeddings are based on a token's position in the sequence. BERT uses absolute position embeddings, where each position in sequence is mapped to a real-valued vector. Each dimension of the vector consists of a sinusoidal function that takes the position in the sequence as input.\nSegment type: Using a vocabulary of just 0 or 1, this embedding layer produces a dense vector based on whether the token belongs to the first or second text segment in that input. In other words, type-1 tokens are all tokens that appear after the [SEP] special token. All prior tokens are type-0.\nThe three embedding vectors are added together representing the initial token representation as a function of these three pieces of information. After embedding, the vector representation is normalized using a LayerNorm operation, outputting a 768-dimensional vector for each input token. After this, the representation vectors are passed forward through 12 Transformer encoder blocks, and are decoded back to 30,000-dimensional vocabulary space using a basic affine transformation layer.Architectural family\nThe encoder stack of BERT has 2 free parameters: \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n, the number of layers, and \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  \n, the hidden size. There are always \n  \n    \n      \n        H\n        \n          /\n        \n        64\n      \n    \n    {\\displaystyle H/64}\n  \n self-attention heads, and the feed-forward/filter size is always \n  \n    \n      \n        4\n        H\n      \n    \n    {\\displaystyle 4H}\n  \n. By varying these two numbers, one obtains an entire family of BERT models.\nFor BERT\n\nthe feed-forward size and filter size are synonymous. Both of them denote the number of dimensions in the middle layer of the feed-forward network.\nthe hidden size and embedding size are synonymous. Both of them denote the number of real numbers used to represent a token.\nThe notation for encoder stack is written as L/H. For example, BERTBASE is written as 12L/768H, BERTLARGE as 24L/1024H, and BERTTINY as 2L/128H.Training\nPre-training\nBERT was pre-trained simultaneously on two tasks.\n\nMasked Language Model (MLM): In this task, BERT randomly masks some words in a sentence and tries to predict the original words that were masked. For example, in the sentence \"The cat sat on the [MASK],\" BERT would need to predict \"mat.\" This helps BERT learn bidirectional context, meaning it understands the relationships between words not just from left to right or right to left but from both directions at the same time.\nNext Sentence Prediction (NSP): In this task, BERT is trained to predict whether one sentence logically follows another. For example, given two sentences, \"The cat sat on the mat.\" and \"It was a sunny day,\" BERT has to decide if the second sentence is a valid continuation of the first one. This helps BERT understand relationships between sentences, which is important for tasks like question answering or document classification.Masked language modeling\nIn masked language modeling, 15% of tokens would be randomly selected for masked-prediction task, and the training objective was to predict the masked token given its context. In more detail, the selected token is \n\nreplaced with a [MASK] token with probability 80%,\nreplaced with a random word token with probability 10%,\nnot replaced with probability 10%.\nThe reason not all selected tokens are masked is to avoid the dataset shift problem. The dataset shift problem arises when the distribution of inputs seen during training differs significantly from the distribution encountered during inference. A trained BERT model might be applied to word representation (like Word2Vec), where it would be run over sentences not containing any [MASK] tokens. It is later found that more diverse training objectives are generally better.\nAs an illustrative example, consider the sentence \"my dog is cute\". It would first be divided into tokens like \"my1 dog2 is3 cute4\". Then a random token in the sentence would be picked. Let it be the 4th one \"cute4\". Next, there would be three possibilities:\n\nwith probability 80%, the chosen token is masked, resulting in \"my1 dog2 is3 [MASK]4\";\nwith probability 10%, the chosen token is replaced by a uniformly sampled random token, such as \"happy\", resulting in \"my1 dog2 is3 happy4\";\nwith probability 10%, nothing is done, resulting in \"my1 dog2 is3 cute4\".\nAfter processing the input text, the model's 4th output vector is passed to its decoder layer, which outputs a probability distribution over its 30,000-dimensional vocabulary space.Next sentence prediction\nGiven two spans of text, the model predicts if these two spans appeared sequentially in the training corpus, outputting either [IsNext] or [NotNext]. The first span starts with a special token [CLS] (for \"classify\"). The two spans are separated by a special token [SEP] (for \"separate\"). After processing the two spans, the 1-st output vector (the vector coding for [CLS]) is passed to a separate neural network for the binary classification into [IsNext] and [NotNext].\n\nFor example, given \"[CLS] my dog is cute [SEP] he likes playing\" the model should output token [IsNext].\nGiven \"[CLS] my dog is cute [SEP] how do magnets work\" the model should output token [NotNext].Fine-tuning\nBERT is meant as a general pretrained model for various applications in natural language processing. That is, after pre-training, BERT can be fine-tuned with fewer resources on smaller datasets to optimize its performance on specific tasks such as natural language inference and text classification, and sequence-to-sequence-based language generation tasks such as question answering and conversational response generation.\nThe original BERT paper published results demonstrating that a small amount of finetuning (for BERTLARGE, 1 hour on 1 Cloud TPU) allowed it to achieved state-of-the-art performance on a number of natural language understanding tasks:\n\nGLUE (General Language Understanding Evaluation) task set (consisting of 9 tasks);\nSQuAD (Stanford Question Answering Dataset) v1.1 and v2.0;\nSWAG (Situations With Adversarial Generations).\nIn the original paper, all parameters of BERT are finetuned, and recommended that, for downstream applications that are text classifications, the output token at the [CLS] input token is fed into a linear-softmax layer to produce the label outputs.\nThe original code base defined the final linear layer as a \"pooler layer\", in analogy with global pooling in computer vision, even though it simply discards all output tokens except the one corresponding to  [CLS] .Cost\nBERT was trained on the BookCorpus (800M words) and a filtered version of English Wikipedia (2,500M words) without lists, tables, and headers.\nTraining BERTBASE  on 4 cloud TPU (16 TPU chips total) took 4 days, at an estimated cost of 500 USD. Training BERTLARGE on 16 cloud TPU (64 TPU chips total) took 4 days.Interpretation\nLanguage models like ELMo, GPT-2, and BERT, spawned the study of \"BERTology\", which attempts to interpret what is learned by these models. Their performance on these natural language understanding tasks are not yet well understood. Several research publications in 2018 and 2019 focused on investigating the relationship behind BERT's output as a result of carefully chosen input sequences, analysis of internal vector representations through probing classifiers, and the relationships represented by attention weights.\nThe high performance of the BERT model could also be attributed to the fact that it is bidirectionally trained. This means that BERT, based on the Transformer model architecture, applies its self-attention mechanism to learn information from a text from the left and right side during training, and consequently gains a deep understanding of the context. For example, the word fine can have two different meanings depending on the context (I feel fine today, She has fine blond hair). BERT considers the words surrounding the target word fine from the left and right side.\nHowever it comes at a cost: due to encoder-only architecture lacking a decoder, BERT can't be prompted and can't generate text, while bidirectional models in general do not work effectively without the right side, thus being difficult to prompt. As an illustrative example, if one wishes to use BERT to continue a sentence fragment \"Today, I went to\", then naively one would mask out all the tokens as \"Today, I went to  [MASK]  [MASK]  [MASK] ...  [MASK] .\" where the number of  [MASK]  is the length of the sentence one wishes to extend to. However, this constitutes a dataset shift, as during training, BERT has never seen sentences with that many tokens masked out. Consequently, its performance degrades. More sophisticated techniques allow text generation, but at a high computational cost.History\nBERT was originally published by Google researchers Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. The design has its origins from pre-training contextual representations, including semi-supervised sequence learning, generative pre-training, ELMo, and ULMFit. Unlike previous models, BERT is a deeply bidirectional, unsupervised language representation, pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary, whereas BERT takes into account the context for each occurrence of a given word. For instance, whereas the vector for \"running\" will have the same word2vec vector representation for both of its occurrences in the sentences \"He is running a company\" and \"He is running a marathon\", BERT will provide a contextualized embedding that will be different according to the sentence.\nOn October 25, 2019, Google announced that they had started applying BERT models for English language search queries within the US. On December 9, 2019, it was reported that BERT had been adopted by Google Search for over 70 languages. In October 2020, almost every single English-based query was processed by a BERT model.Variants\nThe BERT models were influential and inspired many variants.\nRoBERTa (2019) was an engineering improvement. It preserves BERT's architecture (slightly larger, at 355M parameters), but improves its training, changing key hyperparameters, removing the next-sentence prediction task, and using much larger mini-batch sizes. \nDistilBERT (2019) distills BERTBASE to a model with just 60% of its parameters (66M), while preserving 95% of its benchmark scores. Similarly, TinyBERT (2019) is a distilled model with just 28% of its parameters.\nALBERT (2019) used shared-parameter across layers, and experimented with independently varying the hidden size and the word-embedding layer's output size as two hyperparameters. They also replaced the next sentence prediction task with the sentence-order prediction (SOP) task, where the model must distinguish the correct order of two consecutive text segments from their reversed order. \nELECTRA (2020) applied the idea of generative adversarial networks to the MLM task. Instead of masking out tokens, a small language model generates random plausible substitutions, and a larger network identify these replaced tokens. The small model aims to fool the large model.\nDeBERTa (2020) is a significant architectural variant, with disentangled attention. Its key idea is to treat the positional and token encodings separately throughout the attention mechanism. Instead of combining the positional encoding (\n  \n    \n      \n        \n          x\n          \n            p\n            o\n            s\n            i\n            t\n            i\n            o\n            n\n          \n        \n      \n    \n    {\\displaystyle x_{position}}\n  \n) and token encoding (\n  \n    \n      \n        \n          x\n          \n            token\n          \n        \n      \n    \n    {\\displaystyle x_{\\text{token}}}\n  \n) into a single input vector (\n  \n    \n      \n        \n          x\n          \n            i\n            n\n            p\n            u\n            t\n          \n        \n        =\n        \n          x\n          \n            p\n            o\n            s\n            i\n            t\n            i\n            o\n            n\n          \n        \n        +\n        \n          x\n          \n            t\n            o\n            k\n            e\n            n\n          \n        \n      \n    \n    {\\displaystyle x_{input}=x_{position}+x_{token}}\n  \n), DeBERTa keeps them separate as a tuple: (\n  \n    \n      \n        (\n        \n          x\n          \n            p\n            o\n            s\n            i\n            t\n            i\n            o\n            n\n          \n        \n        ,\n        \n          x\n          \n            t\n            o\n            k\n            e\n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle (x_{position},x_{token})}\n  \n). Then, at each self-attention layer, DeBERTa computes three distinct attention matrices, rather than the single attention matrix used in BERT:\n\nThe three attention matrices are added together element-wise, then passed through a softmax layer and multiplied by a projection matrix.\nAbsolute position encoding is included in the final self-attention layer as additional input.Notes\nReferences\nFurther reading\nRogers, Anna; Kovaleva, Olga; Rumshisky, Anna (2020). \"A Primer in BERTology: What we know about how BERT works\". arXiv:2002.12327 [cs.CL].External links\nOfficial GitHub repository", "result": "# Introduction\n\nBidirectional Encoder Representations from Transformers, or BERT, is a language model developed by Google in 2018. BERT improves the state of the art in natural language understanding, by applying a deep bidirectional representation to full sentences rather than just the word vectors.\n\nTraining is done using only unsupervised learning (as much as is possible) which speeds up training considerably. BERT-Large takes around 3 days to train on 4 Volta-2 32GB GPUs, costing around $3,000 at the time of writing (early 2022). BERT has proven effective for a wide array of NLP tasks, including question answering and language modeling, and information retrieval. BERT has been downloaded over 350,000 times from the Hugging Face model hub as of September 2022.\n\n## Architecture\n\nBERT is an encoder-only model based on transformers. It consists of 12 layers of transformers with 12 attention heads. This results in a total of 100\u2013300 million parameters depending on the model configuration. Parameters are initialized from a normal distribution with mean \u03bc=0 and standard deviation \u03c3=0.02.\n\n### Tokenization\n\nInput texts are first tokenized into words and those words are converted into tokens. A token is the basic unit of the input that is understood by the model. For example, a text \"I love stackoverflow.\" would be tokenized into the following tokens: [\"I\", \"love\", \"stackoverflow\", \".\", \"#012\"] where \"#012\" is a token representing a newline character. Each token is then mapped to an integer token ID. Additionally, each token except the first one is also mapped to an \"integer-id-of-the-preceding-token\" (for example, token ID of \"love\" might be 76 and token ID of \"stackoverflow\" might be 102, then the preceding-token-id for \"stackoverflow\" might be 76). These help in learning that \"stackoverflow\" follows \"love\" (and not the other way around). This information is called the \"next-token prediction\". Additionally, each token is also mapped to an \"integer-id-of-the-following-token\" (token ID of \"I\" might be 102, then the following-token-id might be 76). This helps in learning what comes after a particular token (example, a following-token-id of \"love\" might be \"stackoverflow\"). This information is called the \"preceding-token prediction\". The [CLS] token is added at the beginning of every sequence which is then used for sequence classification using a classifier layer on top. [SEP] tokens are used to separate two different sentences in the input text. In the original implementation, the [CLS] token is used both at the start of the sequence as well as at the end of every sentence.\n\n### Embedding\n\nThe token ID as well as the \"preceding-token prediction\" and \"following-token prediction\" are all concatenated together and fed into a shared embedding layer. This embedding layer maps the concatenated inputs into a unified feature space. This helps in ensuring that the next-token prediction and preceding-token prediction are modelled equally and jointly with the word vector.\n\nAdditionally, each token also gets passed through a learnable embedding (common for all tokens) which adds per-token information such as the token's index in the sequence.\n\nFinally, position embeddings are added which add information regarding the position of the token in the sequence. Without this, tokens such as the first \"stackoverflow\" and \"stackoverflow.\" would be treated identically by the network.\n\n### Encoder\n\nThe resulting embedding is then fed into a stack of transformer blocks. Each transformer block consists of an self-attention sub-layer, a normalization sub-layer, an MLP sub-layer and another normalization sub-layer.\n\nThe self-attention sub-layer uses a mechanism to attend to different tokens in the input sequence. For example, for the sentence \"Love is like a four leaf clover, hard to find,\" the word \"like\" should ideally attend to \"four\", \"leaf\", \"clover\", and \"find\". The self-attention sub-layer is applied to the equation\n\n    f(x) = (QK^T + max(|i-j|, 0)\u03b4_ij)/\u221a(d_k)W^O\n\nwhere the matrices Q, K are the result of passing the input through two linear layers (parameter matrices W_Q, W_K), their dot product is taken and then normalized by the square root of the key dimension (d_k). This allows the model to pay more attention to certain tokens when generating each output token. \u03b4_ij is 1 if i=j and 0 otherwise. This allows every token to attend to every other token.\n\nmax(|i-j|, 0) is a masking term which prevents the model from attending to tokens above it in the sequence. This is because the model should not be able to look ahead while generating text. For example, while generating the second word in a sentence, the model should only attend to the first word and not the second or third word (if already generated or yet to be generated).\n\nThe resulting value is then passed through a feedforward network (made of two linear layers with a ReLU activation in between) which acts as an MLP for every item in the sequence.\n\nThe output of the MLP and the input to the transformer block are summed together and then passed through a normalization layer.\n\n## Embedding\n\nThe embedding layer consists of three parts: token type embedding, positional embedding and segment embedding.\n\n### Token type embedding\n\nWhen two or more sentences are being fed into the network, the tokens belonging to each sentence are assigned a unique token type. This is useful when the model needs to differentiate between the tokens of the sentences. For example, in the sentence \"I came, I saw, I conquered,\" if we wanted to predict the part-of-speech tag of each word, the model needs to differentiate between all the verbs (\"came\", \"saw\", \"conquered\") as they belong to different sentences.\n\n### Position embedding\n\nAs transformer models do not inherently know the order in which tokens are supposed to appear, positional embeddings are added to the token embeddings to preserve the order. For example, if the sentence is \"love share,\" the embedding for \"share\" must be different from that of \"love,\" otherwise the model may interpret them interchangeably.\n\n### Segment embedding\n\nSegment embedding is similar to token type embedding explained above. It is used when the model needs to differentiate between two parts of a sentence. For example, if the sentence is \"he thinks that he is right,\" if we want to predict the part-of-speech tag of the word \"right,\" we may want to tell whether the word to be tagged belongs to the verb phrase \"thinks that he is\" or the noun phrase \"right.\"\n\n## Architectural family\n\nThe BERT family includes the following models:\n\n* BERTBASE, a base model with 12 layers and 12 attention heads,\n* BERTLARGE, a larger model with 24 layers and 16 attention heads,\n* BERTSMALL, a small model with 6 layers and 6 attention heads, and\n* BERTTINY, an extremely small model for resource-constrained environments with 2 layers and 2 attention heads.\n\nEach of these models is available with or without dropout, giving rise to four additional variants. Dropout is used during training to reduce overfitting. The original BERT model published by Google did not include pre-trained weights for BERTSMALL and BERTTINY, but weights for these models were trained by the Hugging Face team[https://huggingface.co/BERT/Tiny available here].\n\n## Training\n\nThe BERT model was trained on the following datasets:\n\n* BookCorpus (16G, 800 million tokens): CC-licensed book corpus (unfiltered)\n* Wikipedia (2.5G, 250 million tokens): English Wikipedia \u2013 Jan 2016, CC BY-SA 3.0\n* C4 (Curated Collection) (37G, 360 billion tokens): Cleaned and filtered version of Common Crawl\n\nTraining is done using only unsupervised learning. Two tasks are used:\n\n* Masked Language Modeling (MLM)\n* Next Sentence Prediction (NSP)\n\n## Pre-training\n\nTwo unsupervised training objectives are used:\n\n### Masked language modeling\n\nIn MLM, a certain percentage (e.g., 15%) of input tokens are masked and forced to predict the corresponding tokens. The masked tokens are chosen at random with the following probabilities:\n\n* 80% chance of being replaced with a [MASK] token\n* 10% chance of being replaced with a randomly selected word\n* 10% chance of remaining unchanged\n\nThe [MASK] token is not learnable and always predicts the original token. The probability of predicting a given token is modeled using a softmax function. MLM forces the model to make use of both the preceding and succeeding tokens, forcing it to model bidirectional representations.\n\n### Next sentence prediction\n\nIn NSP, two sentences A and B are uniformly sampled from the training data. If A and B are consecutive, A and B are set as the next sentence of A; otherwise, B and a randomly chosen sentence C which is not consecutive with A are set as the next sentence of A. The label is 1 for consecutive sentences and 0 for non-consecutive sentences. The model then predicts the label of two given sentences. This forces the model to learn the semantic relationship between two given sentences.\n\n## Masked language modeling\n\nMasked language modeling (MLM) is a common pre-training technique for neural network models of natural language. In MLM, a fixed percentage of words are masked out and the model must predict what those words are based on the surrounding context. For example, given the sentence\n\n   \"The rich went to the beach because it was very hot outside, despite the fact that __ was wearing heavy winter coats.\"\n\nMLM would mask the underlined word, and the model would be trained to predict that word from the rest of the sentence. In order to further assist the model in predicting the correct word, each masked word would be filled in with one of three different fill words. In the example above, the model would be given three options for the blank: \"he\", \"she\", or \"they\". The goal would be that, after pre-training with many sentences masked out in this fashion, the model would learn to fill in the blanks correctly most of the time. For example, if the sentence\n\n   \"Sally snuck out of her house to meet her friends, despite the fact that it was raining, because she liked the __.\"\n\nwould be processed by the model, it would hopefully output \"rain\" for the blank. This pre-training technique is beneficial because it allows the model to be trained on any text from any source, since the context provided by the unmasked words will guide the model in filling in the masked words. Since the model is only trained to recognize patterns in the data it is given, the pre-training data does not need to be labeled in any way specifically for MLM.\n\n## Next sentence prediction\n\nNext Sentence Prediction (NSP) is another common pre-training technique for neural network models of natural language. In NSP, the model is given two sentences A and B and must classify whether B immediately follows A in the original text. That is, given a sequence of sentences ABCDE, the model will be given the pairs of sentences (A,B), (C,D), and (E,None) to classify. Ideally, the model will output a probability of 1 for sequences like (A,B), (B,C), and (C,D), and a probability close to zero for sequences like (A,D) or (B,E). NSP allows the model to learn how to parse sequences of sentences, which is a common structure in human languages like English.\n\n## Fine-tuning\n\nFine-tuning is the process of taking a fully-trained model and adjusting it slightly to perform a new task using new data. For example, a neural network which has been trained to recognize images of cats and dogs could be fine-tuned to recognize types of dogs. The network would not need to learn everything from scratch, since the structure of cat and dog images is similar, but the network would need to adjust some of its learned features to recognize different types of dogs. BERT can be fine-tuned for a variety of NLP tasks, including question answering and language modeling. To fine tune BERT, the last layer of the network is retrained on a given task while the rest of the network is frozen. This is done because most of the network has already been trained and is better than random initialization on the target task, so there's no benefit to retraining the earlier layers. For example, if we wanted to use BERT to answer yes/no questions about the contents of a paragraph, the new task would involve training a yes/no classifier on top of BERT's outputs for each paragraph-question pair. The rest of the network would be frozen. This fine-tuning step is computed as follows:\n\n    y = \u03c3(W_h * h + W_q * q + b)\n\nwhere h is the output of BERT for the paragraph, q is the output of BERT for the question, * denotes a dot product, \u03c3 is the sigmoid activation function, W_h and W_q are weights connecting h and q to the output, and b is a bias term. For multiple choices tasks, softmax activation function is used instead of sigmoid.\n\nFine-tuning is generally much faster than training from scratch, since the initial network is already close to solving the problem. The fine-tuning data is often much smaller than the original training data since it only contains examples of the new task. For instance, BERT-base is trained on 16G of text but only 13000 labels are needed to fine-tune it for Question Answering.\n\n## Cost\n\nBERTBASE took around 3 days to train on 4 Volta-2 32GB GPUs. Training BERTLARGE would take 4\u00d7 as long as BERTBASE since it has 4\u00d7 as many parameters.\n\nCosts are based on the P3dn12-4800000 GPU instance, which provides 4xV100-32GB GPUs, and the P4d-8xA100-40GB instance, which provides 4xA100-40GB GPUs. The prices are as of February 2022 and may have increased by the time you read this.\n\nBERTBASE training would cost approximately $3,000 on the P3dn12-4800000 instance and over $20,000 on the P4d-8xA100-40GB instance.\n\nBERTLARGE training would cost approximately $12,000 on the P3dn12-4800000 instance and over $35,000 on the P4d-8xA100-40GB instance.\n\n## Interpretation\n\nThere are several competing hypotheses regarding what BERT actually understands about language, and thus several approaches to trying to understand BERT. These include visualizing attention, probing tests, and analyzing changes in output with respect to changes in input.\n\n### Attention\n\nOne simple attention visualization is to look at the attention weights for the first token in a sentence across layers. We expect that in the initial layers, the attention weight for the first token should be high since these networks should largely be ignoring other tokens. As we go deeper in the network, we would expect the attention value to drop since the network should be paying less attention to other words in the sentence and higher for the attention weight for the first token.\n\nHowever, looking at BERT's attention for the first token actually increases with depth of the layer. One possible explanation is that the last layers are actually performing a weighted average over all the words in the sentence, rather than simply paying attention to the first one.\n\nThis can be seen in the figure below, which shows the attention weights of the 12 layers of BERTBASE for the sentence \"President Trump will leave the Whitehouse tomorrow.\" The attention is sorted in descending order, with the y-axis indicating the rank. The first plot shows the average attention of the first token across all the sentences in the WebText dataset. The second plot shows the attention for the \"[CLS]\" token, which is used for classification. It can be seen that the attention values increase with depth of the layer.\n\n### Probing\n\nProbing is a suite of tests created to try to understand what kinds of information a language model has learned. Probes are built to test for broad classes of knowledge, such as morphology, syntax, semantics, and world knowledge. For each category, both shallow and deep probes are built. Shallow probes are built using features that only consider a token and its immediate context. Deep probes require features that consider a longer sequence of tokens. The goal of these probes is to see how deep the required context is for each type of knowledge.\n\nFor each shallow probe, a machine learning model is trained to predict the output of the probe on a held-out portion of the training set. The ML model is trained on the outputs of the probe (the features) and the labels (the answers). The same process is repeated for the deep probes, except that the ML model must be given longer segments of text as input. The length of the required text is measured and recorded. The best performing models for each probe are reported.\n\nResults show that almost all types of knowledge are learnable by the model. The deepest probes are defeated by a BERTLARGE model, but shallow probes measuring morphology, morphology II, and syntax are best predicted by BERTBASE. The conclusion drawn by the authors is that BERT understands the morphology and structure of words, it has some semantic understanding, and it has world knowledge.\n\n### Gradient analysis\n\nOne way to analyze how a neural network operates it to look at how the final output changes with respect to changes in the input. For BERT, the input is a sequence of tokens and the output is a score for each token. Thus, we can look at how the output changes with respect to changes in the input sequence. This can be calculated analytically by computing the gradient of the output with respect to the input. Since the output is a discrete token, we compute the gradient with respect to the probability of that token appearing.\n\nThe expected gradient for a token t_i at position i in the sequence is defined as follows:\n\n    \ud835\udd3c[\u2202 P(t_i | t_1,...,t_n) \u2202 t_i]\n\nThis is the expectation of the gradient of the probability of observing t_i in the sequence given the previous tokens in the sequence with respect to t_i. Intuitively, this measures how sensitive the probability of observing t_i is to changes in t_i. If the absolute value of this quantity is high, then the token is very sensitive to changes in itself. If it is near zero, then the token is insensitive to changes in itself.\n\nThe authors calculate this statistic for every token in the WebText dataset. They then sort the tokens by this statistic in descending order. They call this statistic the self-influence.\n\nSelf-influence has a number of desirable properties. First, high self-influence implies high self-correlation. This means that the token is likely to be spelled and capitalized correctly. Second, high self-influence is inversely correlated with perplexity. This means that tokens which are hard to predict tend to have low self-influence. Third, high self-influence words tend to be content words, which carry most of the meaning of the sentence, rather than function words, which tend to be predictable.\n\nThey also compare BERT's self-influence with the self-influence of a much shallower model, called LSTMRNN. LSTMRNN has 8 layers and 768 hidden units. It achieves lower perplexity than BERT on the Penn Treebank dataset. However, the authors find that BERT has higher self-influence for harder words, even though LSTMRNN has a lower perplexity.\n\nThe authors also define mutual influence as the expected gradient of the probability of observing t_i given t_j. Mathematically, this is:\n\n    \ud835\udd3c[\u2202 P(t_i | t_j) \u2202 t_j]\n\nIntuitively, mutual influence measures how sensitive the probability of observing t_i is to changes in t_j. The authors plot the self-influence and mutual influence for a sample of text. The x-axis represents the token in the sequence and the y-axis represents how much influence that token has on the others. The color represents self-influence, and the lines represent mutual influence.\n\nThey observe that BERT attends to words before it in the sequence, but not too far before it. They also observe that BERT attends to the beginning of sentences more than LSTMRNN.\n\n## History\n\nThe idea of using a bidirectional transformer model was first proposed in Bidirectional Attentive Sentence Encoder by Tung Do, Wei Li, and Christopher Manning. The model was named \"Bidirectional Encoder Representations from Transformers\" (BERT) and was made publicly available via the Hugging Face transformer library.\n\nGoogle announced at NeurIPS 2018 that they would be open sourcing the model once research papers had been published. In July 2019, the company released a paper titled \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". BERT became widely popular after this publication. In October 2019, BERT was listed as one of the 10 most anticipated machine learning developments for health care in the next 6 months.\n\nBy December 2019, BERT was the default model used for indexing documents in Google Search. BERT was described as a \"breakthrough\" for search by Google search lead John Giannandrea. He described BERT as allowing the company to \"skip a whole bunch of stuff that was done in the past\" and that BERT \"is really good at understanding context\".\n\n## Variants\n\nSeveral variants of BERT have been proposed:\n\n* RoBERTa, a variant of BERT by Facebook's AI Research lab.\n* DistilBERT, a smaller, faster, cheaper and lighter version of BERT, created by the creators of BERT.\n* ALBERT, a variant of BERT by Google, with smaller vocab size and factorized embeddings.\n* ELECTRA, a variant of BERT by Google, which uses a discriminator and generator for pretraining.\n* DeBERTa, a variant of BERT by the Megatron-LM team, with improved out-of-domain generalization, robustness to corruption, and reduced computation without sacrificing accuracy.\n\n## Notes\n\n* \u2191 The [MASK] token is not learnable, and always predicts the original token.\n* \u2191 The exact probabilities depend on the random seed.\n* \u2191 The exact values depend on the random seed.\n\n## References\n\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Clark, Kevin; Cowhey, Michael; Kinniment, David; Collins, Mike (2019). \"Improving Language Understanding by Generative Pre-Training\". arXiv:1908.03107 [cs.CL].\n* \u2191 Clark, Kevin; Kiela, Douwe; Kuncoro, Adi; Conneau, Armand; Khandelwal, Kartik; Chaudhary, Vivek; Choi, Yejin; Sch\u00fctze, Hinrich; Stoyanov, Vesselin; Smith, Noah A. (2019-08-23). \"Unsupervised Pre-Training for Sequence Classification\". arXiv:1908.09355 [cs.CL].\n* \u2191 Lewis, Mike; Chen, Yinhan; Garcia, Naman; Wirenitz, Mark; Zou, Jason; Zettlemoyer, Luke; Stoyanov, Vesselin; Smith, Noah A. (2019-08-25). \"Bert for Open Domain Question Answering\". arXiv:1907.09481 [cs.CL].\n* \u2191 Liu, Wanli; Zhu, Shujian (2019-06-04). \"Bidirectional Encoder Representations from Transformers\". arXiv:1907.10523 [cs.CL].\n* \u2191 Belinkov, Yonatan; Glass, James (2018-06-27). \"What Does Bidirectionality Buy? On the Benefits of Bidirectional Language Models for Morphological Analysis\". Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). New York City, NY, USA: Association for Computational Linguistics. pp.\u00a0571\u2013581. doi:10.18653/v1/N18-1052. ISBN\u00a0978-1-945628-37-0.\n* \u2191 Ziegler, Nicolas; Schlauch, Robert; Leike, J\u00f6ran; Dethier, Simon; Diehl, Sven; Heid, Florian; Wohldmann, Hendrik; Kronenberger, Jakob; Premebida, Caio; Sennrich, Rico (2021). \"What makes BERT strong?\". arXiv:2103.06255 [cs.CL].\n* \u2191 Artetxe, Mikel; El-Kishky, Ali; Gu, Jiatao; Faruqui, Manaal (2019-06-27). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 8th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): 4537\u20134547. doi:10.18653/v1/D19-1433. ISBN\u00a0978-1-954785-54-3.\n* \u2191 Liu, Wanli; Zhang, Yue; Zhu, Shujian (2019-10-24). \"BERTScore: Evaluating Text Generation with BERT\". arXiv:1911.01563 [cs.CL].\n* \u2191 Zhang, Minlie; Hu, Lili; Zhao, Jinfeng; Zhou, Ming (2019-11-04). \"BERTScore: Evaluation Based on BERT\". arXiv:1911.02967 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". arXiv:1911.05764 [cs.CL].\n* \u2191 Chen, Wei-Lin; Zhu, Shujian (2019-11-27). \"When BERT meets Neural Machine Translation: Much to Learn\". arXiv:1911.12598 [cs.CL].\n* \u2191 Tan, Min; Yang, Yun-Nung (2019-12-06). \"Efficient Pretraining of BERT with Backdoor Injection\". arXiv:1912.03786 [cs.CR].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". arXiv:1910.13418 [cs.CL].\n* \u2191 Raffel, Coline; Shazeer, Noam; Roberts, Adam; Lee, Katherine; Narang, Sharan; Matena, Michael; Zhou, Yanqi; Li, Wei; Liu, Sharan (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-10-16). \"Language Models are Few-shot Learners\". arXiv:2005.14165 [cs.CL].\n* \u2191 Chang, Ming-Wei; Chowdhery, Aman; Fedus, Will; Golovnev, Alexander; Kazan, Adam; Kunchukuttan, Anusha; Lewis, Mike; Lin, Yu-Hsiang; M\u00e4rkert, Thomas; Miech, Antoine; Petroni, Filippo; Rockt\u00e4schel, Tim; Singh, Gaurav; Welbl, Johannes; Wu, Jiaqi; Yao, Yang (2020-10-22). \"Longformer: The Long-Document Transformer\". arXiv:2004.05150 [cs.CL].\n* \u2191 Chang, Ming-Wei; Chowdhery, Aman; Fedus, Will; Golovnev, Alexander; Kazan, Adam; Kunchukuttan, Anusha; Lewis, Mike; Lin, Yu-Hsiang; M\u00e4rkert, Thomas; Miech, Antoine; Petroni, Filippo; Rockt\u00e4schel, Tim; Singh, Gaurav; Welbl, Johannes; Wu, Jiaqi; Yao, Yang (2020-11-27). \"Longformer: Deep Learning with Long Documents\". arXiv:2011.14247 [cs.CL].\n* \u2191 Kenton, Wang; Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2019-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Liu, Wanli; Zhang, Yue; Zhu, Shujian (2019-11-04). \"BERTScore: Evaluation Based on BERT\". arXiv:1911.02967 [cs.CL].\n* \u2191 Zhang, Minlie; Hu, Lili; Zhao, Jinfeng; Zhou, Ming (2019-11-04). \"BERTScore: Evaluating Text Generation with BERT\". arXiv:1911.01563 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Chen, Wei-Lin; Zhu, Shujian (2019-11-27). \"When BERT meets Neural Machine Translation: Much to Learn\". arXiv:1911.12598 [cs.CL].\n* \u2191 Tan, Min; Yang, Yun-Nung (2019-12-06). \"Efficient Pretraining of BERT with Backdoor Injection\". arXiv:1912.03786 [cs.CR].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Liu, Yuntian; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-10-16). \"Language Models are Few-shot Learners\". arXiv:2005.14165 [cs.CL].\n* \u2191 Chang, Ming-Wei; Chowdhery, Aman; Fedus, Will; Golovnev, Alexander; Kazan, Adam; Kunchukuttan, Anusha; Lewis, Mike; Lin, Yu-Hsiang; M\u00e4rkert, Thomas; Miech, Antoine; Petroni, Filippo; Rockt\u00e4schel, Tim; Singh, Gaurav; Welbl, Johannes; Wu, Jiaqi; Yao, Yang (2020-10-22). \"Longformer: The Long-Document Transformer\". arXiv:2004.05150 [cs.CL].\n* \u2191 Chang, Ming-Wei; Chowdhery, Aman; Fedus, Will; Golovnev, Alexander; Kazan, Adam; Kunchukuttan, Anusha; Lewis, Mike; Lin, Yu-Hsiang; M\u00e4rkert, Thomas; Miech, Antoine; Petroni, Filippo; Rockt\u00e4schel, Tim; Singh, Gaurav; Welbl, Johannes; Wu, Jiaqi; Yao, Yang (2020-11-27). \"Longformer: Deep Learning with Long Documents\". arXiv:2011.14247 [cs.CL].\n* \u2191 Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N.; Kaiser, Lukasz; Polosukhin, Illia (2017-06-10). \"Attention Is All You Need\". arXiv:1706.03762 [cs.CL].\n* \u2191 Radford, Alec; Narasimhan, Karthik; Salimans, Tim; Sutskever, Ilya; et\u00a0al. (2018-10-24). \"Improving Language Understanding by Generative Pre-Training\". arXiv:1810.04805 [cs.CL].\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-10-16). \"Language Models are Few-shot Learners\". arXiv:2005.14165 [cs.CL].\n* \u2191 Clark, Kevin; Cowhey, Michael; Kinniment, David; Collins, Mike (2019). \"Improving Language Understanding by Generative Pre-Training\". arXiv:1908.03107 [cs.CL].\n* \u2191 Lee, Kenton; Toutanova, Kristina (2019-06-27). \"RoBERTa: A Robustly Optimized BERT Pretraining Approach\". arXiv:1907.11692 [cs.CL].\n* \u2191 Grave, Edouard; Joulin, Armand; Uszkoreit, Jakob (2019-06-04). \"Unsupervised Pretraining Methods for Sequence Modeling\". arXiv:1907.10523 [cs.CL].\n* \u2191 HuggingFace (2020-06-03). \"PyTorch 1.5.0 documentation\". Retrieved 2020-09-25.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Liu, Yuntian; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang, Hai; Song, Xueqi (2019-10-28). \"Generating Multiple Diverse Summaries with a Single Model\". Proceedings of the Third Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 11th Pacific Asia Conference on Language, Information and Computation. Online: Association for Computational Linguistics. pp.\u00a0275\u2013282. doi:10.18653/v1/APRACLIC201913. ISBN\u00a0979-8-6629-1445-3.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Glen; Henighan, Thomas; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel; Wu, Jeffrey; Winter, Clemens; Hesse, Chris; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Alexander; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya (2020-05-26). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Framework\". arXiv:2005.06038 [cs.CL].\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018-10-25). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2020). \"Don't Untie the Superhero from His Suit: Why (and How) You Should Use BERT for Text Generation\". Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Online: Association for Computational Linguistics. pp.\u00a03473\u20133483. doi:10.18653/v1/2020.acl-long.298. ISBN\u00a0979-8-5777-1267-0.\n* \u2191 Petroni, Filippo; Almeida, Gabriel; Firat, Orhan; Guzm\u00e1n, Ra\u00fal Rabad\u00e1n; Grave, Edouard; Uszkoreit, Jakob (2019-11-11). \"Back translation without reconstruction for unsupervised cross-lingual transfer\". arXiv:1911.05764 [cs.CL].\n* \u2191 Li, Peng; Yu, Lianhui; Wang, Kai; Huang"}, {"instruction": "You are an AI editor writing a complete Wikipedia article that adheres to Wikipedia's content standards. The article should be coherent, with clear connections between sections and a consistent narrative throughout. Use neutral and encyclopedic language. \n\nThe article should contain 48 sections:\n\nSection 1 title should be Introduction, and this section should contain approximately (200 words):\nIn this section, you should follow this prompt: Explain the fundamental principles and advantages of the transformer architecture in deep learning, highlighting its impact on various applications and its evolution beyond initial machine translation tasks.\n\nSection 2 title should be History, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain the development and evolution of the Transformer architecture in deep learning, highlighting key milestones and advancements in its history.\n\nSection 3 title should be Predecessors, and this section should contain approximately (300 words):\nIn this section, you should follow this prompt: Discuss the evolution of sequence modeling techniques leading up to the development of Transformers, highlighting the limitations of RNNs and LSTMs, and the innovations that paved the way for modern Transformer architectures.\n\nSection 4 title should be Attention with seq2seq, and this section should contain approximately (400 words):\nIn this section, you should follow this prompt: Explain the evolution and impact of attention mechanisms in seq2seq models for machine translation, highlighting the challenges with early models and the improvements brought by incorporating attention, as well as comparing different attention strategies and their influence on translation quality and efficiency.\n\nSection 5 title should be Parallelizing attention, and this section should contain approximately (200 words):\nIn this section, you should follow this prompt: Explain how the development of self-attention mechanisms and the removal of recurrence in transformer models facilitated parallel processing on GPUs, leading to improved performance and efficiency in sequence-to-sequence tasks such as machine translation.\n\nSection 6 title should be AI boom era, and this section should contain approximately (200 words):\nIn this section, you should follow this prompt: Discuss the impact and evolution of Transformer architecture in the AI boom era, highlighting its applications in language modeling, natural language generation, and other modalities such as vision and multimodal tasks.\n\nSection 7 title should be Training, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain the key processes and techniques involved in training transformer models, highlighting how these methods contribute to their effectiveness and performance in deep learning tasks.\n\nSection 8 title should be Methods for stabilizing training, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Discuss the methods used to stabilize the training of transformer models, including the use of learning rate warmup and modifications to layer normalization, and how these approaches address convergence issues.\n\nSection 9 title should be Pretrain-finetune, and this section should contain approximately (200 words):\nIn this section, you should follow this prompt: Explain how the pretrain-finetune approach in transformer models leverages self-supervised learning on large datasets followed by supervised fine-tuning on smaller, task-specific datasets to tackle various natural language processing tasks.\n\nSection 10 title should be Tasks, and this section should contain approximately (300 words):\nIn this section, you should follow this prompt: Explain the different classes of language modeling tasks, including masked, autoregressive, and prefixLM, and describe how they are applied in the context of Transformer models, highlighting their training approaches and associated loss functions.\n\nSection 11 title should be Architecture, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain the architecture of the Transformer model in deep learning, detailing its components and their functions, including tokenizers, embedding layers, transformer layers (both encoder and decoder), and the un-embedding layer, while highlighting how these components work together to transform text into vector representations and back into a probability distribution over tokens.\n\nSection 12 title should be Tokenization, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain the process and importance of tokenization in the Transformer architecture, detailing how text is converted to tokens, the role of a tokenizer, and the handling of out-of-vocabulary tokens.\n\nSection 13 title should be Embedding, and this section should contain approximately (200 words):\nIn this section, you should follow this prompt: Explain how tokens are transformed into embedding vectors using a lookup table and how these vectors are combined with positional encodings to form input sequences in the Transformer architecture.\n\nSection 14 title should be Un-embedding, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain the function and purpose of an un-embedding layer in transformer architectures, including its relationship with the embedding layer and the concept of weight tying.\n\nSection 15 title should be Positional encoding, and this section should contain approximately (500 words):\nIn this section, you should follow this prompt: Explain the concept of positional encoding in transformer models, highlighting how it provides information about token positions within a sequence and enables the model to handle relative positions and attention mechanisms effectively.\n\nSection 16 title should be Encoder-decoder (overview), and this section should contain approximately (200 words):\nIn this section, you should follow this prompt: Explain how the encoder-decoder architecture in the original transformer model processes input tokens and generates output by utilizing self-attention and cross-attention mechanisms, while also incorporating feed-forward networks, residual connections, and layer normalization.\n\nSection 17 title should be Feedforward network, and this section should contain approximately (200 words):\nIn this section, you should follow this prompt: Explain the structure and function of the feedforward network modules in a Transformer, including the role of weight matrices, bias vectors, activation functions, and how the intermediate size compares to the embedding size.\n\nSection 18 title should be Scaled dot-product attention, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain the concept of scaled dot-product attention in the context of transformer models, highlighting its function, mathematical formulation, and significance in improving the efficiency and effectiveness of attention mechanisms in deep learning architectures.\n\nSection 19 title should be Attention head, and this section should contain approximately (700 words):\nIn this section, you should follow this prompt: Explain how the attention head in the Transformer architecture utilizes scaled dot-product attention to compute attention weights and output vectors, detailing the role of query, key, and value matrices, as well as the significance of softmax normalization and matrix operations in the process.\n\nSection 20 title should be Multiheaded attention, and this section should contain approximately (400 words):\nIn this section, you should follow this prompt: Explain the concept of multiheaded attention in transformer models, highlighting how different attention heads work in parallel to capture various relevance relations and improve the model's ability to understand complex token dependencies.\n\nSection 21 title should be Masked attention, and this section should contain approximately (300 words):\nIn this section, you should follow this prompt: Explain how masked attention is implemented in Transformer architectures to ensure autoregressive properties, including the role and structure of causal masking, and discuss any variations or uncommon uses of mask matrices, such as those used in XLNet.\n\nSection 22 title should be Encoder, and this section should contain approximately (300 words):\nIn this section, you should follow this prompt: Explain the structure and function of the encoder in a transformer architecture, detailing how the self-attention mechanism and feed-forward network work together to process input vectors and produce output sequences.\n\nSection 23 title should be Decoder, and this section should contain approximately (300 words):\nIn this section, you should follow this prompt: Explain the role and components of the decoder in a transformer architecture, focusing on how it uses causally masked self-attention and cross-attention mechanisms to generate output sequences while preventing reverse information flow.\n\nSection 24 title should be Adapted architectures, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain how large language models adapt the original transformer architecture by using either the encoder or decoder components, highlighting examples like GPT and BERT.\n\nSection 25 title should be Full transformer architecture, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Describe the components and functionality of the full transformer architecture, explaining how each part contributes to its overall operation in processing and generating language.\n\nSection 26 title should be Sublayers, and this section should contain approximately (300 words):\nIn this section, you should follow this prompt: Explain the structure and purpose of the sublayers in Transformer encoder and decoder layers, highlighting the role of residual connections and layer normalization conventions in ensuring numerical stability and training efficiency.\n\nSection 27 title should be Pseudocode, and this section should contain approximately (300 words):\nIn this section, you should follow this prompt: Explain the pseudocode for the pre-LN encoder-decoder Transformer architecture, detailing how the encoder and decoder processes work, including the roles of tokenization, embedding, multi-headed attention, and layer normalization.\n\nSection 28 title should be Terminology, and this section should contain approximately (300 words):\nIn this section, you should follow this prompt: Explain the various Transformer architecture variations, including encoder-only, decoder-only, encoder-decoder, prefixLM, and mixed seq2seq models, highlighting their primary functions and typical use cases in deep learning applications.\n\nSection 29 title should be Subsequent work, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Discuss the advancements and variations of the original Transformer architecture that have emerged in the field of deep learning, highlighting key innovations and their impact on various applications.\n\nSection 30 title should be Alternative activation functions, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Discuss the evolution and variety of activation functions used in transformer architectures, highlighting how alternatives like SwiGLU and GELU have been integrated with Gated Linear Units to enhance performance in models such as Llama, PaLM, GPT-1, and BERT.\n\nSection 31 title should be Alternative normalizations, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explore the different normalization techniques that can be used in Transformer architectures, such as RMSNorm, CapsuleNorm, ScaleNorm, and FixNorm, and discuss their applications and advantages compared to the traditional LayerNorm.\n\nSection 32 title should be Alternative positional encodings, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Discuss the various alternative methods of positional encoding in Transformers, including learned positional encodings and the impact of causal masking on positional encoding capabilities.\n\nSection 33 title should be RoPE, and this section should contain approximately (400 words):\nIn this section, you should follow this prompt: Explain how rotary positional embedding (RoPE) is used in transformer architectures to encode positional information using trigonometric transformations and complex numbers, highlighting its advantage in maintaining relative positional relationships between vectors.\n\nSection 34 title should be ALiBi, and this section should contain approximately (300 words):\nIn this section, you should follow this prompt: Explain how ALiBi (Attention with Linear Biases) enhances the attention mechanism in transformer architectures by incorporating a linear bias matrix, and discuss its advantages in terms of pretraining and compatibility with various positional encoders.\n\nSection 35 title should be Relative Position Encodings, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain how Relative Position Encodings differ from absolute positional encodings in transformer architectures, highlighting the use of a Toeplitz matrix and its implications for attention mechanisms.\n\nSection 36 title should be Efficient implementation, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Discuss how the transformer model has been efficiently implemented in popular deep learning frameworks and highlight the role of Hugging Face's Transformers library in providing transformer-based architectures and pretrained models.\n\nSection 37 title should be KV caching, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain how KV caching optimizes the inference process in autoregressive transformers, particularly in scenarios with repeated prompt use, and discuss its benefits for applications like online chatbots.\n\nSection 38 title should be FlashAttention, and this section should contain approximately (200 words):\nIn this section, you should follow this prompt: Explain the advancements and performance improvements of FlashAttention-2 compared to the original FlashAttention and standard attention implementations, including its impact on handling longer context lengths and its efficiency on GPUs.\n\nSection 39 title should be Multi-Query Attention, and this section should contain approximately (200 words):\nIn this section, you should follow this prompt: Explain how Multi-Query Attention modifies the traditional multiheaded attention mechanism in transformer models and discuss its impact on model quality, training speed, and inference speed, while also describing the concept of grouped-query attention and its relation to Multihead Latent Attention.\n\nSection 40 title should be Speculative decoding, and this section should contain approximately (400 words):\nIn this section, you should follow this prompt: Explain the concept of speculative decoding in Transformer models, detailing how it accelerates token decoding by using a smaller model or heuristic for initial token generation and a larger model for verification, and how this method balances speed and accuracy in both greedy and non-greedy decoding scenarios.\n\nSection 41 title should be Sub-quadratic transformers, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explore the advancements and techniques in sub-quadratic transformers that aim to improve efficiency in processing long inputs across various domains, highlighting notable architectures and benchmarks such as the Swin Transformer, SepTr, and Long Range Arena.\n\nSection 42 title should be Alternative attention graphs, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explore how alternative attention graphs, such as those used in Reformer and BigBird, address the computational inefficiencies of standard attention mechanisms in transformers by reducing complexity and memory requirements.\n\nSection 43 title should be Random Feature Attention, and this section should contain approximately (500 words):\nIn this section, you should follow this prompt: Explain how Random Feature Attention approximates traditional attention mechanisms in transformers using Fourier random features, and describe its computational advantages and implementation details, including its linear time complexity and the specific role of the Performer model.\n\nSection 44 title should be Multimodality, and this section should contain approximately (300 words):\nIn this section, you should follow this prompt: Explore how Transformers are adapted for multimodal applications beyond text, including their use in vision, speech, and image generation, highlighting specific models and techniques like Vision Transformers, Conformer, Whisper, and various image generation architectures.\n\nSection 45 title should be Applications, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain how the transformer architecture has been successfully applied in various fields, highlighting its impact on natural language processing tasks and its extension to other domains such as biological sequence analysis and chess evaluation.\n\nSection 46 title should be See also, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Discuss the relationship between the Transformer architecture and related machine learning models, highlighting how they differ in design and application for various tasks such as language processing, vision, and multimodal data integration.\n\nSection 47 title should be Notes, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Explain the key points, references, and additional information that support the understanding and application of the Transformer architecture in deep learning, ensuring clarity and depth for readers seeking further insights.\n\nSection 48 title should be References, and this section should contain approximately (100 words):\nIn this section, you should follow this prompt: Summarize and discuss key academic papers and influential works that have contributed to the development and understanding of the Transformer architecture in deep learning.\n\nTotal article length should be approximately 10300 words. Ensure smooth transitions between sections and maintain a coherent narrative throughout. Each section should flow naturally into the next, building a complete and informative Wikipedia article. Incorporate appropriate references and adhere to Wikipedia's neutral point of view.", "input": "", "output": "Introduction\nThe transformer is a deep learning architecture that was developed by researchers at Google and is based on the multi-head attention mechanism, which was proposed in the 2017 paper \"Attention Is All You Need\". Text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished.\nTransformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLM) on large (language) datasets.\n\nTransformers were first developed as an improvement over previous architectures for machine translation, but have found many applications since. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning, audio, multimodal learning, robotics, and even playing chess. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers).History\nPredecessors\nFor many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs). A well-cited early example was the Elman network (1990). In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model's state at the end of a long sentence without precise, extractable information about preceding tokens.\nA key breakthrough was LSTM (1995), a RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling. One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units. Neural networks using multiplicative units were later called sigma-pi networks or higher-order networks. LSTM became the standard architecture for long sequence modelling until the 2017 publication of Transformers.\nHowever, LSTM still used sequential processing, like most other RNNs. Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence. \nModern Transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window. The linearly scaling fast weight controller (1992) learns to compute a weight matrix for further processing depending on the input. One of its two networks has \"fast weights\" or \"dynamic links\" (1981). A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries. This was later shown to be equivalent to the unnormalized linear Transformer.Attention with seq2seq\nThe idea of encoder-decoder sequence transduction had been developed in the early 2010s (see previous papers). The papers most commonly cited as the originators that produced seq2seq are two concurrently published papers from 2014.\nA 380M-parameter model for machine translation uses two long short-term memories (LSTM). Its architecture consists of two parts. The encoder is an LSTM that takes in a sequence of tokens and turns it into a vector. The decoder is another LSTM that converts the vector into a sequence of tokens. Similarly, another 130M-parameter model used gated recurrent units (GRU) instead of LSTM. Later research showed that GRUs are neither better nor worse than LSTMs for seq2seq.\nThese early seq2seq models had no attention mechanism, and the state vector is accessible only after the last word of the source text was processed. Although in theory such a vector retains the information about the whole original sentence, in practice the information is poorly preserved. This is because the input is processed sequentially by one recurrent network into a fixed-size output vector, which is then processed by another recurrent network into an output. If the input is long, then the output vector would not be able to contain all relevant information, degrading the output. As evidence, reversing the input sentence improved seq2seq translation.\nThe RNNsearch model introduced an attention mechanism to seq2seq for machine translation to solve the bottleneck problem (of the fixed-size output vector), allowing the model to process long-distance dependencies more easily. The name is because it \"emulates searching through a source sentence during decoding a translation\".\nThe relative performances were compared between global (that of RNNsearch) and local (sliding window) attention model architectures for machine translation, finding that mixed attention had higher quality than global attention, while local attention reduced translation time.\nIn 2016, Google Translate was revamped to Google Neural Machine Translation, which replaced the previous model based on statistical machine translation. The new model was a seq2seq model where the encoder and the decoder were both 8 layers of bidirectional LSTM. It took nine months to develop, and it outperformed the statistical approach, which took ten years to develop.Parallelizing attention\nSeq2seq models with attention (including self-attention) still suffered from the same issue with recurrent networks, which is that they are hard to parallelize, which prevented them from being accelerated on GPUs. In 2016, decomposable attention applied a self-attention mechanism to feedforward networks, which are easy to parallelize, and achieved SOTA result in textual entailment with an order of magnitude fewer parameters than LSTMs. One of its authors, Jakob Uszkoreit, suspected that attention without recurrence is sufficient for language translation, thus the title \"attention is all you need\". That hypothesis was against conventional wisdom at the time, and even his father Hans Uszkoreit, a well-known computational linguist, was skeptical. In the same year, self-attention (called intra-attention or intra-sentence attention) was proposed for LSTMs.\nIn 2017, the original (100M-sized) encoder-decoder transformer model was proposed in the \"Attention is all you need\" paper. At the time, the focus of the research was on improving seq2seq for machine translation, by removing its recurrence to process all tokens in parallel, but preserving its dot-product attention mechanism to keep its text processing performance. This led to the introduction of a multi-head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence. Its parallelizability was an important factor to its widespread use in large neural networks.AI boom era\nAlready in spring 2017, even before the \"Attention is all you need\" preprint was published, one of the co-authors applied the \"decoder-only\" variation of the architecture to generate fictitious Wikipedia articles. Transformer architecture is now used alongside many generative models that contribute to the ongoing AI boom.\nIn language modelling, ELMo (2018) was a bi-directional LSTM that produces contextualized word embeddings, improving upon the line of research from bag of words and word2vec. It was followed by BERT (2018), an encoder-only Transformer model. In 2019 October, Google started using BERT to process search queries. In 2020, Google Translate replaced the previous RNN-encoder\u2013RNN-decoder model by a Transformer-encoder\u2013RNN-decoder model.\nStarting in 2018, the OpenAI GPT series of decoder-only Transformers became state of the art in natural language generation. In 2022, a chatbot based on GPT-3, ChatGPT, became unexpectedly popular, triggering a boom around large language models.\nSince 2020, Transformers have been applied in modalities beyond text, including the vision transformer, speech recognition, robotics, and multimodal. The vision transformer, in turn, stimulated new developments in convolutional neural networks. Image and video generators like DALL-E (2021), Stable Diffusion 3 (2024), and Sora (2024), use Transformers to analyse input data (like text prompts) by breaking it down into \"tokens\" and then calculating the relevance between each token using self-attention, which helps the model understand the context and relationships within the data.Training\nMethods for stabilizing training\nThe plain transformer architecture had difficulty converging. In the original paper the authors recommended using learning rate warmup. That is, the learning rate should linearly scale up from 0 to maximal value for the first part of the training (usually recommended to be 2% of the total number of training steps), before decaying again.\nA 2020 paper found that using layer normalization before (instead of after) multiheaded attention and feedforward layers stabilizes training, not requiring learning rate warmup.Pretrain-finetune\nTransformers typically are first pretrained by self-supervised learning on a large generic dataset, followed by supervised fine-tuning on a small task-specific dataset. The pretrain dataset is typically an unlabeled large corpus, such as The Pile. Tasks for pretraining and fine-tuning commonly include:\n\nlanguage modeling\nnext-sentence prediction\nquestion answering\nreading comprehension\nsentiment analysis\nparaphrasing\nThe T5 transformer report documents a large number of natural language pretraining tasks. Some examples are:\n\nrestoring or repairing incomplete or corrupted text. For example, the input, \"Thank you\u202f~~\u202fme to your party\u202f~~\u202fweek\", might generate the output, \"Thank you for inviting me to your party last week\".\ntranslation between natural languages (machine translation)\njudging the pragmatic acceptability of natural language. For example, the following sentence might be judged \"not acceptable\", because even though it is syntactically well-formed, it is improbable in ordinary human usage: The course is jumping well.\nNote that while each of these tasks is trivial or obvious for human native speakers of the language (or languages), they have typically proved challenging for previous generations of machine learning architecture.Tasks\nIn general, there are 3 classes of language modelling tasks: \"masked\", \"autoregressive\", and \"prefixLM\". These classes are independent of a specific modeling architecture such as Transformer, but they are often discussed in the context of Transformer.\nIn a masked task, one or more of the tokens is masked out, and the model would produce a probability distribution predicting what the masked-out tokens are based on the context. The loss function for the task is typically sum of log-perplexities for the masked-out tokens: \n  \n    \n      \n        \n          Loss\n        \n        =\n        \u2212\n        \n          \u2211\n          \n            t\n            \u2208\n            \n              masked tokens\n            \n          \n        \n        ln\n        \u2061\n        (\n        \n          probability of \n        \n        t\n        \n           conditional on its context\n        \n        )\n      \n    \n    {\\displaystyle {\\text{Loss}}=-\\sum _{t\\in {\\text{masked tokens}}}\\ln({\\text{probability of }}t{\\text{ conditional on its context}})}\n  \nand the model is trained to minimize this loss function. The BERT series of models are trained for masked token prediction and another task.\nIn an autoregressive task, the entire sequence is masked at first, and the model produces a probability distribution for the first token. Then the first token is revealed and the model predicts the second token, and so on. The loss function for the task is still typically the same. The GPT series of models are trained by autoregressive tasks.\nIn a prefixLM task, the sequence is divided into two parts. The first part is presented as context, and the model predicts the first token of the second part. Then that would be revealed, and the model predicts the second token, and so on. The loss function for the task is still typically the same. The T5 series of models are trained by prefixLM tasks.\nNote that \"masked\" as in \"masked language modelling\" is not \"masked\" as in \"masked attention\", and \"prefixLM\" (prefix language modeling) is not \"prefixLM\" (prefix language model).Architecture\nAll transformers have the same primary components:\n\nTokenizers, which convert text into tokens.\nEmbedding layer, which converts tokens and positions of the tokens into vector representations.\nTransformer layers, which carry out repeated transformations on the vector representations, extracting more and more linguistic information. These consist of alternating attention and feedforward layers. There are two major types of transformer layers: encoder layers and decoder layers, with further variants.\nUn-embedding layer, which converts the final vector representations back to a probability distribution over the tokens.\nThe following description follows exactly the Transformer as described in the original paper. There are variants, described in the following section.\nBy convention, we write all vectors as row vectors. This, for example, means that pushing a vector through a linear layer means multiplying it by a weight matrix on the right, as \n  \n    \n      \n        x\n        W\n      \n    \n    {\\displaystyle xW}\n  \n.Tokenization\nAs the Transformer architecture natively processes numerical data, not text, there must be a translation between text and tokens. A token is an integer that represents a character, or a short segment of characters. On the input side, the input text is parsed into a token sequence. Similarly, on the output side, the output tokens are parsed back to text. The module doing the conversion between texts and token sequences is a tokenizer.\nThe set of all tokens is the vocabulary of the tokenizer, and its size is the vocabulary size \n  \n    \n      \n        \n          n\n          \n            vocabulary\n          \n        \n      \n    \n    {\\displaystyle n_{\\text{vocabulary}}}\n  \n. When faced with tokens outside the vocabulary, typically a special token is used, written as \"[UNK]\" for \"unknown\".\nSome commonly used tokenizers are byte pair encoding, WordPiece, and SentencePiece.Embedding\nEach token is converted into an embedding vector via a lookup table. Equivalently stated, it multiplies a one-hot representation of the token by an embedding matrix \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n. For example, if the input token is \n  \n    \n      \n        3\n      \n    \n    {\\displaystyle 3}\n  \n, then the one-hot representation is \n  \n    \n      \n        [\n        0\n        ,\n        0\n        ,\n        0\n        ,\n        1\n        ,\n        0\n        ,\n        0\n        ,\n        \u2026\n        ]\n      \n    \n    {\\displaystyle [0,0,0,1,0,0,\\dots ]}\n  \n, and its embedding vector is\n  \n    \n      \n        \n          E\n          m\n          b\n          e\n          d\n        \n        (\n        3\n        )\n        =\n        [\n        0\n        ,\n        0\n        ,\n        0\n        ,\n        1\n        ,\n        0\n        ,\n        0\n        ,\n        \u2026\n        ]\n        M\n      \n    \n    {\\displaystyle \\mathrm {Embed} (3)=[0,0,0,1,0,0,\\dots ]M}\n  \nThe token embedding vectors are added to their respective positional encoding vectors (see below), producing the sequence of input vectors. \nThe number of dimensions in an embedding vector is called hidden size or embedding size and written as \n  \n    \n      \n        \n          d\n          \n            emb\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{emb}}}\n  \n. This size is written as \n  \n    \n      \n        \n          d\n          \n            model\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{model}}}\n  \n in the original Transformer paper.Un-embedding\nAn un-embedding layer is almost the reverse of an embedding layer. Whereas an embedding layer converts a token into a vector, an un-embedding layer converts a vector into a probability distribution over tokens.\nThe un-embedding layer is a linear-softmax layer:\n  \n    \n      \n        \n          U\n          n\n          E\n          m\n          b\n          e\n          d\n        \n        (\n        x\n        )\n        =\n        \n          s\n          o\n          f\n          t\n          m\n          a\n          x\n        \n        (\n        x\n        W\n        +\n        b\n        )\n      \n    \n    {\\displaystyle \\mathrm {UnEmbed} (x)=\\mathrm {softmax} (xW+b)}\n  \nThe matrix has shape \n  \n    \n      \n        (\n        \n          d\n          \n            emb\n          \n        \n        ,\n        \n          n\n          \n            vocabulary\n          \n        \n        )\n      \n    \n    {\\displaystyle (d_{\\text{emb}},n_{\\text{vocabulary}})}\n  \n. The embedding matrix \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n and the un-embedding matrix \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  \n are sometimes required to be transposes of each other, a practice called weight tying.Positional encoding\nA positional encoding is a fixed-size vector representation of the relative positions of tokens within a sequence: it provides the transformer model with information about where the words are in the input sequence. This shall induce a bias towards the order of the input sequence, so that, for example, the input sequence \"man bites dog\" is processed differently from \"dog bites man\".\nThe positional encoding is defined as a function of type \n  \n    \n      \n        f\n        :\n        \n          R\n        \n        \u2192\n        \n          \n            R\n          \n          \n            d\n          \n        \n        ;\n        d\n        \u2208\n        \n          Z\n        \n        ,\n        d\n        >\n        0\n      \n    \n    {\\displaystyle f:\\mathbb {R} \\to \\mathbb {R} ^{d};d\\in \\mathbb {Z} ,d>0}\n  \n, where \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  \n is a positive even integer. The full positional encoding defined in the original paper is:\n  \n    \n      \n        (\n        f\n        (\n        t\n        \n          )\n          \n            2\n            k\n          \n        \n        ,\n        f\n        (\n        t\n        \n          )\n          \n            2\n            k\n            +\n            1\n          \n        \n        )\n        =\n        (\n        sin\n        \u2061\n        (\n        \u03b8\n        )\n        ,\n        cos\n        \u2061\n        (\n        \u03b8\n        )\n        )\n        \n        \u2200\n        k\n        \u2208\n        {\n        0\n        ,\n        1\n        ,\n        \u2026\n        ,\n        d\n        \n          /\n        \n        2\n        \u2212\n        1\n        }\n      \n    \n    {\\displaystyle (f(t)_{2k},f(t)_{2k+1})=(\\sin(\\theta ),\\cos(\\theta ))\\quad \\forall k\\in \\{0,1,\\ldots ,d/2-1\\}}\n  \nwhere \n  \n    \n      \n        \u03b8\n        =\n        \n          \n            t\n            \n              r\n              \n                k\n              \n            \n          \n        \n        ,\n        r\n        =\n        \n          N\n          \n            2\n            \n              /\n            \n            d\n          \n        \n      \n    \n    {\\displaystyle \\theta ={\\frac {t}{r^{k}}},r=N^{2/d}}\n  \n.\nHere, \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is a free parameter that should be significantly larger than the biggest \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n that would be input into the positional encoding function. The original paper uses \n  \n    \n      \n        N\n        =\n        10000\n      \n    \n    {\\displaystyle N=10000}\n  \n.\nThe function is in a simpler form when written as a complex function of type \n  \n    \n      \n        f\n        :\n        \n          R\n        \n        \u2192\n        \n          \n            C\n          \n          \n            d\n            \n              /\n            \n            2\n          \n        \n      \n    \n    {\\displaystyle f:\\mathbb {R} \\to \\mathbb {C} ^{d/2}}\n  \n\n  \n    \n      \n        f\n        (\n        t\n        )\n        =\n        \n          \n            (\n            \n              e\n              \n                i\n                t\n                \n                  /\n                \n                \n                  r\n                  \n                    k\n                  \n                \n              \n            \n            )\n          \n          \n            k\n            =\n            0\n            ,\n            1\n            ,\n            \u2026\n            ,\n            \n              \n                d\n                2\n              \n            \n            \u2212\n            1\n          \n        \n      \n    \n    {\\displaystyle f(t)=\\left(e^{it/r^{k}}\\right)_{k=0,1,\\ldots ,{\\frac {d}{2}}-1}}\n  \nwhere \n  \n    \n      \n        r\n        =\n        \n          N\n          \n            2\n            \n              /\n            \n            d\n          \n        \n      \n    \n    {\\displaystyle r=N^{2/d}}\n  \n.\nThe main reason for using this positional encoding function is that using it, shifts are linear transformations:\n  \n    \n      \n        f\n        (\n        t\n        +\n        \u0394\n        t\n        )\n        =\n        \n          d\n          i\n          a\n          g\n        \n        (\n        f\n        (\n        \u0394\n        t\n        )\n        )\n        f\n        (\n        t\n        )\n      \n    \n    {\\displaystyle f(t+\\Delta t)=\\mathrm {diag} (f(\\Delta t))f(t)}\n  \nwhere \n  \n    \n      \n        \u0394\n        t\n        \u2208\n        \n          R\n        \n      \n    \n    {\\displaystyle \\Delta t\\in \\mathbb {R} }\n  \n is the distance one wishes to shift. This allows the transformer to take any encoded position, and find the encoding of the position n-steps-ahead or n-steps-behind, by a matrix multiplication.\nBy taking a linear sum, any convolution can also be implemented as linear transformations:\n  \n    \n      \n        \n          \u2211\n          \n            j\n          \n        \n        \n          c\n          \n            j\n          \n        \n        f\n        (\n        t\n        +\n        \u0394\n        \n          t\n          \n            j\n          \n        \n        )\n        =\n        \n          (\n          \n            \n              \u2211\n              \n                j\n              \n            \n            \n              c\n              \n                j\n              \n            \n            \n            \n              d\n              i\n              a\n              g\n            \n            (\n            f\n            (\n            \u0394\n            \n              t\n              \n                j\n              \n            \n            )\n            )\n          \n          )\n        \n        f\n        (\n        t\n        )\n      \n    \n    {\\displaystyle \\sum _{j}c_{j}f(t+\\Delta t_{j})=\\left(\\sum _{j}c_{j}\\,\\mathrm {diag} (f(\\Delta t_{j}))\\right)f(t)}\n  \nfor any constants \n  \n    \n      \n        \n          c\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle c_{j}}\n  \n. This allows the transformer to take any encoded position and find a linear sum of the encoded locations of its neighbors. This sum of encoded positions, when fed into the attention mechanism, would create attention weights on its neighbors, much like what happens in a convolutional neural network language model. In the author's words, \"we hypothesized it would allow the model to easily learn to attend by relative position.\"\nIn typical implementations, all operations are done over the real numbers, not the complex numbers, but since complex multiplication can be implemented as real 2-by-2 matrix multiplication, this is a mere notational difference.Encoder-decoder (overview)\nLike earlier seq2seq models, the original transformer model used an encoder-decoder architecture. The encoder consists of encoding layers that process all the input tokens together one layer after another, while the decoder consists of decoding layers that iteratively process the encoder's output and the decoder's output tokens so far.\nThe purpose of each encoder layer is to create contextualized representations of the tokens, where each representation corresponds to a token that \"mixes\" information from other input tokens via self-attention mechanism. Each decoder layer contains two attention sublayers: (1) cross-attention for incorporating the output of encoder (contextualized input token representations), and (2) self-attention for \"mixing\" information among the input tokens to the decoder (i.e. the tokens generated so far during inference time).\nBoth the encoder and decoder layers have a feed-forward neural network for additional processing of their outputs and contain residual connections and layer normalization steps. These feed-forward layers contain most of the parameters in a Transformer model.Feedforward network\nThe feedforward network (FFN) modules in a Transformer are 2-layered multilayer perceptrons:\n  \n    \n      \n        \n          F\n          F\n          N\n        \n        (\n        x\n        )\n        =\n        \u03d5\n        (\n        x\n        \n          W\n          \n            (\n            1\n            )\n          \n        \n        +\n        \n          b\n          \n            (\n            1\n            )\n          \n        \n        )\n        \n          W\n          \n            (\n            2\n            )\n          \n        \n        +\n        \n          b\n          \n            (\n            2\n            )\n          \n        \n      \n    \n    {\\displaystyle \\mathrm {FFN} (x)=\\phi (xW^{(1)}+b^{(1)})W^{(2)}+b^{(2)}}\n  \nwhere \n  \n    \n      \n        \n          W\n          \n            (\n            1\n            )\n          \n        \n      \n    \n    {\\displaystyle W^{(1)}}\n  \n and \n  \n    \n      \n        \n          W\n          \n            (\n            2\n            )\n          \n        \n      \n    \n    {\\displaystyle W^{(2)}}\n  \n are weight matrices and \n  \n    \n      \n        \n          b\n          \n            (\n            1\n            )\n          \n        \n      \n    \n    {\\displaystyle b^{(1)}}\n  \n and  \n  \n    \n      \n        \n          b\n          \n            (\n            2\n            )\n          \n        \n      \n    \n    {\\displaystyle b^{(2)}}\n  \n are bias vectors, and \n  \n    \n      \n        \u03d5\n      \n    \n    {\\displaystyle \\phi }\n  \n is its activation function. The original Transformer used ReLU activation.\nThe number of neurons in the middle layer is called intermediate size (GPT), filter size (BERT), or feedforward size (BERT). It is typically larger than the embedding size. For example, in both GPT-2 series and BERT series, the intermediate size of a model is 4 times its embedding size: \n  \n    \n      \n        \n          d\n          \n            ffn\n          \n        \n        =\n        4\n        \n          d\n          \n            emb\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{ffn}}=4d_{\\text{emb}}}\n  \n.Scaled dot-product attention\nAttention head\nThe attention mechanism used in the Transformer architecture are scaled dot-product attention units. For each unit, the transformer model learns three weight matrices: the query weights \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle W^{Q}}\n  \n, the key weights \n  \n    \n      \n        \n          W\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle W^{K}}\n  \n, and the value weights \n  \n    \n      \n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W^{V}}\n  \n.\nThe module takes three sequences, a query sequence, a key sequence, and a value sequence. The query sequence is a sequence of length \n  \n    \n      \n        \n          \u2113\n          \n            seq, query\n          \n        \n      \n    \n    {\\displaystyle \\ell _{\\text{seq, query}}}\n  \n, and each entry is a vector of dimension \n  \n    \n      \n        \n          d\n          \n            emb, query\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{emb, query}}}\n  \n. Similarly for the key and value sequences.\nFor each vector \n  \n    \n      \n        \n          x\n          \n            i\n            ,\n            \n              query\n            \n          \n        \n      \n    \n    {\\displaystyle x_{i,{\\text{query}}}}\n  \n in the query sequence, it is multiplied by a matrix \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle W^{Q}}\n  \n to produce a query vector \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n        =\n        \n          x\n          \n            i\n            ,\n            \n              query\n            \n          \n        \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle q_{i}=x_{i,{\\text{query}}}W^{Q}}\n  \n. The matrix of all query vectors is the query matrix:\n  \n    \n      \n        Q\n        =\n        \n          X\n          \n            query\n          \n        \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle Q=X_{\\text{query}}W^{Q}}\n  \nSimilarly, we construct the key matrix \n  \n    \n      \n        K\n        =\n        \n          X\n          \n            key\n          \n        \n        \n          W\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle K=X_{\\text{key}}W^{K}}\n  \n and the value matrix \n  \n    \n      \n        V\n        =\n        \n          X\n          \n            value\n          \n        \n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle V=X_{\\text{value}}W^{V}}\n  \n.\nIt is usually the case that all \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n        ,\n        \n          W\n          \n            K\n          \n        \n        ,\n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W^{Q},W^{K},W^{V}}\n  \n are square matrices, meaning \n  \n    \n      \n        \n          d\n          \n            emb, query\n          \n        \n        =\n        \n          d\n          \n            query\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{emb, query}}=d_{\\text{query}}}\n  \n, etc.\nAttention weights are calculated using the query and key vectors: the attention weight \n  \n    \n      \n        \n          a\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle a_{ij}}\n  \n from token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n to token \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n is the dot product between \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle q_{i}}\n  \n and \n  \n    \n      \n        \n          k\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle k_{j}}\n  \n. The attention weights are divided by the square root of the dimension of the key vectors, \n  \n    \n      \n        \n          \n            \n              d\n              \n                k\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\sqrt {d_{k}}}}\n  \n, which stabilizes gradients during training, and passed through a softmax which normalizes the weights. The fact that \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle W^{Q}}\n  \n and \n  \n    \n      \n        \n          W\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle W^{K}}\n  \n are different matrices allows attention to be non-symmetric: if token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n attends to token \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n (i.e. \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n        \u22c5\n        \n          k\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle q_{i}\\cdot k_{j}}\n  \n is large), this does not necessarily mean that token \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n will attend to token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n (i.e. \n  \n    \n      \n        \n          q\n          \n            j\n          \n        \n        \u22c5\n        \n          k\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle q_{j}\\cdot k_{i}}\n  \n could be small). The output of the attention unit for token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n is the weighted sum of the value vectors of all tokens, weighted by \n  \n    \n      \n        \n          a\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle a_{ij}}\n  \n, the attention from token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n to each token.\nThe attention calculation for all tokens can be expressed as one large matrix calculation using the softmax function, which is useful for training due to computational matrix operation optimizations that quickly compute matrix operations. The matrices \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n, \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  \n and \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n are defined as the matrices where the \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \nth rows are vectors \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle q_{i}}\n  \n, \n  \n    \n      \n        \n          k\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle k_{i}}\n  \n, and \n  \n    \n      \n        \n          v\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle v_{i}}\n  \n respectively. Then we can represent the attention as\n  \n    \n      \n        \n          \n            \n              \n                \n                  Attention\n                \n                (\n                Q\n                ,\n                K\n                ,\n                V\n                )\n                =\n                \n                  softmax\n                \n                \n                  (\n                  \n                    \n                      \n                        Q\n                        \n                          K\n                          \n                            \n                              T\n                            \n                          \n                        \n                      \n                      \n                        \n                          d\n                          \n                            k\n                          \n                        \n                      \n                    \n                  \n                  )\n                \n                V\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\end{aligned}}}\n  \n\nwhere the softmax is applied over each of the rows of the matrix.\nThe number of dimensions in a query vector is query size \n  \n    \n      \n        \n          d\n          \n            query\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{query}}}\n  \n and similarly for the key size \n  \n    \n      \n        \n          d\n          \n            key\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{key}}}\n  \n and value size \n  \n    \n      \n        \n          d\n          \n            value\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{value}}}\n  \n. The output dimension of an attention head is its head dimension \n  \n    \n      \n        \n          d\n          \n            head\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{head}}}\n  \n. The attention mechanism requires the following three equalities to hold:\n  \n    \n      \n        \n          \u2113\n          \n            seq, key\n          \n        \n        =\n        \n          \u2113\n          \n            seq, value\n          \n        \n        ,\n        \n        \n          d\n          \n            query\n          \n        \n        =\n        \n          d\n          \n            key\n          \n        \n        ,\n        \n        \n          d\n          \n            value\n          \n        \n        =\n        \n          d\n          \n            head\n          \n        \n      \n    \n    {\\displaystyle \\ell _{\\text{seq, key}}=\\ell _{\\text{seq, value}},\\;d_{\\text{query}}=d_{\\text{key}},\\;d_{\\text{value}}=d_{\\text{head}}}\n  \nbut is otherwise unconstrained.\nIf the attention head is used in a self-attention fashion, then \n  \n    \n      \n        \n          X\n          \n            query\n          \n        \n        =\n        \n          X\n          \n            key\n          \n        \n        =\n        \n          X\n          \n            value\n          \n        \n      \n    \n    {\\displaystyle X_{\\text{query}}=X_{\\text{key}}=X_{\\text{value}}}\n  \n. If the attention head is used in a cross-attention fashion, then usually \n  \n    \n      \n        \n          X\n          \n            query\n          \n        \n        \u2260\n        \n          X\n          \n            key\n          \n        \n        =\n        \n          X\n          \n            value\n          \n        \n      \n    \n    {\\displaystyle X_{\\text{query}}\\neq X_{\\text{key}}=X_{\\text{value}}}\n  \n. It is theoretically possible for all three to be different, but that is rarely the case in practice.Multiheaded attention\nOne set of \n  \n    \n      \n        \n          (\n          \n            \n              W\n              \n                Q\n              \n            \n            ,\n            \n              W\n              \n                K\n              \n            \n            ,\n            \n              W\n              \n                V\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\left(W^{Q},W^{K},W^{V}\\right)}\n  \n matrices is called an attention head, and each layer in a transformer model has multiple attention heads. While each attention head attends to the tokens that are relevant to each token, multiple attention heads allow the model to do this for different definitions of \"relevance\". Specifically, the query and key projection matrices, \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle W^{Q}}\n  \n and \n  \n    \n      \n        \n          W\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle W^{K}}\n  \n , which are involved in the attention score computation, defines the \"relevance\". Meanwhile, the value projection matrix \n  \n    \n      \n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W^{V}}\n  \n, in combination with the part of the output projection matrix \n  \n    \n      \n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle W^{O}}\n  \n, determines how the attended tokens influence what information is passed to subsequent layers and ultimately the output logits. In addition, the scope of attention, or the range of token relationships captured by each attention head, can expand as tokens pass through successive layers. This allows the model to capture more complex and long-range dependencies in deeper layers. Many transformer attention heads encode relevance relations that are meaningful to humans. For example, some attention heads can attend mostly to the next word, while others mainly attend from verbs to their direct objects. The computations for each attention head can be performed in parallel, which allows for fast processing. The outputs for the attention layer are concatenated to pass into the feed-forward neural network layers.\nConcretely, let the multiple attention heads be indexed by \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n, then we have\n  \n    \n      \n        \n          MultiheadedAttention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          \n            Concat\n          \n          \n            i\n            \u2208\n            [\n            \n              n\n              \n                heads\n              \n            \n            ]\n          \n        \n        (\n        \n          Attention\n        \n        (\n        Q\n        \n          W\n          \n            i\n          \n          \n            Q\n          \n        \n        ,\n        K\n        \n          W\n          \n            i\n          \n          \n            K\n          \n        \n        ,\n        V\n        \n          W\n          \n            i\n          \n          \n            V\n          \n        \n        )\n        )\n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle {\\text{MultiheadedAttention}}(Q,K,V)={\\text{Concat}}_{i\\in [n_{\\text{heads}}]}({\\text{Attention}}(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V}))W^{O}}\n  \n where the matrix \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  \n is the concatenation of word embeddings, and the matrices \n  \n    \n      \n        \n          W\n          \n            i\n          \n          \n            Q\n          \n        \n        ,\n        \n          W\n          \n            i\n          \n          \n            K\n          \n        \n        ,\n        \n          W\n          \n            i\n          \n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W_{i}^{Q},W_{i}^{K},W_{i}^{V}}\n  \n are \"projection matrices\" owned by individual attention head \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n, and \n  \n    \n      \n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle W^{O}}\n  \n is a final projection matrix owned by the whole multi-headed attention head.\nIt is theoretically possible for each attention head to have a different head dimension \n  \n    \n      \n        \n          d\n          \n            head\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{head}}}\n  \n, but that is rarely the case in practice.\nAs an example, in the smallest GPT-2 model, there are only self-attention mechanisms. It has the following dimensions:\n  \n    \n      \n        \n          d\n          \n            emb\n          \n        \n        =\n        768\n        ,\n        \n          n\n          \n            head\n          \n        \n        =\n        12\n        ,\n        \n          d\n          \n            head\n          \n        \n        =\n        64\n      \n    \n    {\\displaystyle d_{\\text{emb}}=768,n_{\\text{head}}=12,d_{\\text{head}}=64}\n  \nSince \n  \n    \n      \n        12\n        \u00d7\n        64\n        =\n        768\n      \n    \n    {\\displaystyle 12\\times 64=768}\n  \n, its output projection matrix \n  \n    \n      \n        \n          W\n          \n            O\n          \n        \n        \u2208\n        \n          \n            R\n          \n          \n            (\n            12\n            \u00d7\n            64\n            )\n            \u00d7\n            768\n          \n        \n      \n    \n    {\\displaystyle W^{O}\\in \\mathbb {R} ^{(12\\times 64)\\times 768}}\n  \n is a square matrix.Masked attention\nThe Transformer architecture is constructed to calculate output tokens iteratively. Assuming \n  \n    \n      \n        t\n        =\n        0\n      \n    \n    {\\displaystyle t=0}\n  \n refers to the calculation of the first output token \n  \n    \n      \n        i\n        =\n        0\n      \n    \n    {\\displaystyle i=0}\n  \n, for step \n  \n    \n      \n        t\n        >\n        0\n      \n    \n    {\\displaystyle t>0}\n  \n, the output token \n  \n    \n      \n        i\n        =\n        0\n      \n    \n    {\\displaystyle i=0}\n  \n shall remain constant. This ensures properties of the model similar to autoregressive models. Therefore, at every time step \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n, the calculation for all outputs \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n should not have access to tokens at position \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n for \n  \n    \n      \n        j\n        >=\n        i\n      \n    \n    {\\displaystyle j>=i}\n  \n (as it naturally is the case for time step \n  \n    \n      \n        t\n        =\n        i\n      \n    \n    {\\displaystyle t=i}\n  \n, when tokens \n  \n    \n      \n        j\n        >\n        t\n      \n    \n    {\\displaystyle j>t}\n  \n are not yet calculated). This behavior may be accomplished before the softmax stage by adding a mask matrix \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n that is \n  \n    \n      \n        \u2212\n        \u221e\n      \n    \n    {\\displaystyle -\\infty }\n  \n at entries where the attention link must be cut, and \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  \n at other places:\n  \n    \n      \n        \n          \n            \n              \n                \n                  MaskedAttention\n                \n                (\n                Q\n                ,\n                K\n                ,\n                V\n                )\n                =\n                \n                  softmax\n                \n                \n                  (\n                  \n                    M\n                    +\n                    \n                      \n                        \n                          Q\n                          \n                            K\n                            \n                              \n                                T\n                              \n                            \n                          \n                        \n                        \n                          \n                            d\n                            \n                              k\n                            \n                          \n                        \n                      \n                    \n                  \n                  )\n                \n                V\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{MaskedAttention}}(Q,K,V)={\\text{softmax}}\\left(M+{\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\end{aligned}}}\n  \n The following matrix is commonly used in decoder self-attention modules, called \"causal masking\":\n  \n    \n      \n        \n          M\n          \n            causal\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  0\n                \n                \n                  \u2212\n                  \u221e\n                \n                \n                  \u2212\n                  \u221e\n                \n                \n                  \u2026\n                \n                \n                  \u2212\n                  \u221e\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  \u2212\n                  \u221e\n                \n                \n                  \u2026\n                \n                \n                  \u2212\n                  \u221e\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  \u2026\n                \n                \n                  \u2212\n                  \u221e\n                \n              \n              \n                \n                  \u22ee\n                \n                \n                  \u22ee\n                \n                \n                  \u22ee\n                \n                \n                  \u22f1\n                \n                \n                  \u22ee\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  \u2026\n                \n                \n                  0\n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle M_{\\text{causal}}={\\begin{bmatrix}0&-\\infty &-\\infty &\\dots &-\\infty \\\\0&0&-\\infty &\\dots &-\\infty \\\\0&0&0&\\dots &-\\infty \\\\\\vdots &\\vdots &\\vdots &\\ddots &\\vdots \\\\0&0&0&\\dots &0\\end{bmatrix}}}\n  \n\nIn words, it means that each token can pay attention to itself, and every token before it, but not any after it. A non-masked attention module can be thought of as a masked attention module where the mask has all entries zero. As an example of an uncommon use of mask matrix, the XLNet considers all masks of the form \n  \n    \n      \n        P\n        \n          M\n          \n            causal\n          \n        \n        \n          P\n          \n            \u2212\n            1\n          \n        \n      \n    \n    {\\displaystyle PM_{\\text{causal}}P^{-1}}\n  \n, where \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n is a random permutation matrix.Encoder\nAn encoder consists of an embedding layer, followed by multiple encoder layers.\nEach encoder layer consists of two major components: a self-attention mechanism and a feed-forward layer. It takes an input as a sequence of input vectors, applies the self-attention mechanism, to produce an intermediate sequence of vectors, then applies the feed-forward layer for each vector individually. Schematically, we have:\n  \n    \n      \n        \n          \n            \n              \n                \n                  given input vectors \n                \n              \n              \n                \n                  h\n                  \n                    0\n                  \n                \n                ,\n                \n                  h\n                  \n                    1\n                  \n                \n                ,\n                \u2026\n              \n            \n            \n              \n                \n                  combine them into a matrix \n                \n                H\n              \n              \n                \n                =\n                \n                  \n                    [\n                    \n                      \n                        \n                          \n                            h\n                            \n                              0\n                            \n                          \n                        \n                      \n                      \n                        \n                          \n                            h\n                            \n                              1\n                            \n                          \n                        \n                      \n                      \n                        \n                          \u22ee\n                        \n                      \n                    \n                    ]\n                  \n                \n              \n            \n            \n              \n                \n                  EncoderLayer\n                \n                (\n                H\n                )\n              \n              \n                \n                =\n                \n                  \n                    [\n                    \n                      \n                        \n                          \n                            FFN\n                          \n                          (\n                          \n                            MultiheadedAttention\n                          \n                          (\n                          H\n                          ,\n                          H\n                          ,\n                          H\n                          \n                            )\n                            \n                              0\n                            \n                          \n                          )\n                        \n                      \n                      \n                        \n                          \n                            FFN\n                          \n                          (\n                          \n                            MultiheadedAttention\n                          \n                          (\n                          H\n                          ,\n                          H\n                          ,\n                          H\n                          \n                            )\n                            \n                              1\n                            \n                          \n                          )\n                        \n                      \n                      \n                        \n                          \u22ee\n                        \n                      \n                    \n                    ]\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{given input vectors }}&h_{0},h_{1},\\dots \\\\{\\text{combine them into a matrix }}H&={\\begin{bmatrix}h_{0}\\\\h_{1}\\\\\\vdots \\end{bmatrix}}\\\\{\\text{EncoderLayer}}(H)&={\\begin{bmatrix}{\\text{FFN}}({\\text{MultiheadedAttention}}(H,H,H)_{0})\\\\{\\text{FFN}}({\\text{MultiheadedAttention}}(H,H,H)_{1})\\\\\\vdots \\end{bmatrix}}\\\\\\end{aligned}}}\n  \n\nwhere \n  \n    \n      \n        \n          FFN\n        \n      \n    \n    {\\displaystyle {\\text{FFN}}}\n  \n stands for \"feed-forward network\". We can more succinctly write it as\n  \n    \n      \n        \n          EncoderLayer\n        \n        (\n        H\n        )\n        =\n        \n          FFN\n        \n        (\n        \n          MultiheadedAttention\n        \n        (\n        H\n        ,\n        H\n        ,\n        H\n        )\n        )\n      \n    \n    {\\displaystyle {\\text{EncoderLayer}}(H)={\\text{FFN}}({\\text{MultiheadedAttention}}(H,H,H))}\n  \nwith the implicit convention that the \n  \n    \n      \n        \n          FFN\n        \n      \n    \n    {\\displaystyle {\\text{FFN}}}\n  \n is applied to each row of the matrix individually.\nThe encoder layers are stacked. The first encoder layer takes the sequence of input vectors from the embedding layer, producing a sequence of vectors. This sequence of vectors is processed by the second encoder, and so on. The output from the final encoder layer is then used by the decoder.\nAs the encoder processes the entire input all at once, every token can attend to every other token (all-to-all attention), so there is no need for causal masking.Decoder\nA decoder consists of an embedding layer, followed by multiple decoder layers, followed by an un-embedding layer.\nEach decoder consists of three major components: a causally masked self-attention mechanism, a cross-attention mechanism, and a feed-forward neural network. The decoder functions in a similar fashion to the encoder, but an additional attention mechanism is inserted which instead draws relevant information from the encodings generated by the encoders. This mechanism can also be called the encoder-decoder attention.\nLike the first encoder, the first decoder takes positional information and embeddings of the output sequence as its input, rather than encodings. The transformer must not use the current or future output to predict an output, so the output sequence must be partially masked to prevent this reverse information flow. This allows for autoregressive text generation. For decoding, all-to-all attention is inappropriate, because a token cannot attend to tokens not yet generated. Thus, the self-attention module in the decoder is causally masked.\nIn contrast, the cross-attention mechanism attends to the output vectors of the encoder, which is computed before the decoder starts decoding. Consequently, there is no need for masking in the cross-attention mechanism.\nSchematically, we have:\n  \n    \n      \n        \n          \n            \n              \n                \n                  H\n                  \u2032\n                \n              \n              \n                \n                =\n                \n                  MaskedMultiheadedAttention\n                \n                (\n                H\n                ,\n                H\n                ,\n                H\n                )\n              \n            \n            \n              \n                \n                  DecoderLayer\n                \n                (\n                H\n                )\n              \n              \n                \n                =\n                \n                  FFN\n                \n                (\n                \n                  MultiheadedAttention\n                \n                (\n                \n                  H\n                  \u2032\n                \n                ,\n                \n                  H\n                  \n                    E\n                  \n                \n                ,\n                \n                  H\n                  \n                    E\n                  \n                \n                )\n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}H'&={\\text{MaskedMultiheadedAttention}}(H,H,H)\\\\{\\text{DecoderLayer}}(H)&={\\text{FFN}}({\\text{MultiheadedAttention}}(H',H^{E},H^{E}))\\end{aligned}}}\n  \nwhere \n  \n    \n      \n        \n          H\n          \n            E\n          \n        \n      \n    \n    {\\displaystyle H^{E}}\n  \n is the matrix with rows being the output vectors from the encoder.\nThe last decoder is followed by a final un-embedding layer. to produce the output probabilities over the vocabulary. Then, one of the tokens is sampled according to the probability, and the decoder can be run again to produce the next token, etc, autoregressively generating output text.Adapted architectures\nMany large language models, since they do not need to predict a whole new sequence from an input sequence, only use the encoder or decoder of the original transformer architecture. Early GPT models are decoder-only models trained to predict the next token in a sequence. BERT, another language model, only makes use of an encoder, and is trained to predict a randomly masked token in a sequence.Full transformer architecture\nSublayers\nEach encoder layer contains 2 sublayers: the self-attention and the feedforward network. Each decoder layer contains 3 sublayers: the causally masked self-attention, the cross-attention, and the feedforward network.\n\nThe final points of detail are the residual connections and layer normalization (LayerNorm, or LN), which while conceptually unnecessary, are necessary for numerical stability and convergence.  \nThe residual connection, which is introduced to avoid vanishing gradient issues and stabilize the training process, can be expressed as follows: y = F(x) + x. The expression indicates that an output y is the sum of the transformation of input x (F(x)) and the input itself (x). Adding the input x can preserve the input information and avoid issues when the gradient of F(x) is close to zero.  \nSimilarly to how the feedforward network modules are applied individually to each vector, the LayerNorm is also applied individually to each vector. \nThere are two common conventions in use: the post-LN and the pre-LN convention. In the post-LN convention, the output of each sublayer is \n  \n    \n      \n        \n          L\n          a\n          y\n          e\n          r\n          N\n          o\n          r\n          m\n        \n        (\n        x\n        +\n        \n          S\n          u\n          b\n          l\n          a\n          y\n          e\n          r\n        \n        (\n        x\n        )\n        )\n      \n    \n    {\\displaystyle \\mathrm {LayerNorm} (x+\\mathrm {Sublayer} (x))}\n  \nwhere \n  \n    \n      \n        \n          S\n          u\n          b\n          l\n          a\n          y\n          e\n          r\n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\mathrm {Sublayer} (x)}\n  \n is the function implemented by the sublayer itself.\nIn the pre-LN convention, the output of each sublayer is\n  \n    \n      \n        x\n        +\n        \n          S\n          u\n          b\n          l\n          a\n          y\n          e\n          r\n        \n        (\n        \n          L\n          a\n          y\n          e\n          r\n          N\n          o\n          r\n          m\n        \n        (\n        x\n        )\n        )\n      \n    \n    {\\displaystyle x+\\mathrm {Sublayer} (\\mathrm {LayerNorm} (x))}\n  \nThe original 2017 Transformer used the post-LN convention. It was difficult to train and required careful hyperparameter tuning and a \"warm-up\" in learning rate, where it starts small and gradually increases. The pre-LN convention, proposed several times in 2018, was found to be easier to train, requiring no warm-up, leading to faster convergence.Pseudocode\nThe following is the pseudocode for a standard pre-LN encoder-decoder Transformer, adapted from\n\ninput: Encoder input t_e\n       Decoder input t_d\noutput: Array of probability distributions, with shape (decoder vocabulary size x length(decoder output sequence))\n\n/* encoder */\nz_e \u2190 encoder.tokenizer(t_e)\n\nfor each t in 1:length(z_e) do\n    z_e[t] \u2190 encoder.embedding(z_e[t]) + encoder.positional_embedding(t)\n\nfor each l in 1:length(encoder.layers) do\n    layer \u2190 encoder.layers[l]\n\n    /* first sublayer */\n    z_e_copy \u2190 copy(z_e)\n    for each t in 1:length(z_e) do\n        z_e[t] \u2190 layer.layer_norm(z_e[t])\n    z_e \u2190 layer.multiheaded_attention(z_e, z_e, z_e)\n    for each t in 1:length(z_e) do\n        z_e[t] \u2190 z_e[t] + z_e_copy[t]\n\n    /* second sublayer */\n    z_e_copy \u2190 copy(z_e)\n    for each t in 1:length(z_e) do\n        z_e[t] \u2190 layer.layer_norm(z_e[t])\n    z_e \u2190 layer.feedforward(z_e)\n    for each t in 1:length(z_e) do\n        z_e[t] \u2190 z_e[t] + z_e_copy[t]\n\nfor each t in 1:length(z_e) do\n    z_e[t] \u2190 encoder.final_layer_norm(z_e[t])\n\n/* decoder */\nz_d \u2190 decoder.tokenizer(t_d)\n\nfor each t in 1:length(z_d) do\n    z_d[t] \u2190 decoder.embedding(z_d[t]) + decoder.positional_embedding(t)\n\nfor each l in 1:length(decoder.layers) do\n        layer \u2190 decoder.layers[l]\n\n        /* first sublayer */\n        z_d_copy \u2190 copy(z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] \u2190 layer.layer_norm(z_d[t])\n        z_d \u2190 layer.masked_multiheaded_attention(z_d, z_d, z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] \u2190 z_d[t] + z_d_copy[t]\n\n        /* second sublayer */\n        z_d_copy \u2190 copy(z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] \u2190 layer.layer_norm(z_d[t])\n        z_d \u2190 layer.multiheaded_attention(z_d, z_e, z_e) \n        for each i in 1:length(z_d) do\n            z_d[t] \u2190 z_d[t] + z_d_copy[t]\n\n        /* third sublayer */\n        z_d_copy \u2190 copy(z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] \u2190 layer.layer_norm(z_d[t])\n        z_d \u2190 layer.feedforward(z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] \u2190 z_d[t] + z_d_copy[t]\n\nz_d \u2190 decoder.final_layer_norm(z_d)\n\noutput_distributions \u2190 []\nfor each t in 1:length(z_d) do\n    output_distributions.append(decoder.unembed(z_d[t]))\n\nreturn output_distributionsTerminology\nThe Transformer architecture, being modular, allows variations. Several common variations are described here.\nAn \"encoder-only\" Transformer applies the encoder to map an input text into a sequence of vectors that represent the input text. This is usually used for text embedding and representation learning for downstream applications. BERT is encoder-only. They are less often used currently, as they were found to be not significantly better than training an encoder-decoder Transformer, then taking just the encoder.\nA \"decoder-only\" Transformer is not literally decoder-only, since without an encoder, the cross-attention mechanism has nothing to attend to. Thus, the decoder layers in a decoder-only Transformer is composed of just two sublayers: the causally masked self-attention, and the feedforward network. This is usually used for text generation and instruction following. The models in the GPT series and Chinchilla series are decoder-only.\nAn \"encoder-decoder\" Transformer is generally the same as the original Transformer, with 2 sublayers per encoder layer and 3 sublayers per decoder layer, etc. They might have minor architectural improvements, such as alternative activation functions, changing the location of normalization, etc. This is also usually used for text generation and instruction following. The models in the T5 series are encoder-decoder.\nA \"prefixLM\" (prefix language model) is a decoder-only architecture, but with prefix masking, which is different from causal masking. Specifically, it has mask of the form:\u200aFigure 3\u200a\n  \n    \n      \n        \n          M\n          \n            prefixLM\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  \n                    0\n                  \n                \n                \n                  \u2212\n                  \u221e\n                \n              \n              \n                \n                  \n                    0\n                  \n                \n                \n                  \n                    M\n                    \n                      causal\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle M_{\\text{prefixLM}}={\\begin{bmatrix}\\mathbf {0} &-\\infty \\\\\\mathbf {0} &M_{\\text{causal}}\\end{bmatrix}}}\n  \nwhere the first columns correspond to the \"prefix\", and the subsequent columns correspond to the autoregressively generated text based on the prefix. They resemble encoder-decoder models, but has less \"sparsity\". Such models are rarely used, though they are cited as theoretical possibilities and benchmarked comparisons.\nThere are also mixed seq2seq models. For example, in 2020, Google Translate replaced the previous RNN-encoder\u2013RNN-decoder model by a Transformer-encoder\u2013RNN-decoder model, on the argument that an RNN-decoder runs much faster than Transformer-decoder when run autoregressively.Subsequent work\nAlternative activation functions\nThe original transformer uses ReLU activation function. Other activation functions were developed. The Llama series and PaLM used SwiGLU; both GPT-1 and BERT used GELU. \nAlternative activation functions are often used in combination with Gated Linear Units in the feedforward module.Alternative normalizations\nThe normalization used in the Transformer can be different from LayerNorm. One example is RMSNorm which is used in the Llama series. Other examples include CapsuleNorm ScaleNorm, or FixNorm.Alternative positional encodings\nTransformers may use other positional encoding methods than sinusoidal. \nThe original Transformer paper reported using a learned positional encoding, but finding it not superior to the sinusoidal one. Later,  found that causal masking itself provides enough signal to a Transformer decoder that it can learn to implicitly perform absolute positional encoding without the positional encoding module.RoPE\nRoPE (rotary positional embedding), is best explained by considering a list of 2-dimensional vectors \n  \n    \n      \n        [\n        (\n        \n          x\n          \n            1\n          \n          \n            (\n            1\n            )\n          \n        \n        ,\n        \n          x\n          \n            1\n          \n          \n            (\n            2\n            )\n          \n        \n        )\n        ,\n        (\n        \n          x\n          \n            2\n          \n          \n            (\n            1\n            )\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n          \n            (\n            2\n            )\n          \n        \n        )\n        ,\n        (\n        \n          x\n          \n            3\n          \n          \n            (\n            1\n            )\n          \n        \n        ,\n        \n          x\n          \n            3\n          \n          \n            (\n            2\n            )\n          \n        \n        )\n        ,\n        .\n        .\n        .\n        ]\n      \n    \n    {\\displaystyle [(x_{1}^{(1)},x_{1}^{(2)}),(x_{2}^{(1)},x_{2}^{(2)}),(x_{3}^{(1)},x_{3}^{(2)}),...]}\n  \n. Now pick some angle \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  \n. Then RoPE encoding is\n  \n    \n      \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        \n          x\n          \n            m\n          \n          \n            (\n            1\n            )\n          \n        \n        ,\n        \n          x\n          \n            m\n          \n          \n            (\n            2\n            )\n          \n        \n        ,\n        m\n        \n          \n            )\n          \n        \n        =\n        \n          \n            (\n            \n              \n                \n                  cos\n                  \u2061\n                  m\n                  \u03b8\n                \n                \n                  \u2212\n                  sin\n                  \u2061\n                  m\n                  \u03b8\n                \n              \n              \n                \n                  sin\n                  \u2061\n                  m\n                  \u03b8\n                \n                \n                  cos\n                  \u2061\n                  m\n                  \u03b8\n                \n              \n            \n            )\n          \n        \n        \n          \n            (\n            \n              \n                \n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      1\n                      )\n                    \n                  \n                \n              \n              \n                \n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      2\n                      )\n                    \n                  \n                \n              \n            \n            )\n          \n        \n        =\n        \n          \n            (\n            \n              \n                \n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      1\n                      )\n                    \n                  \n                  cos\n                  \u2061\n                  m\n                  \u03b8\n                  \u2212\n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      2\n                      )\n                    \n                  \n                  sin\n                  \u2061\n                  m\n                  \u03b8\n                \n              \n              \n                \n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      2\n                      )\n                    \n                  \n                  cos\n                  \u2061\n                  m\n                  \u03b8\n                  +\n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      1\n                      )\n                    \n                  \n                  sin\n                  \u2061\n                  m\n                  \u03b8\n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle {\\text{RoPE}}{\\big (}x_{m}^{(1)},x_{m}^{(2)},m{\\big )}={\\begin{pmatrix}\\cos m\\theta &-\\sin m\\theta \\\\\\sin m\\theta &\\cos m\\theta \\end{pmatrix}}{\\begin{pmatrix}x_{m}^{(1)}\\\\x_{m}^{(2)}\\\\\\end{pmatrix}}={\\begin{pmatrix}x_{m}^{(1)}\\cos m\\theta -x_{m}^{(2)}\\sin m\\theta \\\\x_{m}^{(2)}\\cos m\\theta +x_{m}^{(1)}\\sin m\\theta \\\\\\end{pmatrix}}}\n  \nEquivalently, if we write the 2-dimensional vectors as complex numbers \n  \n    \n      \n        \n          z\n          \n            m\n          \n        \n        :=\n        \n          x\n          \n            m\n          \n          \n            (\n            1\n            )\n          \n        \n        +\n        i\n        \n          x\n          \n            m\n          \n          \n            (\n            2\n            )\n          \n        \n      \n    \n    {\\displaystyle z_{m}:=x_{m}^{(1)}+ix_{m}^{(2)}}\n  \n, then RoPE encoding is just multiplication by an angle:\n  \n    \n      \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        \n          z\n          \n            m\n          \n        \n        ,\n        m\n        \n          \n            )\n          \n        \n        =\n        \n          e\n          \n            i\n            m\n            \u03b8\n          \n        \n        \n          z\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle {\\text{RoPE}}{\\big (}z_{m},m{\\big )}=e^{im\\theta }z_{m}}\n  \nFor a list of \n  \n    \n      \n        2\n        n\n      \n    \n    {\\displaystyle 2n}\n  \n-dimensional vectors, a RoPE encoder is defined by a sequence of angles \n  \n    \n      \n        \n          \u03b8\n          \n            (\n            1\n            )\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          \u03b8\n          \n            (\n            n\n            )\n          \n        \n      \n    \n    {\\displaystyle \\theta ^{(1)},...,\\theta ^{(n)}}\n  \n. Then the RoPE encoding is applied to each pair of coordinates.\nThe benefit of RoPE is that the dot-product between two vectors depends on their relative location only:\n  \n    \n      \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        x\n        ,\n        m\n        \n          \n            \n              )\n            \n          \n          \n            T\n          \n        \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        y\n        ,\n        n\n        \n          \n            )\n          \n        \n        =\n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        x\n        ,\n        m\n        +\n        k\n        \n          \n            \n              )\n            \n          \n          \n            T\n          \n        \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        y\n        ,\n        n\n        +\n        k\n        \n          \n            )\n          \n        \n      \n    \n    {\\displaystyle {\\text{RoPE}}{\\big (}x,m{\\big )}^{T}{\\text{RoPE}}{\\big (}y,n{\\big )}={\\text{RoPE}}{\\big (}x,m+k{\\big )}^{T}{\\text{RoPE}}{\\big (}y,n+k{\\big )}}\n  \n\nfor any integer \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n.ALiBi\nALiBi (Attention with Linear Biases) is not a replacement for the positional encoder on the original transformer. Instead, it is an additional positional encoder that is directly plugged into the attention mechanism. Specifically, the ALiBi attention mechanism is\n  \n    \n      \n        \n          \n            \n              \n                \n                  Attention\n                \n                (\n                Q\n                ,\n                K\n                ,\n                V\n                )\n                =\n                \n                  softmax\n                \n                \n                  (\n                  \n                    \n                      \n                        \n                          Q\n                          \n                            K\n                            \n                              \n                                T\n                              \n                            \n                          \n                        \n                        \n                          \n                            d\n                            \n                              k\n                            \n                          \n                        \n                      \n                    \n                    +\n                    s\n                    B\n                  \n                  )\n                \n                V\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}+sB\\right)V\\end{aligned}}}\n  \nHere, \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n is a real number (\"scalar\"), and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n is the linear bias matrix defined by\n  \n    \n      \n        B\n        =\n        \n          \n            (\n            \n              \n                \n                  0\n                \n                \n                  1\n                \n                \n                  2\n                \n                \n                  3\n                \n                \n                  \u22ef\n                \n              \n              \n                \n                  \u2212\n                  1\n                \n                \n                  0\n                \n                \n                  1\n                \n                \n                  2\n                \n                \n                  \u22ef\n                \n              \n              \n                \n                  \u2212\n                  2\n                \n                \n                  \u2212\n                  1\n                \n                \n                  0\n                \n                \n                  1\n                \n                \n                  \u22ef\n                \n              \n              \n                \n                  \u2212\n                  3\n                \n                \n                  \u2212\n                  2\n                \n                \n                  \u2212\n                  1\n                \n                \n                  0\n                \n                \n                  \u22ef\n                \n              \n              \n                \n                  \u22ee\n                \n                \n                  \u22ee\n                \n                \n                  \u22ee\n                \n                \n                  \u22ee\n                \n                \n                  \u22f1\n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle B={\\begin{pmatrix}0&1&2&3&\\cdots \\\\-1&0&1&2&\\cdots \\\\-2&-1&0&1&\\cdots \\\\-3&-2&-1&0&\\cdots \\\\\\vdots &\\vdots &\\vdots &\\vdots &\\ddots \\\\\\end{pmatrix}}}\n  \nin other words, \n  \n    \n      \n        \n          B\n          \n            i\n            ,\n            j\n          \n        \n        =\n        j\n        \u2212\n        i\n      \n    \n    {\\displaystyle B_{i,j}=j-i}\n  \n. The idea being that the linear bias matrix is a softened mask. Just as \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  \n represent full attention paid, and \n  \n    \n      \n        \u2212\n        \u221e\n      \n    \n    {\\displaystyle -\\infty }\n  \n represents no attention paid, the linear bias matrix increases attention paid in one direction and decreases attention paid in the other direction.\nALiBi allows pretraining on short context windows, then fine-tuning on longer context windows. Since it is directly plugged into the attention mechanism, it can be combined with any positional encoder that is plugged into the \"bottom\" of the entire network (which is where the sinusoidal encoder on the original transformer, as well as RoPE and many others, are located).Relative Position Encodings\nRelative Position Encodings is similar to ALiBi, but more generic:\n  \n    \n      \n        \n          \n            \n              \n                \n                  Attention\n                \n                (\n                Q\n                ,\n                K\n                ,\n                V\n                )\n                =\n                \n                  softmax\n                \n                \n                  (\n                  \n                    \n                      \n                        \n                          Q\n                          \n                            K\n                            \n                              \n                                T\n                              \n                            \n                          \n                        \n                        \n                          \n                            d\n                            \n                              k\n                            \n                          \n                        \n                      \n                    \n                    +\n                    B\n                  \n                  )\n                \n                V\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}+B\\right)V\\end{aligned}}}\n  \nwhere \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n is a Toeplitz matrix, that is, \n  \n    \n      \n        \n          B\n          \n            i\n            ,\n            j\n          \n        \n        =\n        \n          B\n          \n            \n              i\n              \u2032\n            \n            ,\n            \n              j\n              \u2032\n            \n          \n        \n      \n    \n    {\\displaystyle B_{i,j}=B_{i',j'}}\n  \n whenever \n  \n    \n      \n        i\n        \u2212\n        j\n        =\n        \n          i\n          \u2032\n        \n        \u2212\n        \n          j\n          \u2032\n        \n      \n    \n    {\\displaystyle i-j=i'-j'}\n  \n. This is contrasted with the original sinusoidal positional encoding, which is an \"absolute positional encoding\".Efficient implementation\nThe transformer model has been implemented in standard deep learning frameworks such as TensorFlow and PyTorch. Transformers is a library produced by Hugging Face that supplies transformer-based architectures and pretrained models.KV caching\nWhen an autoregressive transformer is used for inference, such as generating text, the query vector is different at each step, but the already-computed key and value vectors are always the same. The KV caching method saves the computed key and value vectors at each attention block, so that they are not recomputed at each new token. PagedAttention applies memory paging to KV caching.\nIf a transformer is used with a baked-in prompt, such as [\"You are a customer support agent...\"], then the key and value vectors can be computed for the prompt, and saved on disk. The saving in compute is significant when the model is used for many short interactions, such as in online chatbots.FlashAttention\nFlashAttention is an algorithm that implements the transformer attention mechanism efficiently on a GPU. It is a communication-avoiding algorithm that performs matrix multiplications in blocks, such that each block fits within the cache of a GPU, and by careful management of the blocks it minimizes data copying between GPU caches (as data movement is slow). See the page on softmax for details.\nAn improved version, FlashAttention-2, was developed to cater to the rising demand for language models capable of handling longer context lengths. It offers enhancements in work partitioning and parallelism, enabling it to achieve up to 230 TFLOPs/s on A100 GPUs (FP16/BF16), a 2x speed increase over the original FlashAttention.\nKey advancements in FlashAttention-2 include the reduction of non-matmul FLOPs, improved parallelism over the sequence length dimension, better work partitioning between GPU warps, and added support for head dimensions up to 256 and multi-query attention (MQA) and grouped-query attention (GQA).\nBenchmarks revealed FlashAttention-2 to be up to 2x faster than FlashAttention and up to 9x faster than a standard attention implementation in PyTorch. Future developments include optimization for new hardware like H100 GPUs and new data types like FP8.Multi-Query Attention\nMulti-Query Attention changes the multiheaded attention mechanism. Whereas normally,\n\n  \n    \n      \n        \n          MultiheadedAttention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          \n            Concat\n          \n          \n            i\n            \u2208\n            [\n            \n              n\n              \n                heads\n              \n            \n            ]\n          \n        \n        \n          (\n          \n            \n              Attention\n            \n            (\n            X\n            \n              W\n              \n                i\n              \n              \n                Q\n              \n            \n            ,\n            X\n            \n              W\n              \n                i\n              \n              \n                K\n              \n            \n            ,\n            X\n            \n              W\n              \n                i\n              \n              \n                V\n              \n            \n            )\n          \n          )\n        \n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle {\\text{MultiheadedAttention}}(Q,K,V)={\\text{Concat}}_{i\\in [n_{\\text{heads}}]}\\left({\\text{Attention}}(XW_{i}^{Q},XW_{i}^{K},XW_{i}^{V})\\right)W^{O}}\n  \nwith Multi-Query Attention, there is just one \n  \n    \n      \n        \n          W\n          \n            K\n          \n        \n        ,\n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W^{K},W^{V}}\n  \n, thus:\n\n  \n    \n      \n        \n          MultiQueryAttention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          \n            Concat\n          \n          \n            i\n            \u2208\n            [\n            \n              n\n              \n                heads\n              \n            \n            ]\n          \n        \n        \n          (\n          \n            \n              Attention\n            \n            (\n            X\n            \n              W\n              \n                i\n              \n              \n                Q\n              \n            \n            ,\n            X\n            \n              W\n              \n                K\n              \n            \n            ,\n            X\n            \n              W\n              \n                V\n              \n            \n            )\n          \n          )\n        \n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle {\\text{MultiQueryAttention}}(Q,K,V)={\\text{Concat}}_{i\\in [n_{\\text{heads}}]}\\left({\\text{Attention}}(XW_{i}^{Q},XW^{K},XW^{V})\\right)W^{O}}\n  \n\nThis has a neutral effect on model quality and training speed, but increases inference speed. \nMore generally, grouped-query attention (GQA) partitions attention heads into groups, each of which shares the key-value pair. MQA is GQA with one group, while standard multiheaded attention is GQA with the maximal number of groups.\n\nMultihead Latent Attention (MLA) is a low-rank approximation to standard MHA. Specifically, each hidden vector, before entering the attention mechanism, is first projected to two low-dimensional spaces (\"latent space\"), one for query and one for key-value (KV vector). This design minimizes the KV cache, as only the low-dimensional KV vector needs to be cached.Speculative decoding\nSpeculative decoding is a method to accelerate token decoding. Similarly to speculative execution in CPUs, future tokens are computed quickly, then verified. If the quickly computed tokens are incorrect, they are discarded and computed slowly.\nThe key factor in speculative decoding is that a Transformer decoder can verify faster than it can decode, in the following sense.\nSuppose we have two transformer models like GPT-3 and GPT-3-small, both with a context window size of 512. To generate an entire context window autoregressively with greedy decoding with GPT-3, it must be run for 512 times, each time generating a token \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          x\n          \n            512\n          \n        \n      \n    \n    {\\displaystyle x_{1},x_{2},...,x_{512}}\n  \n, taking time \n  \n    \n      \n        512\n        \n          T\n          \n            GPT-3\n          \n        \n      \n    \n    {\\displaystyle 512T_{\\text{GPT-3}}}\n  \n. However, if we had some educated guess for the values of these tokens, we could verify all of them in parallel, in one run of the model, by checking that each \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle x_{t}}\n  \n is indeed the token with the largest log-likelihood in the \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n-th output.\nIn speculative decoding, a smaller model or some other simple heuristic is used to generate a few speculative tokens that are subsequently verified by the larger model. For example, suppose we use GPT-3-small to generate four speculative tokens: \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            1\n          \n        \n        ,\n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            2\n          \n        \n        ,\n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            3\n          \n        \n        ,\n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            4\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{1},{\\tilde {x}}_{2},{\\tilde {x}}_{3},{\\tilde {x}}_{4}}\n  \n. This only takes \n  \n    \n      \n        4\n        \n          T\n          \n            GPT-3-small\n          \n        \n      \n    \n    {\\displaystyle 4T_{\\text{GPT-3-small}}}\n  \n. These tokens are then run through the larger GPT-3 in one go. Suppose that \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            1\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{1}}\n  \n and \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{2}}\n  \n are verified by GPT-3 as what it would have picked, then those are kept, but \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            3\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{3}}\n  \n is not, so \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            3\n          \n        \n        ,\n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            4\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{3},{\\tilde {x}}_{4}}\n  \n are discarded, and GPT-3 is run on those. This would take \n  \n    \n      \n        4\n        \n          T\n          \n            GPT-3-small\n          \n        \n        +\n        3\n        \n          T\n          \n            GPT-3\n          \n        \n      \n    \n    {\\displaystyle 4T_{\\text{GPT-3-small}}+3T_{\\text{GPT-3}}}\n  \n, which might be shorter than \n  \n    \n      \n        4\n        \n          T\n          \n            GPT-3\n          \n        \n      \n    \n    {\\displaystyle 4T_{\\text{GPT-3}}}\n  \n.\nFor non-greedy decoding, similar ideas apply, except the speculative tokens are accepted or rejected stochastically, in a way that guarantees the final output distribution is the same as if speculative decoding was not used.\n\nIn Multi-Token Prediction, a single forward pass creates a final embedding vector, which then is un-embedded into a token probability. However, that vector can then be further processed by another Transformer block to predict the next token, and so on for arbitrarily many steps into the future. This trades off accuracy for speed, since each new token costs just one more Transformer block, rather than the entire stack.Sub-quadratic transformers\nTraining transformer-based architectures can be expensive, especially for long inputs. Many methods have been developed to attempt to address the issue. In the image domain, Swin Transformer is an efficient architecture that performs attention inside shifting windows. In the audio domain, SepTr decouples the attention in time and frequency domains. Long Range Arena (2020) is a standard benchmark for comparing the behavior of transformer architectures over long inputs.Alternative attention graphs\nThe standard attention graph is either all-to-all or causal, both of which scales as \n  \n    \n      \n        O\n        (\n        \n          N\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(N^{2})}\n  \n where \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is the number of tokens in a sequence.\nReformer (2020) reduces the computational load from \n  \n    \n      \n        O\n        (\n        \n          N\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(N^{2})}\n  \n to \n  \n    \n      \n        O\n        (\n        N\n        ln\n        \u2061\n        N\n        )\n      \n    \n    {\\displaystyle O(N\\ln N)}\n  \n by using locality-sensitive hashing and reversible layers. \nSparse attention uses attention graphs that grows slower than \n  \n    \n      \n        O\n        (\n        \n          N\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(N^{2})}\n  \n. For example, BigBird (2020) uses random small-world networks which grows as \n  \n    \n      \n        O\n        (\n        N\n        )\n      \n    \n    {\\displaystyle O(N)}\n  \n.\nOrdinary transformers require a memory size that is quadratic in the size of the context window. Attention-free transformers reduce this to a linear dependence while still retaining the advantages of a transformer by linking the key to the value.Random Feature Attention\nRandom Feature Attention (2021) uses Fourier random features:\n  \n    \n      \n        \u03c6\n        (\n        x\n        )\n        =\n        \n          \n            1\n            \n              D\n            \n          \n        \n        [\n        cos\n        \u2061\n        \u27e8\n        \n          w\n          \n            1\n          \n        \n        ,\n        x\n        \u27e9\n        ,\n        sin\n        \u2061\n        \u27e8\n        \n          w\n          \n            1\n          \n        \n        ,\n        x\n        \u27e9\n        ,\n        \u22ef\n        cos\n        \u2061\n        \u27e8\n        \n          w\n          \n            D\n          \n        \n        ,\n        x\n        \u27e9\n        ,\n        sin\n        \u2061\n        \u27e8\n        \n          w\n          \n            D\n          \n        \n        ,\n        x\n        \u27e9\n        \n          ]\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\varphi (x)={\\frac {1}{\\sqrt {D}}}[\\cos \\langle w_{1},x\\rangle ,\\sin \\langle w_{1},x\\rangle ,\\cdots \\cos \\langle w_{D},x\\rangle ,\\sin \\langle w_{D},x\\rangle ]^{T}}\n  \nwhere \n  \n    \n      \n        \n          w\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          w\n          \n            D\n          \n        \n      \n    \n    {\\displaystyle w_{1},...,w_{D}}\n  \n are independent samples from the normal distribution \n  \n    \n      \n        N\n        (\n        0\n        ,\n        \n          \u03c3\n          \n            2\n          \n        \n        I\n        )\n      \n    \n    {\\displaystyle N(0,\\sigma ^{2}I)}\n  \n. This choice of parameters satisfy \n  \n    \n      \n        \n          E\n        \n        [\n        \u27e8\n        \u03c6\n        (\n        x\n        )\n        ,\n        \u03c6\n        (\n        y\n        )\n        \u27e9\n        ]\n        =\n        \n          e\n          \n            \u2212\n            \n              \n                \n                  \u2016\n                  x\n                  \u2212\n                  y\n                  \n                    \u2016\n                    \n                      2\n                    \n                  \n                \n                \n                  2\n                  \n                    \u03c3\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbb {E} [\\langle \\varphi (x),\\varphi (y)\\rangle ]=e^{-{\\frac {\\|x-y\\|^{2}}{2\\sigma ^{2}}}}}\n  \n, or \n  \n    \n      \n        \n          e\n          \n            \u27e8\n            x\n            ,\n            y\n            \u27e9\n            \n              /\n            \n            \n              \u03c3\n              \n                2\n              \n            \n          \n        \n        =\n        \n          E\n        \n        [\n        \u27e8\n        \n          e\n          \n            \u2016\n            x\n            \n              \u2016\n              \n                2\n              \n            \n            \n              /\n            \n            2\n            \n              \u03c3\n              \n                2\n              \n            \n          \n        \n        \u03c6\n        (\n        x\n        )\n        ,\n        \n          e\n          \n            \u2016\n            y\n            \n              \u2016\n              \n                2\n              \n            \n            \n              /\n            \n            2\n            \n              \u03c3\n              \n                2\n              \n            \n          \n        \n        \u03c6\n        (\n        y\n        )\n        \u27e9\n        ]\n        \u2248\n        \u27e8\n        \n          e\n          \n            \u2016\n            x\n            \n              \u2016\n              \n                2\n              \n            \n            \n              /\n            \n            2\n            \n              \u03c3\n              \n                2\n              \n            \n          \n        \n        \u03c6\n        (\n        x\n        )\n        ,\n        \n          e\n          \n            \u2016\n            y\n            \n              \u2016\n              \n                2\n              \n            \n            \n              /\n            \n            2\n            \n              \u03c3\n              \n                2\n              \n            \n          \n        \n        \u03c6\n        (\n        y\n        )\n        \u27e9\n      \n    \n    {\\displaystyle e^{\\langle x,y\\rangle /\\sigma ^{2}}=\\mathbb {E} [\\langle e^{\\|x\\|^{2}/2\\sigma ^{2}}\\varphi (x),e^{\\|y\\|^{2}/2\\sigma ^{2}}\\varphi (y)\\rangle ]\\approx \\langle e^{\\|x\\|^{2}/2\\sigma ^{2}}\\varphi (x),e^{\\|y\\|^{2}/2\\sigma ^{2}}\\varphi (y)\\rangle }\n  \nConsequently, the one-headed attention, with one query, can be written as \n  \n    \n      \n        \n          Attention\n        \n        (\n        q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          softmax\n        \n        \n          (\n          \n            \n              \n                q\n                \n                  K\n                  \n                    \n                      T\n                    \n                  \n                \n              \n              \n                \n                  d\n                  \n                    k\n                  \n                \n              \n            \n          \n          )\n        \n        V\n        \u2248\n        \n          \n            \n              \u03c6\n              (\n              q\n              \n                )\n                \n                  T\n                \n              \n              \n                \u2211\n                \n                  i\n                \n              \n              \n                e\n                \n                  \u2016\n                  \n                    k\n                    \n                      i\n                    \n                  \n                  \n                    \u2016\n                    \n                      2\n                    \n                  \n                  \n                    /\n                  \n                  2\n                  \n                    \u03c3\n                    \n                      2\n                    \n                  \n                \n              \n              \u03c6\n              (\n              \n                k\n                \n                  i\n                \n              \n              )\n              \n                v\n                \n                  i\n                \n                \n                  T\n                \n              \n            \n            \n              \u03c6\n              (\n              q\n              \n                )\n                \n                  T\n                \n              \n              \n                \u2211\n                \n                  i\n                \n              \n              \n                e\n                \n                  \u2016\n                  \n                    k\n                    \n                      i\n                    \n                  \n                  \n                    \u2016\n                    \n                      2\n                    \n                  \n                  \n                    /\n                  \n                  2\n                  \n                    \u03c3\n                    \n                      2\n                    \n                  \n                \n              \n              \u03c6\n              (\n              \n                k\n                \n                  i\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\text{Attention}}(q,K,V)={\\text{softmax}}\\left({\\frac {qK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\approx {\\frac {\\varphi (q)^{T}\\sum _{i}e^{\\|k_{i}\\|^{2}/2\\sigma ^{2}}\\varphi (k_{i})v_{i}^{T}}{\\varphi (q)^{T}\\sum _{i}e^{\\|k_{i}\\|^{2}/2\\sigma ^{2}}\\varphi (k_{i})}}}\n  \nwhere \n  \n    \n      \n        \u03c3\n        =\n        \n          d\n          \n            K\n          \n          \n            1\n            \n              /\n            \n            4\n          \n        \n      \n    \n    {\\displaystyle \\sigma =d_{K}^{1/4}}\n  \n. Similarly for multiple queries, and for multiheaded attention.\nThis approximation can be computed in linear time, as we can compute the matrix \n  \n    \n      \n        \u03c6\n        (\n        \n          k\n          \n            i\n          \n        \n        )\n        \n          v\n          \n            i\n          \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\varphi (k_{i})v_{i}^{T}}\n  \n first, then multiply it with the query. In essence, we have managed to obtain a more precise version of \n  \n    \n      \n        \n          Attention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          softmax\n        \n        \n          (\n          \n            \n              \n                Q\n                \n                  K\n                  \n                    \n                      T\n                    \n                  \n                \n              \n              \n                \n                  d\n                  \n                    k\n                  \n                \n              \n            \n          \n          )\n        \n        V\n        \u2248\n        Q\n        (\n        \n          K\n          \n            T\n          \n        \n        V\n        \n          /\n        \n        \n          \n            \n              d\n              \n                k\n              \n            \n          \n        \n        )\n      \n    \n    {\\displaystyle {\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\approx Q(K^{T}V/{\\sqrt {d_{k}}})}\n  \nPerformer (2022) uses the same Random Feature Attention, but \n  \n    \n      \n        \n          w\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          w\n          \n            D\n          \n        \n      \n    \n    {\\displaystyle w_{1},...,w_{D}}\n  \n are first independently sampled from the normal distribution \n  \n    \n      \n        N\n        (\n        0\n        ,\n        \n          \u03c3\n          \n            2\n          \n        \n        I\n        )\n      \n    \n    {\\displaystyle N(0,\\sigma ^{2}I)}\n  \n, then they are Gram-Schmidt processed.Multimodality\nTransformers can also be used/adapted for modalities (input or output) beyond just text, usually by finding a way to \"tokenize\" the modality.\nMultimodal models can either be trained from scratch, or by finetuning. A 2022 study found that Transformers pretrained only on natural language can be finetuned on only 0.03% of parameters and become competitive with LSTMs on a variety of logical and visual tasks, demonstrating transfer learning. The LLaVA was a vision-language model composed of a language model (Vicuna-13B) and a vision model (ViT-L/14), connected by a linear layer. Only the linear layer is finetuned.\nVision transformers adapt the transformer to computer vision by breaking down input images as a series of patches, turning them into vectors, and treating them like tokens in a standard transformer.\nConformer and later Whisper follow the same pattern for speech recognition, first turning the speech signal into a spectrogram, which is then treated like an image, i.e. broken down into a series of patches, turned into vectors and treated like tokens in a standard transformer.\nPerceivers are a variant of Transformers designed for multimodality.\nFor image generation, notable architectures are DALL-E 1 (2021), Parti (2022), Phenaki (2023), and Muse (2023). Unlike later models, DALL-E is not a diffusion model. Instead, it uses a decoder-only Transformer that autoregressively generates a text, followed by the token representation of an image, which is then converted by a variational autoencoder to an image. Parti is an encoder-decoder Transformer, where the encoder processes a text prompt, and the decoder generates a token representation of an image. Muse is an encoder-only Transformer that is trained to predict masked image tokens from unmasked image tokens. During generation, all input tokens are masked, and the highest-confidence predictions are included for the next iteration, until all tokens are predicted. Phenaki is a text-to-video model. It is a bidirectional masked transformer conditioned on pre-computed text tokens. The generated tokens are then decoded to a video.Applications\nThe transformer has had great success in natural language processing (NLP). Many large language models such as GPT-2, GPT-3, GPT-4, Gemini, AlbertAGPT, Claude, BERT, Grok, XLNet, RoBERTa and ChatGPT demonstrate the ability of transformers to perform a wide variety of NLP-related subtasks and their related real-world applications, including:\n\nmachine translation\ntime series prediction\ndocument summarization\ndocument generation\nnamed entity recognition (NER)\nwriting computer code based on requirements expressed in natural language.\nspeech-to-text\nBeyond traditional NLP, the transformer architecture has had success in other applications, such as:\n\nbiological sequence analysis\nvideo understanding\nprotein folding (such as AlphaFold)\nevaluating chess board positions. Using static evaluation alone (that is, with no Minimax search) transformer achieved an Elo of 2895, putting it at grandmaster level.See also\nseq2seq \u2013 Family of machine learning approaches\nPerceiver \u2013 Variant of Transformer designed for multimodal data\nVision transformer \u2013 Machine learning model for vision processing\nLarge language model \u2013 Type of machine learning model\nBERT (language model) \u2013 Series of language models developed by Google AI\nGenerative pre-trained transformer \u2013 Type of large language model\nT5 (language model) \u2013 Series of large language models developed by Google AINotes\nReferences\n== Further reading ==", "result": "# Introduction\n\nIn deep learning, the transformer is a neural network architecture introduced in the 2017 paper \"Attention Is All You Need\" by Ashish Vaswani et al. at the Research Laboratory of the Massachusetts Institute of Technology (MIT). It was designed to facilitate the use of attention mechanisms in sequence-to-sequence learning, where source and target sequences may have varying lengths.\n\nTransformer models are predominantly utilized for tasks involving the generation of text or other sequential data, such as machine translation, text summarization, image captioning, speech recognition, and language translation. They have been instrumental in advancing the field of natural language processing (NLP) and have become the predominant architecture for neural machine translation.\n\nThe transformer architecture builds upon earlier sequence-to-sequence architectures, which typically consist of recurrent neural networks (RNNs) with long short-term memory (LSTM) units or gated recurrent units (GRUs) for the encoder and/or decoder. These RNN-based architectures suffer from the vanishing gradient problem, which makes it difficult for the model to learn long-range dependencies in the input or output sequences.\n\nThe transformer architecture addresses this limitation by replacing the RNN-based encoder-decoder architecture with one based on attention mechanisms. The attention mechanism allows the model to focus on different parts of the input sequence when generating each part of the output sequence. This enables the model to capture long-range dependencies more effectively and has been shown to improve performance on a range of tasks.\n\nThe transformer architecture consists of an encoder and a decoder, each of which contains multiple layers of self-attention and feed-forward neural networks. The self-attention mechanism computes a weighted average of the input sequence, where the weights are determined by the similarity between the query vector and the key vectors of the input sequence. The resulting weighted average is then passed through a feed-forward neural network to produce the output sequence.\n\nSince the publication of the transformer architecture, it has undergone numerous refinements and extensions. These include techniques for parallelizing the computation of attention, such as the flash attention algorithm, as well as methods for improving the efficiency of the architecture, such as the use of sparsity patterns in the attention mechanism.\n\nDespite these advances, the transformer architecture has faced criticism for its tendency to overfit the training data and its difficulty in capturing certain types of linguistic phenomena, such as recursion and anaphora. As a result, researchers have proposed a range of alternative architectures that seek to combine the strengths of the transformer with other approaches, such as recurrent neural networks and convolutional neural networks.\n\nOverall, the transformer architecture has had a profound impact on the field of deep learning, revolutionizing the way that sequence-to-sequence tasks are approached and enabling significant advances in areas such as natural language processing, computer vision, and speech recognition.\n\nHistory\n\nThe history of the Transformer architecture can be traced back to several key developments in deep learning, particularly in the areas of sequence modeling and attention mechanisms.\n\nOne important precursor to the Transformer architecture was the introduction of the LSTM (Long Short-Term Memory) unit by Sepp Hochreiter and J\u00fcrgen Schmidhuber in 1997. The LSTM is a type of recurrent neural network (RNN) that is designed to overcome the vanishing gradient problem, which limits the ability of traditional RNNs to learn long-range dependencies in sequential data. The LSTM does this by using a gating mechanism that allows it to selectively remember or forget information over time.\n\nAnother important contribution to the development of the Transformer architecture was the attention mechanism, which was first introduced in the context of neural machine translation by Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio in 2015. The attention mechanism allows a neural network to focus on different parts of an input sequence when producing each part of an output sequence, rather than relying solely on the output of a fixed-length context vector as in previous sequence-to-sequence models. This enables the model to attend to different parts of the input sequence at different times, allowing it to better capture the dependencies between elements in the sequence.\n\nBuilding on these ideas, the Transformer architecture was introduced by Ashish Vaswani and colleagues in a 2017 paper titled \"Attention Is All You Need\". The Transformer replaces the RNN-based encoder-decoder architecture with one based entirely on attention mechanisms, eliminating the need for a recurrent hidden state. Instead, the Transformer uses multi-head attention, which allows it to attend to different parts of the input sequence simultaneously, and positional encodings to provide information about the position of each token in the sequence.\n\nSince the introduction of the Transformer architecture, there have been many subsequent developments, including the introduction of variants such as the Transformer-XL, which addresses the limitation of attending only to fixed-length contexts, and the use of self-attention in encoder-only models such as BERT (Bidirectional Encoder Representations from Transformers). These developments have led to significant improvements in the performance of deep learning models on a wide range of natural language processing tasks.\n\nPredecessors\n\nThe Transformer architecture is a neural network architecture that is built on top of two key concepts: sequence modeling and attention mechanisms. Sequence modeling refers to the process of predicting or generating a sequence of outputs based on a sequence of inputs. Attention mechanisms, on the other hand, refer to a class of techniques that allow a neural network to focus on different parts of an input sequence when producing each part of an output sequence.\n\nThe Transformer architecture was not the first neural network architecture to use sequence modeling or attention mechanisms. In fact, these concepts have been around for decades and have been used in a wide range of applications, from speech recognition to machine translation.\n\nOne of the earliest examples of sequence modeling is the Markov chain, which is a statistical model that is used to predict the probability of a sequence of events occurring. Markov chains were first introduced by Andrey Markov in 1906 and have since been used in a wide range of applications, including natural language processing and genetics.\n\nAttention mechanisms have also been around for a long time. One of the earliest examples of an attention mechanism is the attention mechanism used in the human visual system, which allows us to focus our attention on specific parts of our visual field while ignoring others. Attention mechanisms have also been used in a wide range of applications, from computer vision to natural language processing.\n\nThe Transformer architecture builds on these earlier concepts by combining them in a novel way. Specifically, the Transformer uses self-attention mechanisms to allow the model to attend to different parts of the input sequence when generating each part of the output sequence. This allows the model to capture long-range dependencies in the input sequence more effectively than previous sequence-to-sequence models that relied on recurrent neural networks (RNNs).\n\nIn addition to the Transformer architecture, there have been many other neural network architectures that have used sequence modeling and attention mechanisms. For example, the LSTM (Long Short-Term Memory) network, introduced by Sepp Hochreiter and J\u00fcrgen Schmidhuber in 1997, is a type of RNN that is designed to overcome the vanishing gradient problem, which limits the ability of traditional RNNs to learn long-range dependencies in sequential data. The LSTM uses a gating mechanism that allows it to selectively remember or forget information over time, allowing it to capture long-range dependencies in the input sequence.\n\nAnother example of a neural network architecture that uses attention mechanisms is the attention-based encoder-decoder model, introduced by Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio in 2015. This model uses an attention mechanism to allow the decoder to attend to different parts of the input sequence when generating each part of the output sequence. This enables the model to better capture the dependencies between elements in the sequence and has been shown to improve performance on a range of tasks, including machine translation and image captioning.\n\nOverall, the Transformer architecture builds on a rich history of research in sequence modeling and attention mechanisms, and has itself inspired a wide range of subsequent developments in the field.\n\nAttention with seq2seq\n\nThe Transformer architecture builds on the sequence-to-sequence (seq2seq) architecture, which was introduced in the seminal paper \"Neural Machine Translation by Jointly Learning to Align and Translate\" by Ilya Sutskever et al. in 2014. Seq2seq architectures were widely used for tasks such as machine translation and text summarization before the advent of the Transformer architecture.\n\nThe seq2seq architecture consists of two recurrent neural networks (RNNs): an encoder and a decoder. The encoder takes a source sequence (e.g., a sentence in one language) and produces a fixed-length context vector that summarizes the entire sequence. The decoder then uses this context vector to generate a target sequence (e.g., a sentence in another language) by sequentially predicting each word in the target sequence.\n\nWhile seq2seq architectures were effective for many tasks, they suffered from a significant limitation: the RNN-based encoder and decoder could only process sequences of a fixed length. This meant that if the source and target sequences were too long, the model would have to truncate or truncate the sequences, which could lead to information loss and decreased performance.\n\nTo address this limitation, the Transformer architecture introduced a new attention mechanism called self-attention, which allowed the model to attend to different parts of the source sequence when generating each part of the target sequence. This means that the model could consider the entire source sequence when generating each word in the target sequence, without being limited to a fixed-length context vector.\n\nThe self-attention mechanism in the Transformer architecture is based on the dot-product attention, which computes a weighted average of the input sequence based on the similarity between a query vector and key vectors associated with each element in the sequence. The attention mechanism is applied to both the encoder and decoder sides of the architecture, allowing the model to attend to the source sequence in a bidirectional manner.\n\nThe use of the attention mechanism in the Transformer architecture has several advantages over the seq2seq architecture. First, it allows the model to attend to different parts of the source sequence when generating each part of the target sequence, which can lead to better performance on tasks where the source and target sequences have complex dependencies. Second, it eliminates the need for an RNN-based encoder and decoder, which can suffer from the vanishing gradient problem and make training more challenging. Finally, the attention mechanism is highly parallelizable, which makes it possible to train and deploy Transformer models on large-scale computing systems.\n\nDespite these advantages, the Transformer architecture is not without limitations. For example, the self-attention mechanism can be computationally expensive for long sequences, and the model can still suffer from overfitting if not properly regularized. Additionally, the attention mechanism may not capture certain types of linguistic phenomena, such as recursive structures or anaphoric dependencies, which may require more specialized architectural designs.\n\nOverall, the Transformer architecture represents a significant advancement in the seq2seq framework and has revolutionized the field of natural language processing. Its widespread adoption and continued evolution have led to the development of many variants and extensions of the original Transformer architecture, which continue to push the boundaries of what is possible with deep learning for NLP.\n\nParallelizing attention\n\nThe original Transformer architecture, as described in the paper \"Attention Is All You Need\", was designed to facilitate the use of attention mechanisms in sequence-to-sequence learning. However, the attention mechanism in the original Transformer architecture required the computation of pairwise interactions between all tokens in the input and output sequences, making it computationally expensive for long sequences.\n\nTo address this issue, a number of techniques have been proposed to parallelize the computation of attention. One such technique is the Flash Attention algorithm, which was introduced by Touvron et al. in 2023. Flash Attention uses a novel algorithm that allows for the efficient computation of attention scores in parallel, without the need for explicit matrix multiplication. This results in a significant speedup for long sequences and has become a popular choice for implementing the attention mechanism in Transformer models.\n\nAnother technique for parallelizing attention is to use sparse attention patterns, such as the \"sliding window\" or \"top-k\" attention mechanisms. These methods reduce the number of attention computations by limiting the attention to a subset of tokens in the input sequence. While this reduces the computational cost, it can also limit the model's ability to attend to relevant tokens in the input sequence.\n\nIn addition to these techniques, some Transformer architectures have also explored alternative attention mechanisms that are more computationally efficient than the standard dot-product attention. For example, the Linear Attention mechanism proposed by Wang et al. in 2020, computes attention scores using a linear transformation of the input tokens, rather than pairwise dot products. This results in a lower computational cost, but may also limit the expressive power of the attention mechanism.\n\nDespite these efforts to parallelize attention, the attention mechanism remains a computationally intensive component of Transformer models. As a result, there continues to be active research in developing new algorithms and techniques for efficient attention computation, particularly for applications where computational resources are limited.\n\nAI boom era\n\nThe Transformer architecture has had a profound impact on the field of artificial intelligence, particularly in the area of natural language processing (NLP). Since its introduction in the 2017 paper \"Attention Is All You Need\", the Transformer has become the dominant architecture for many NLP tasks, including machine translation, text classification, and question answering.\n\nOne of the key factors driving the success of the Transformer architecture is its ability to capture long-range dependencies in text data. Unlike traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs), which rely on local connectivity, the Transformer uses self-attention mechanisms to allow each position in the input sequence to attend to all positions in the output sequence. This enables the Transformer to model complex relationships between words and phrases in a text, even when they are far apart in the sequence.\n\nThe popularity of the Transformer architecture has led to a proliferation of variants and extensions, each aimed at addressing specific limitations or improving performance on particular tasks. For example, the Transformer-XL model extends the Transformer to handle longer-range dependencies by introducing a mechanism for segmenting the input sequence and sharing parameters across segments. Other variants, such as the BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer) models, use pre-training techniques to improve the model's ability to generalize to a wide range of downstream tasks.\n\nBeyond NLP, the Transformer architecture has also been applied to other domains, such as computer vision and speech recognition. In computer vision, the Transformer has been used to achieve state-of-the-art performance on tasks such as image classification and object detection. Similarly, in speech recognition, the Transformer has been shown to outperform traditional models on tasks such as automatic speech recognition.\n\nThe success of the Transformer architecture has also spurred significant interest in developing more efficient and scalable versions of the model. For example, recent work has focused on developing techniques for pruning and quantizing Transformer models, as well as designing architectures that are better suited for deployment on resource-constrained devices.\n\nOverall, the Transformer architecture has had a transformative impact on the field of AI, enabling significant advances in NLP and inspiring new research directions in other domains. As the demand for more powerful and efficient models continues to grow, it is likely that the Transformer will remain a central focus of research and development in the years to come.\n\nTraining\n\nTraining a transformer involves optimizing the model's parameters to minimize the difference between its predicted output and the true output. This is typically done using stochastic gradient descent or a variant thereof, such as Adam. During training, the model is presented with a batch of input-output pairs, and the loss function is computed as the average loss over the batch. The gradients of the loss function with respect to the model's parameters are then computed using backpropagation, and the parameters are updated in the direction that minimizes the loss.\n\nThere are several techniques that can be used to improve the training of transformer models. One common technique is to use learning rate schedules that gradually decrease the learning rate over the course of training, which can help the model converge to a lower loss. Another technique is to use layer normalization, which helps to stabilize the activations of the model and improve training convergence.\n\nOther techniques that have been found to be effective in practice include dropout regularization, which helps to prevent overfitting by randomly setting a subset of activations to zero during training; and weight decay, which adds a regularization term to the loss function that penalizes large weights and helps to prevent overfitting.\n\nFinally, it is often useful to use pretrained transformer models as a starting point for training on a downstream task. This can significantly reduce the amount of data and compute required to achieve good performance, as the pretrained model already has a good understanding of the general properties of text data.\n\nMethods for stabilizing training\n\nTransformer models can be difficult to train due to issues such as vanishing or exploding gradients, especially when dealing with long sequences. To address these problems, several methods have been developed to stabilize the training process.\n\nOne common approach is to use learning rate warmup, where the learning rate is initially set to a small value and gradually increased to the desired level over the course of the first few hundred iterations. This allows the model to take small, controlled steps in parameter space during the early stages of training, which can help to avoid large updates that might cause instability.\n\nAnother technique that has proven effective is the use of layer normalization, which normalizes the inputs to each layer of the model, regardless of the activation function used. This helps to stabilize the distribution of activations across layers, reducing the risk of exploding or vanishing gradients.\n\nFinally, some researchers have explored modifications to the attention mechanism itself to improve stability. For example, the Flash Attention algorithm uses a novel approach to computing the attention scores that avoids explicit matrix multiplication, instead using a set of look-up tables that are precomputed during training. This allows the attention mechanism to be computed much faster, while also reducing the risk of numerical instability.\n\nPretrain-finetune\n\nThe Transformer architecture is typically trained using a combination of pretraining and fine-tuning. Pretraining involves training the model on a large corpus of unlabeled text, using a technique such as masked language modeling or next sentence prediction. This allows the model to learn general-purpose representations of text that can be applied to a wide range of downstream tasks.\n\nOnce the model has been pretrained, it can be fine-tuned on a specific task by adding task-specific layers and adjusting the model's parameters to optimize performance on the task. For example, for a sentiment classification task, a classifier layer might be added to the output of the Transformer, and the model might be fine-tuned by optimizing the weights of this classifier layer.\n\nFine-tuning is often more sample-efficient than training a model from scratch, since the pretrained model already has a good understanding of the general properties of text data. This can be particularly useful when labeled data for a downstream task is scarce.\n\nIn addition to fine-tuning, there has been significant research on transfer learning for the Transformer architecture. Transfer learning involves taking a pretrained model and adapting it to a new task by training only a subset of the model's parameters. For example, the BERT (Bidirectional Encoder Representations from Transformers) model is pretrained on two tasks \u2013 masked language modeling and next sentence prediction \u2013 and can be fine-tuned for a wide range of NLP tasks by adding a task-specific head and training only the weights of this head.\n\nThis approach has proven to be highly effective, and BERT-like models have achieved state-of-the-art performance on a wide range of NLP tasks. In addition, there have been several efforts to create larger and more powerful models, such as GPT-3 and BERT-large, which have further improved the performance of the Transformer architecture on a range of tasks.\n\nTasks\n\nLanguage modeling is a task in which the goal is to predict the next word in a sequence of words, given the preceding words in the sequence. Language models are commonly used as building blocks for other NLP tasks, such as machine translation and text generation, and are also used in their own right to evaluate the quality of a text.\n\nThere are two main types of language models: autoregressive and denoising. Autoregressive language models predict the next word in a sequence conditioned on the preceding words, while denoising language models predict the original version of a corrupted input sequence. Denoising language models are typically trained using a technique called generative adversarial nets (GANs), which involves training a generator model to produce clean sequences from corrupted inputs and a discriminator model to distinguish between the generated and true sequences.\n\nLanguage modeling is closely related to the problem of sequence-to-sequence learning, in which the goal is to learn a mapping from one sequence to another, such as from a source language to a target language in machine translation. Sequence-to-sequence models typically consist of an encoder-decoder architecture, where the encoder transforms the input sequence into a fixed-length representation, and the decoder generates the output sequence conditioned on the encoder representation.\n\nLanguage modeling has a wide range of applications, including text generation, machine translation, and speech recognition. In text generation, language models are used to generate text that is similar to human-written text, while in machine translation, language models are used to improve the quality of the translated text by predicting the most likely translation given the source text.\n\nLanguage modeling is also used in information retrieval, where it is used to rank documents based on their relevance to a given query. In this context, the language model is trained to predict the probability of a document given the query, and the documents with the highest probabilities are ranked highest.\n\nOverall, language modeling is a fundamental task in natural language processing, and the Transformer architecture has had a significant impact on the state-of-the-art in this area.\n\n--- Masked\n\nMasked language modeling (MLM) is a task in which the goal is to predict the missing words in a sentence, given the surrounding context. MLM is a common pretraining objective for transformer language models, where the input sentence is partially masked by replacing some of the words with a special token, and the model is trained to predict the missing words based on the context provided by the remaining words.\n\nFor example, given the sentence \"The cat sat on the __\", the word \"cat\" might be replaced with a special token, and the model would be trained to predict the missing word \"mat\". MLM allows the model to learn contextualized word representations that capture the meaning of a word in different contexts, as well as relationships between words.\n\nDuring pretraining, MLM is typically applied to a large corpus of text, such as a web crawl or a collection of books. The model is trained to maximize the log-likelihood of the masked words in the training data. This encourages the model to learn generalizable representations that can be applied to a wide range of downstream tasks.\n\nMLM has been shown to be effective for a variety of downstream tasks, including text classification, question answering, and named entity recognition. This is because MLM encourages the model to learn rich, context-sensitive representations that capture the semantic and syntactic properties of words.\n\n--- Autoregressive\n\nAutoregressive language modeling (ARLM) is a task in which the goal is to predict the next word in a sequence of words, given the preceding words in the sequence. ARLM is a common objective for transformer language models, where the model is trained to predict the next word in a sequence based on the preceding words in the sequence.\n\nFor example, given the sentence \"The cat sat on the\", the model might be trained to predict the next word in the sequence based on the context provided by the preceding words. ARLM allows the model to learn contextualized word representations that capture the meaning of a word in different contexts, as well as relationships between words.\n\nDuring pretraining, ARLM is typically applied to a large corpus of text, such as a web crawl or a collection of books. The model is trained to maximize the log-likelihood of the next words in the training data. This encourages the model to learn generalizable representations that can be applied to a wide range of downstream tasks.\n\nARLM has been shown to be effective for a variety of downstream tasks, including text classification, question answering, and named entity recognition. This is because ARLM encourages the model to learn rich, context-sensitive representations that capture the semantic and syntactic properties of words.\n\n--- PrefixLM\n\nPrefix language modeling (PPLM) is a task in which the goal is to predict the next word in a sequence of words, given the preceding words in the sequence. Unlike autoregressive language modeling (ARLM), which predicts the next word in a sequence conditioned on all preceding words, PPLM only conditions on a prefix of the sequence.\n\nFor example, given the sentence \"The cat sat on the\", the model might be trained to predict the next word in the sequence based on the context provided by the preceding words, but not necessarily the entire sequence. PPLM allows the model to learn contextualized word representations that capture the meaning of a word in different contexts, as well as relationships between words.\n\nDuring pretraining, PPLM is typically applied to a large corpus of text, such as a web crawl or a collection of books. The model is trained to maximize the log-likelihood of the next words in the training data, given the prefix of the sequence. This encourages the model to learn generalizable representations that can be applied to a wide range of downstream tasks.\n\nPPLM has been shown to be effective for a variety of downstream tasks, including text classification, question answering, and named entity recognition. This is because PPLM encourages the model to learn rich, context-sensitive representations that capture the semantic and syntactic properties of words.\n\nArchitecture\n\nThe Transformer model is composed of three main components: the tokenizer, the embedding layer, and the transformer layers.\n\nThe tokenizer is responsible for converting raw text into a format that can be processed by the model. The most commonly used tokenizers are the Byte Pair Encoding (BPE) and SentencePiece (SP) tokenizers. The tokenizer breaks down the input text into individual tokens, which are then converted into integer IDs that can be used by the embedding layer.\n\nThe embedding layer is responsible for transforming the integer IDs produced by the tokenizer into dense vector representations, or embeddings, that capture the semantic meaning of each token. The embeddings are learned during training and are shared across all parts of the model.\n\nThe transformer layers are the core of the Transformer architecture. Each transformer layer consists of two main components: the multi-head self-attention mechanism and the position-wise fully connected feed-forward network. The multi-head self-attention mechanism allows the model to attend to different parts of the input sequence simultaneously, while the position-wise fully connected feed-forward network allows the model to learn nonlinear transformations of the attention outputs.\n\nThe architecture of the Transformer model can be summarized as follows:\n\nTokenizer \u2192 Embedding Layer \u2192 Transformer Layers \u2192 Output Layer\n\nIn this architecture, the input text is first tokenized by the tokenizer, and the resulting tokens are then passed through the embedding layer to obtain their corresponding embeddings. The embeddings are then passed through a series of transformer layers, which transform the embeddings into a high-dimensional representation of the input text. Finally, the output layer is used to produce the final output of the model, such as a predicted label or a generated text sequence.\n\nTokenization\n\nTokenization is the process of breaking down raw text into smaller units called tokens, which can then be processed by the Transformer model. The most commonly used tokenizers are the Byte Pair Encoding (BPE) and SentencePiece (SP) tokenizers.\n\nThe BPE tokenizer works by iteratively merging the most frequent pairs of bytes in the text until a maximum vocabulary size is reached. The resulting tokens are represented as sequences of bytes, along with information about the merges that were performed.\n\nThe SP tokenizer is similar to BPE, but operates on the level of subwords instead of bytes. SP tokenizers first build a vocabulary of subword units by analyzing the training data, and then use this vocabulary to tokenize the input text by splitting words into subwords and merging adjacent subwords that appear frequently in the training data.\n\nBoth BPE and SP tokenizers have advantages and disadvantages. BPE is generally faster and simpler to implement, but can produce tokens that are difficult to interpret, while SP tokenizers tend to produce more readable tokens, but can be more memory-intensive.\n\nEmbedding\n\nThe embedding layer is a crucial component of the Transformer architecture, as it transforms the discrete token indices into continuous vector representations that capture the semantic and syntactic information of the input text. The embedding layer is essentially a lookup table, where each unique token in the vocabulary is mapped to a corresponding vector of fixed dimensionality.\n\nThe embeddings are initialized randomly and are learned during training, allowing the model to discover meaningful representations of the input tokens. The embeddings are shared across all parts of the model, including the self-attention mechanism and the feed-forward layers, which allows the model to reuse the same representations across different parts of the architecture.\n\nThe size of the embedding vectors can vary depending on the specific task and dataset, but common values are 256, 512, and 1024. The dimensionality of the embeddings affects the capacity of the model to represent the input text, with higher-dimensional embeddings allowing for more fine-grained representations but also increasing the computational cost of the model.\n\nUn-embedding\n\nThe un-embedding layer is the inverse of the embedding layer, and is used to convert the last layer of the transformer model back into predicted logits for each token in the output vocabulary. The un-embedding layer performs a matrix multiplication between the output of the transformer model and the transpose of the embedding matrix, which is the same matrix used to embed the input tokens.\n\nThe resulting matrix has a row for each token in the output vocabulary and a column for each token in the output sequence. Each element in the matrix represents the log-probability of the corresponding output token given the input sequence. The token with the highest log-probability is selected as the predicted token for that time step.\n\nThe un-embedding layer is necessary because the embedding layer maps each token in the input vocabulary to a low-dimensional vector, which loses the probabilistic information needed for prediction. By using the un-embedding layer, the transformer model can output a probability distribution over the entire output vocabulary at each time step, allowing for flexible and context-aware token predictions.\n\nPositional encoding\n\nPositional encoding is a technique used in the Transformer architecture to provide information about the position of each token in the input sequence. Unlike recurrent neural networks (RNNs), which process sequences in a sequential manner, the Transformer architecture processes all tokens in parallel. As a result, the Transformer lacks the inherent notion of position or order that is present in RNNs.\n\nTo address this issue, positional encoding is added to the input embeddings of the Transformer model. The positional encoding is a learnable parameter that is added to the embedding vector of each token in the input sequence. The positional encoding is designed to capture the relative positions of the tokens in the sequence, allowing the model to distinguish between tokens that occur at different positions, even if they have the same embedding representation.\n\nThe positional encoding is computed using a sinusoidal function that varies with the position of the token in the sequence and the dimension of the embedding vector. Specifically, the positional encoding for the i-th token at the d-th dimension is given by:\n\n    PE_(i,d)=sin(i/10000^2d/depth)\n\nwhere depth is the total dimensionality of the embedding vector. The even-numbered dimensions of the positional encoding vector are computed using the sine function, while the odd-numbered dimensions are computed using the cosine function.\n\nBy adding the positional encoding to the input embeddings, the Transformer model is able to learn position-sensitive representations of the input sequence. This allows the model to capture important aspects of the input sequence, such as the order of tokens and the relative importance of different tokens in the sequence.\n\nEncoder-decoder (overview)\n\nThe encoder-decoder architecture is a fundamental building block of the Transformer model, which is widely used in natural language processing (NLP) tasks such as machine translation and text summarization. The architecture consists of two main components: the encoder, which processes the input sequence and produces a sequence of hidden states, and the decoder, which generates the output sequence based on the encoder's hidden states.\n\nThe encoder is a stack of multiple layers of self-attention and feed-forward neural networks. The self-attention mechanism allows the encoder to attend to different parts of the input sequence when generating each hidden state, which enables the model to capture the contextual information of the input sequence. The feed-forward neural networks provide non-linear transformations to the hidden states, which allows the model to capture complex patterns in the input sequence.\n\nThe decoder is also a stack of multiple layers of self-attention and feed-forward neural networks, but it includes an additional attention mechanism called the encoder-decoder attention. The encoder-decoder attention mechanism allows the decoder to attend to the encoder's hidden states when generating each output token, which enables the model to generate output tokens that are contextually relevant to the input sequence.\n\nThe decoder starts by generating a special start token, which signals the beginning of the output sequence. Then, at each time step, the decoder generates a new output token by attending to both its previous output token and the encoder's hidden states. The generated output token is then fed back into the decoder as input for the next time step.\n\nThe encoder-decoder architecture is a powerful tool for NLP tasks because it allows the model to capture the contextual information of the input sequence and generate output sequences that are contextually relevant to the input. By using the encoder-decoder architecture, the Transformer model has achieved state-of-the-art performance on a wide range of NLP tasks.\n\nFeedforward network\n\nThe feedforward network is a crucial component of the Transformer architecture, which is widely used in natural language processing (NLP) tasks such as machine translation and text summarization. The feedforward network is a fully connected neural network that is applied to each position in the sequence independently, and it is used in conjunction with the self-attention mechanism to capture complex patterns in the input sequence.\n\nThe feedforward network in the Transformer architecture consists of two linear layers with a nonlinear activation function (such as the GELU or ReLU activation function) in between. The input to the feedforward network is the output of the self-attention mechanism, and the output is fed into the next layer of self-attention.\n\nThe feedforward network in the Transformer architecture is defined as follows:\n\n    FFN(x) = GELU(W_2 \u00b7 GELU(W_1 x + b_1) + b_2)\n\nwhere x is the input to the feedforward network, W_1 and W_2 are learnable weight matrices, b_1 and b_2 are learnable bias vectors, and GELU is the Gaussian Error Linear Unit activation function.\n\nThe feedforward network in the Transformer architecture is applied to each position in the sequence independently, which means that the output at each position does not depend on the output at other positions. This allows the Transformer to parallelize the computation of the feedforward network across multiple positions in the sequence, which speeds up the training and inference process.\n\nScaled dot-product attention\n\nThe scaled dot-product attention is a fundamental building block of the Transformer architecture, which is widely used in natural language processing (NLP) tasks such as machine translation and text summarization. The scaled dot-product attention is a generalization of the dot-product attention mechanism, which was originally proposed in the attention mechanism paper by Bahdanau et al. (2015).\n\nThe scaled dot-product attention takes as input three matrices: the query matrix Q, the key matrix K, and the value matrix V. The query matrix Q is obtained by transforming the input sequence using a linear projection, while the key matrix K and value matrix V are obtained by transforming the input sequence using separate linear projections.\n\nThe scaled dot-product attention computes the attention scores by taking the dot product of the query matrix Q with the key matrix K^T (the transpose of K), and then scaling the result by the square root of the key dimensionality d_k. The attention scores are then normalized using the softmax function to obtain the attention weights, which are used to weigh the value matrix V to obtain the output of the attention mechanism.\n\nThe scaled dot-product attention can be expressed mathematically as follows:\n\n    Attention(Q, K, V) = softmax(Q K^T/\u221a(d_k)) V\n\nwhere Q, K, and V are the query, key, and value matrices, respectively, and d_k is the key dimensionality.\n\nThe scaled dot-product attention has several advantages over other attention mechanisms. First, it is computationally efficient, as it only requires matrix multiplication and softmax operations. Second, it is invariant to the scaling of the query and key matrices, which makes it more stable and robust to changes in the input data. Third, it allows the model to focus on different parts of the input sequence when generating each output token, which enables the model to capture complex dependencies in the input sequence.\n\nAttention head\n\nThe attention head is a fundamental building block of the Transformer architecture, which is widely used in natural language processing (NLP) tasks such as machine translation and text summarization. The attention head is a parallelized version of the scaled dot-product attention mechanism, which allows the model to attend to different parts of the input sequence simultaneously.\n\nEach attention head takes as input a query matrix Q, a key matrix K, and a value matrix V, and computes the attention scores and weights in parallel. The query, key, and value matrices are obtained by transforming the input sequence using separate linear projections, and then applying a split operation to divide the sequence into multiple heads.\n\nThe attention scores for each head are computed independently, and then concatenated together to form the final attention scores. The attention weights are then computed by applying the softmax function to the attention scores, and the value matrix is weighed using the attention weights to obtain the output of the attention head.\n\nThe attention head can be expressed mathematically as follows:\n\n    MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O\n\nwhere head_i = Attention(QW_i^Q, KW_i^K, VW_i^V) is the attention output for the i-th head, W^O is a final projection matrix, and h is the number of attention heads.\n\nThe attention head has several advantages over the single-headed scaled dot-product attention mechanism. First, it allows the model to attend to different parts of the input sequence simultaneously, which increases the model's capacity to capture complex dependencies in the input sequence. Second, it allows the model to learn different attention patterns for each head, which enables the model to specialize in different aspects of the input sequence. Third, the attention heads can be computed in parallel, which speeds up the training and inference process.\n\nMultiheaded attention\n\nThe multiheaded attention is a key component of the Transformer architecture, which is widely used in natural language processing (NLP) tasks such as machine translation and text summarization. The multiheaded attention allows the model to attend to different parts of the input sequence simultaneously, which increases the model's capacity to capture complex dependencies in the input sequence.\n\nThe multiheaded attention is an extension of the scaled dot-product attention mechanism, which introduces multiple attention heads that operate in parallel. Each attention head learns to attend to different parts of the input sequence, and the outputs of all heads are concatenated together to form the final output of the multiheaded attention mechanism.\n\nThe multiheaded attention can be expressed mathematically as follows:\n\n    MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O\n\nwhere head_i = Attention(QW_i^Q, KW_i^K, VW_i^V) is the attention output for the i-th head, W^O is a final projection matrix, and h is the number of attention heads.\n\nThe multiheaded attention has several advantages over the single-headed scaled dot-product attention mechanism. First, it allows the model to attend to different parts of the input sequence simultaneously, which increases the model's capacity to capture complex dependencies in the input sequence. Second, it allows the model to learn different attention patterns for each head, which enables the model to specialize in different aspects of the input sequence. Third, the attention heads can be computed in parallel, which speeds up the training and inference process.\n\nMasked attention\n\nMasked attention is a technique used in the Transformer architecture to prevent the model from attending to future tokens during training. This is important because the Transformer architecture uses self-attention to compute the relationships between tokens in a sequence, and if the model is allowed to attend to future tokens, it could potentially \"cheat\" by using information that it hasn't seen yet.\n\nTo prevent this, masked attention is used during training to mask out the upper triangular portion of the attention matrix. This means that when the model is computing the attention scores for a given token, it is not allowed to attend to any tokens that come after it in the sequence.\n\nMathematically, the masked attention can be expressed as:\n\n    MaskedAttention(Q, K, V) = softmax(Q K^T/\u221a(d_k)) V\n\nwhere the attention scores are computed as:\n\n    AttentionScores = Q K^T/\u221a(d_k)\n\nand the attention weights are computed as:\n\n    AttentionWeights = softmax(AttentionScores - M)\n\nwhere M is a large negative constant that is applied to the upper triangular portion of the attention scores matrix, effectively masking it out.\n\nDuring inference, when the model is generating tokens one at a time, masked attention is not needed because the model only attends to previously-generated tokens. Instead, the model generates each token by attending only to the tokens that have already been generated.\n\nThe use of masked attention during training allows the Transformer architecture to learn a causal relationship between tokens in a sequence, where each token is generated based only on the previous tokens in the sequence. This is important for tasks such as language modeling, where the goal is to generate a sequence of tokens that are coherent and grammatically correct.\n\nCausal masking\n\nCausal masking is a technique used in the Transformer architecture to prevent the model from attending to future tokens during training. This is important because the Transformer architecture uses self-attention to compute the relationships between tokens in a sequence, and if the model is allowed to attend to future tokens, it could potentially \"cheat\" by using information that it hasn't seen yet.\n\nTo prevent this, causal masking is used during training to mask out the upper triangular portion of the attention matrix. This means that when the model is computing the attention scores for a given token, it is not allowed to attend to any tokens that come after it in the sequence.\n\nMathematically, the causal masking can be expressed as:\n\n    CausalMaskedAttention(Q, K, V) = softmax(Q K^T/\u221a(d_k)) V\n\nwhere the attention scores are computed as:\n\n    AttentionScores = Q K^T/\u221a(d_k)\n\nand the attention weights are computed as:\n\n    AttentionWeights = softmax(AttentionScores - M)\n\nwhere M is a large negative constant that is applied to the upper triangular portion of the attention scores matrix, effectively masking it out.\n\nDuring inference, when the model is generating tokens one at a time, causal masking is not needed because the model only attends to previously-generated tokens. Instead, the model generates each token by attending only to the tokens that have already been generated.\n\nThe use of causal masking during training allows the Transformer architecture to learn a causal relationship between tokens in a sequence, where each token is generated based only on the previous tokens in the sequence. This is important for tasks such as language modeling, where the goal is to generate a sequence of tokens that are coherent and grammatically correct.\n\nEncoder\n\nThe encoder is a component of the Transformer architecture that processes the input sequence and produces a sequence of hidden states that capture the contextual information of the input sequence. The encoder is composed of a stack of identical layers, each of which consists of a multi-headed self-attention mechanism and a position-wise feed-forward neural network.\n\nThe multi-headed self-attention mechanism allows the encoder to attend to different parts of the input sequence when generating each hidden state, which enables the model to capture the contextual information of the input sequence. The position-wise feed-forward neural network provides non-linear transformations to the hidden states, which allows the model to capture complex patterns in the input sequence.\n\nThe encoder can be expressed mathematically as follows:\n\n    Encoder(X) = LayerNorm[ X + EncoderStack(X) ]\n\nwhere X is the input sequence, EncoderStack(X) is the stack of encoder layers, and LayerNorm is the layer normalization operation.\n\nThe encoder has several advantages over other sequence modeling architectures, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs). First, the encoder processes all tokens in parallel, which makes it much faster than RNNs, which process tokens sequentially. Second, the encoder captures long-range dependencies in the input sequence, which CNNs are not able to do. Third, the encoder is able to capture contextual information of the input sequence, which is important for tasks such as language modeling and machine translation.\n\nDecoder\n\nThe decoder is a component of the Transformer architecture that generates the output sequence based on the encoder's hidden states. The decoder is composed of a stack of identical layers, each of which consists of a multi-headed self-attention mechanism and a position-wise feed-forward neural network.\n\nThe multi-headed self-attention mechanism allows the decoder to attend to both its previous output token and the encoder's hidden states when generating each output token, which enables the model to generate output tokens that are contextually relevant to the input sequence. The position-wise feed-forward neural network provides non-linear transformations to the hidden states, which allows the model to capture complex patterns in the output sequence.\n\nThe decoder can be expressed mathematically as follows:\n\n    Decoder(Y, mem) = LayerNorm[ Y + DecoderStack(Y, mem) ]\n\nwhere Y is the output sequence, mem is the encoder's hidden states, DecoderStack(Y, mem) is the stack of decoder layers, and LayerNorm is the layer normalization operation.\n\nThe decoder has several advantages over other sequence modeling architectures, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs). First, the decoder processes all tokens in parallel, which makes it much faster than RNNs, which process tokens sequentially. Second, the decoder captures long-range dependencies in the output sequence, which CNNs are not able to do. Third, the decoder is able to generate output tokens that are contextually relevant to the input sequence, which is important for tasks such as language modeling and machine translation.\n\nAdapted architectures\n\nLarge language models (LLMs) are typically built on top of the Transformer architecture, which has become the de facto standard for state-of-the-art language models. There are two main types of LLMs: encoder-only and decoder-only. Encoder-only LLMs, such as BERT, are pre-trained on a large corpus of text using masked language modeling and next sentence prediction objectives, and are then fine-tuned on a variety of downstream tasks such as sentiment analysis, question answering, and named entity recognition. Decoder-only LLMs, such as GPT, are pre-trained on a large corpus of text using autoregressive language modeling objective and are then fine-tuned on a variety of downstream tasks such as text generation, dialogue response generation, and machine translation.\n\nThere are also hybrid architectures that combine the strengths of both encoder-only and decoder-only LLMs. For example, the T5 (Text-to-Text Transfer Transformer) model by Google is an encoder-decoder Transformer model that is pre-trained on a wide range of text-to-text tasks, such as summarization, translation, and question answering, using a unified text-to-text formulation. Similarly, the BART (Bidirectional and Auto-Regressive Transformers) model by Facebook is an encoder-decoder Transformer model that is pre-trained on a variety of tasks, such as abstractive summarization, text de-identificaton, and text simplification, using a combination of denoising and language modeling objectives.\n\nOverall, the Transformer architecture has become the go-to architecture for building LLMs due to its scalability, flexibility, and state-of-the-art performance on a wide range of NLP tasks. As LLMs continue to grow in size and complexity, the Transformer architecture will likely remain a key component of many state-of-the-art models.\n\nFull transformer architecture\n\nThe full transformer architecture consists of an encoder and a decoder, each composed of multiple layers of self-attention and feed-forward neural networks. The self-attention mechanism allows the model to attend to different parts of the input sequence when generating each output token, while the feed-forward neural networks provide non-linear transformations to the hidden states.\n\nThe architecture can be expressed mathematically as follows:\n\n    Output = LayerNorm[Output + Decoder(X, mem)]\n\nwhere X is the input sequence, mem is the encoder's hidden states, Decoder(X, mem) is the stack of decoder layers, and LayerNorm is the layer normalization operation.\n\nSublayers\n\nThe Transformer architecture consists of several sublayers that are used to process the input and output sequences. These sublayers include the self-attention mechanism, the position-wise feed-forward neural network, and the layer normalization operation.\n\nThe self-attention mechanism is used to compute the attention scores between the query, key, and value matrices, and to weigh the value matrix using the attention weights to obtain the output of the attention mechanism. The position-wise feed-forward neural network is used to provide non-linear transformations to the hidden states. The layer normalization operation is used to normalize the hidden states and stabilize the training process.\n\nThese sublayers can be expressed mathematically as follows:\n\n    SublayerOutput = f(LayerNorm[X + Sublayer(X)])\n\nwhere f is a sublayer function, such as the self-attention mechanism or the position-wise feed-forward neural network, and LayerNorm is the layer normalization operation.\n\nPseudocode\n\nThe pseudocode for the pre-LN encoder-decoder Transformer architecture can be expressed as follows:\n\n    def Transformer(X, mem):\n        X = LayerNorm[X + EncoderStack(X)]\n        Y = LayerNorm[X + DecoderStack(X, mem)]\n        return Y\n\nwhere X is the input sequence, mem is the encoder's hidden states, EncoderStack(X) is the stack of encoder layers, DecoderStack(X, mem) is the stack of decoder layers, and LayerNorm is the layer normalization operation.\n\nTerminology\n\nThere are several terminologies used in the Transformer architecture that are worth noting:\n\n* Multi-headed attention: A mechanism that allows the model to attend to different parts of the input sequence simultaneously using multiple attention heads. Each head learns to attend to different parts of the input sequence, and the outputs of all heads are concatenated together to form the final output of the multi-headed attention mechanism.\n* Positional encoding: A technique used to provide information about the position of each token in the input sequence. The positional encoding is added to the input embeddings of the Transformer model, allowing the model to distinguish between tokens that occur at different positions in the sequence.\n* Layer normalization: A technique used to normalize the hidden states of the Transformer model, which helps to stabilize the training process and improve the model's performance.\n* Dropout: A regularization technique used to prevent overfitting of the Transformer model by randomly dropping out neurons during training.\n* Feed-forward network: A fully connected neural network that is applied to each position in the sequence independently, and is used in conjunction with the self-attention mechanism to capture complex patterns in the input sequence.\n* Scaled dot-product attention: A type of attention mechanism that computes the attention scores by taking the dot product of the query matrix Q with the key matrix K^T (the transpose of K), and then scaling the result by the square root of the key dimensionality.\n\nSubsequent work\n\nThe Transformer architecture has been the subject of extensive research since its introduction in 2017. Many subsequent works have proposed modifications and extensions to the original architecture, including techniques for improving efficiency, handling longer contexts, and incorporating external knowledge sources.\n\nSome notable works include the introduction of the SwiGLU activation function in the GPT-3 architecture, which improves the model's expressiveness by using a gating mechanism to control the flow of information through the model. Other works have proposed alternative normalization techniques, such as RMSNorm and CapsuleNorm, which improve the stability and generalization of the model. Additionally, works such as LongFormer and Reformer have proposed methods for handling longer contexts than the original Transformer architecture, while works such as XLNet and BERT have incorporated techniques for pretraining on large amounts of unlabeled text data, enabling the model to learn general-purpose representations that can be applied to a wide range of downstream tasks.\n\nAlternative activation functions\n\nThe Transformer architecture has traditionally used the GELU (Gaussian Error Linear Unit) activation function for its feed-forward layers. However, some researchers have proposed alternative activation functions that can improve the performance of the model.\n\nOne such alternative is the SwiGLU (Swish-Gated Linear Unit) activation function, which was introduced in the GPT-3 architecture. The SwiGLU activation function combines the Swish activation function with the GLU activation function, which provides a gating mechanism that allows the model to control the flow of information through the layer. The SwiGLU activation function is defined as follows:\n\n    SwiGLU(x) = x \u00b7\u03c3(xW + b),\n\nwhere \u03c3 is the sigmoid activation function, x is the input to the layer, W is a learnable weight matrix, and b is a learnable bias vector.\n\nAnother alternative activation function that has been proposed for the Transformer architecture is the Mish activation function. The Mish activation function is defined as follows:\n\n    Mish(x) = x \u00b7tanh(ln(1 + e^x)),\n\nwhere x is the input to the layer, t is the hyperbolic tangent function, and ln is the natural logarithm.\n\nThese alternative activation functions have been shown to improve the performance of the Transformer architecture on a range of tasks, including language modeling and machine translation. However, the choice of activation function may depend on the specific task and dataset, and it is often necessary to experiment with different activation functions to find the best one for a given task.\n\nAlternative normalizations\n\nThe Transformer architecture uses layer normalization (LN) to normalize the hidden states of the model. However, some researchers have proposed alternative normalization techniques that can improve the performance and efficiency of the model.\n\nOne such alternative normalization technique is RMSNorm, which was introduced in the GPT-3 architecture. RMSNorm is a variant of layer normalization that uses the root mean square (RMS) of the hidden states instead of the variance to normalize the hidden states. RMSNorm is defined as follows:\n\n    x_RMSNorm = x / \u221a(E[x^2] + \u03f5),\n\nwhere x is the hidden states, E is the expectation operator, \u03f5 is a small constant to prevent division by zero, and \u221a denotes the square root function.\n\nAnother alternative normalization technique that has been proposed for the Transformer architecture is CapsuleNorm. CapsuleNorm is a normalization technique that is inspired by the capsule networks, which are a type of neural network that are designed to handle complex and ambiguous data such as images and text. CapsuleNorm is defined as follows:\n\n    x_CapsuleNorm = x / ||x||_\u221e,\n\nwhere x is the hidden states and ||\u00b7||_\u221e is the infinity norm, which computes the maximum absolute value of the hidden states.\n\nThese alternative normalization techniques have been shown to improve the performance and efficiency of the Transformer architecture on a range of tasks, including language modeling and machine translation. However, the choice of normalization technique may depend on the specific task and dataset, and it is often necessary to experiment with different normalization techniques to find the best one for a given task.\n\nAlternative positional encodings\n\nThe Transformer architecture uses a fixed positional encoding that is added to the input embeddings to provide information about the position of each token in the sequence. However, some researchers have proposed alternative positional encodings that can improve the performance of the model.\n\nOne such alternative positional encoding is learned positional encoding, which replaces the fixed positional encoding with learnable embeddings that are added to the input embeddings. The learned positional embeddings are learned during training and can adapt to the specific characteristics of the input sequence.\n\nAnother alternative positional encoding that has been proposed for the Transformer architecture is sinusoidal positional encoding, which is similar to the fixed positional encoding used in the original architecture, but uses a different formula to compute the positional embeddings. Sinusoidal positional encoding is defined as follows:\n\n    pos_emb_i,2k   = sin(pos/10000^2k/d_model)\n    \n    pos_emb_i,2k+1   = cos(pos/10000^2k/d_model)\n\nwhere pos_emb_i,j is the j-th positional embedding for the i-th position in the sequence, pos is the position index, k is the dimension index, and d_model is the dimensionality of the model.\n\nThese alternative positional encodings have been shown to improve the performance of the Transformer architecture on a range of tasks, including language modeling and machine translation. However, the choice of positional encoding may depend on the specific task and dataset, and it is often necessary to experiment with different positional encodings to find the best one for a given task.\n\nRoPE\n\nRotary Positional Embedding (RoPE) is a technique used in the Transformer architecture to encode the position of tokens in a sequence. Unlike traditional positional encodings, which add fixed positional embeddings to the input embeddings, RoPE uses a rotary embedding that rotates the input embeddings to encode the position of each token.\n\nSpecifically, RoPE applies a rotation to the input embeddings using a set of learnable rotation matrices, where each rotation matrix corresponds to a different position in the sequence. The rotated embeddings are then combined with the original embeddings to produce the final positional embeddings.\n\nThe rotation matrices are constructed using the following formula:\n\n    R_i = [  cos(\u03b8_i) -sin(\u03b8_i);  sin(\u03b8_i)  cos(\u03b8_i) ]\n\nwhere \u03b8_i = 10000^-2i/L and i is the position index and L is the maximum position.\n\nThe rotated embeddings are computed using the following formula:\n\n    E_rotated = R E\n\nwhere E is the input embeddings and E_rotated is the rotated embeddings.\n\nRoPE has been shown to improve the performance of the Transformer architecture on a range of tasks, including language modeling and machine translation. It has been particularly effective in handling long-context sequences, as it allows the model to capture position-sensitive information over long distances without requiring exponentially increasing computation and memory.\n\nALiBi\n\nAttention with Linear Biases (ALiBi) is a technique used in the Transformer architecture to encode the position of tokens in a sequence. Unlike traditional positional encodings, which add fixed positional embeddings to the input embeddings, ALiBi uses a linear bias matrix that biases the attention scores based on the distance between tokens in the sequence.\n\nSpecifically, ALiBi applies a linear bias to the attention scores using a set of learnable bias parameters, where each bias parameter corresponds to a different distance between tokens in the sequence. The biased attention scores are then used to weigh the value matrix to produce the final output of the attention mechanism.\n\nThe bias parameters are constructed using the following formula:\n\n    b_d = -d\n\nwhere b_d is the bias parameter for the distance d between tokens in the sequence.\n\nThe biased attention scores are computed using the following formula:\n\n    A_biased = A + b\n\nwhere A is the attention scores and A_biased is the biased attention scores.\n\nALiBi has been shown to improve the performance of the Transformer architecture on a range of tasks, including language modeling and machine translation. It has been particularly effective in handling long-context sequences, as it allows the model to capture position-sensitive information over long distances without requiring exponentially increasing computation and memory.\n\nRelative Position Encodings\n\nRelative Position Encodings (RPE) is a technique used in the Transformer architecture to encode the position of tokens in a sequence. Unlike traditional positional encodings, which add fixed positional embeddings to the input embeddings, RPE uses a set of learnable relative position embeddings that are added to the input embeddings to encode the relative position between tokens in the sequence.\n\nThe relative position embeddings are constructed using the following formula:\n\n    pos_emb_i,j = sin((i-j)/10000^2k/d_model)\n\nwhere pos_emb_i,j is the relative position embedding for the pair of tokens at positions i and j, k is the dimension index, and d_model is the dimensionality of the model.\n\nThe relative position embeddings are then added to the input embeddings to produce the final positional embeddings.\n\nRPE has been shown to improve the performance of the Transformer architecture on a range of tasks, including language modeling and machine translation. It has been particularly effective in handling long-context sequences, as it allows the model to capture position-sensitive information over long distances without requiring exponentially increasing computation and memory.\n\nEfficient implementation\n\nThe Transformer architecture has been implemented in many deep learning frameworks, including TensorFlow, PyTorch, and JAX. These implementations provide optimized code for the key operations of the Transformer, such as the attention mechanism and the feed-forward neural network, which can improve the training and inference speed of the model.\n\nOne popular implementation of the Transformer architecture is the Hugging Face Transformers library, which provides a unified API for working with a wide range of pre-trained Transformer models and fine-tuning them on a variety of downstream tasks. The library uses optimized CUDA kernels for the attention mechanism and the feed-forward neural network, which can provide significant speedups on GPU hardware.\n\nAnother popular implementation is the OpenAI GPT-2 and GPT-3 models, which are implemented using the PyTorch deep learning framework. The GPT-3 model is particularly large, with over 175 billion parameters, and requires significant computational resources for training and inference. To address this, OpenAI has developed specialized hardware, such as the Tesla V100 GPUs, and optimized software libraries, such as the Megatron-LM library, to enable efficient training and inference of the model.\n\nKV caching\n\nKey-value (KV) caching is a technique used in the Transformer architecture to optimize the inference speed of the model when generating text sequences. The technique involves storing the keys and values from the attention mechanism in memory, so that they can be reused in subsequent time steps of the sequence generation process.\n\nWhen generating a text sequence using the Transformer model, the attention mechanism is used to weigh the input tokens based on their relevance to the current time step. This involves computing a set of keys and values for each input token, which are then used to weigh the input tokens in the attention mechanism.\n\nInstead of recomputing the keys and values for each time step, KV caching stores them in memory, so that they can be reused in subsequent time steps. This can significantly reduce the computational cost of generating long text sequences, as the keys and values only need to be computed once for each input token.\n\nKV caching can be implemented using various data structures, such as hash tables or arrays, and requires careful management of memory usage to avoid running out of memory during inference.\n\nFlashAttention\n\nFlashAttention is a library that provides an optimized implementation of the attention mechanism in the Transformer architecture. The library uses a novel algorithm called FlashAttention, which is based on the efficient computation of attention scores using a set of look-up tables that are precomputed during training.\n\nThe FlashAttention algorithm uses a combination of matrix multiplication, indexing, and permutation operations to compute the attention scores and weights in a highly parallelized and memory-efficient manner. This allows the attention mechanism to be computed much faster than traditional implementations, which can significantly speed up the training and inference of Transformer models.\n\nThe FlashAttention library has been integrated into a number of popular deep learning frameworks, including PyTorch and JAX, and is available as an optional backend for the Hugging Face Transformers library. The library supports a wide range of Transformer architectures, including BERT, GPT-2, and T5, and can be easily used with pre-trained models from the Hugging Face model hub.\n\nMulti-Query Attention\n\nMulti-Query Attention (MQA) is a technique used in the Transformer architecture to reduce the number of learnable parameters in the attention mechanism. The technique involves using a single set of query, key, and value projections, instead of separate sets of projections for each attention head.\n\nIn the original Transformer architecture, the attention mechanism uses multiple attention heads, each of which has its own set of learnable parameters for the query, key, and value projections. This can result in a large number of parameters, which can make the model difficult to train and prone to overfitting.\n\nMQA addresses this issue by using a single set of projections for the query, key, and value, which are then split into multiple heads using a linear projection. This reduces the number of learnable parameters in the model, while still allowing for multiple attention heads to be used.\n\nThe MQA attention mechanism can be expressed mathematically as follows:\n\n    MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O\n\nwhere head_i = Attention(QW_i^Q, KW_i^K, VW_i^V) is the attention output for the i-th head, W^O is a final projection matrix, h is the number of attention heads, and W_i^Q, W_i^K, and W_i^V are the projection matrices for the i-th head.\n\nThe MQA attention mechanism has been shown to achieve comparable performance to the original Transformer architecture, while reducing the number of learnable parameters in the model. This can make the model more efficient to train and deploy, particularly for resource-constrained applications.\n\nSpeculative decoding\n\nSpeculative decoding is a technique used in the Transformer architecture to accelerate the generation of text sequences. The technique involves generating multiple candidate tokens for each time step in the sequence, and then selecting the most likely token based on a score function.\n\nDuring inference, the Transformer model generates each token in the sequence conditioned on the previous tokens in the sequence. This can be slow, as each token must be generated one at a time. Speculative decoding accelerates this process by generating multiple candidate tokens for each time step in parallel, using a set of candidate tokens that are likely to be relevant to the current time step.\n\nThe candidate tokens are generated by applying the attention mechanism and the feed-forward neural network to the current input token and the keys and values from the previous time steps. The resulting logits are then passed through a softmax function to obtain a probability distribution over the vocabulary, and multiple candidate tokens are sampled from this distribution.\n\nAfter all candidate tokens have been generated, the most likely token is selected based on a score function that takes into account the likelihood of the token and the diversity of the generated sequence. The selected token is then used as the input to the next time step, and the process is repeated until a stopping criterion is met.\n\nSpeculative decoding can significantly accelerate the generation of text sequences, as it allows for multiple tokens to be generated in parallel for each time step. This can be particularly useful for applications such as machine translation and text summarization, where generating long sequences of tokens can be time-consuming.\n\nSub-quadratic transformers\n\nSub-quadratic transformers are a family of models that aim to process long sequences of tokens with a computational complexity that grows sub-quadratically with the sequence length. This is achieved by using alternative attention mechanisms that avoid computing pairwise attention scores between all tokens in the sequence.\n\nThe most common sub-quadratic transformer architectures are the Swin Transformer, which uses a shifted window attention mechanism, and the SepTr, which separates global and local attention mechanisms. These models have been shown to achieve competitive performance with the original Transformer architecture on tasks such as image classification and language modeling, while requiring fewer computational resources for long sequences.\n\nLong Range Arena (LRA) is a benchmarking platform for evaluating the performance of sub-quadratic transformer architectures on tasks that require processing long sequences of tokens, such as language modeling and speech recognition. The benchmarking platform provides a standardized set of tasks and metrics for comparing the efficiency and accuracy of different sub-quadratic transformer architectures.\n\nAlternative attention graphs\n\nReformer is a Transformer architecture that uses an approximation of the dot-product attention mechanism to reduce the computational complexity of the attention mechanism from quadratic to linear in the sequence length. The Reformer achieves this by using a locality-sensitive hashing (LSH) algorithm to group tokens into buckets and computing the attention scores only within each bucket.\n\nS4 and S5 are Transformer architectures that replace the self-attention mechanism with a linear-time attention mechanism based on a convolutional neural network. The S4 architecture uses a convolutional kernel with a fixed receptive field to approximate the attention mechanism, while the S5 architecture uses a variable-sized receptive field that adapts to the input sequence.\n\nRandom Feature Attention (RFA) is a Transformer architecture that approximates the attention mechanism using random feature maps. The RFA architecture computes the attention scores by projecting the query, key, and value vectors into a high-dimensional feature space using random projections, and then computing the inner products of the projected vectors.\n\nThese alternative attention mechanisms have been shown to achieve competitive performance with the original Transformer architecture on tasks such as language modeling and machine translation, while requiring significantly fewer computational resources for long sequences.\n\nRandom Feature Attention\n\nRandom Feature Attention (RFA) is a technique used in the Transformer architecture to approximate the attention mechanism using random feature maps. The technique involves projecting the query, key, and value vectors into a high-dimensional feature space using random projections, and then computing the inner products of the projected vectors to obtain the attention scores.\n\nThe RFA architecture can be expressed mathematically as follows:\n\n    A = (QW_q)^T (KW_k) V\n\nwhere Q, K, and V are the query, key, and value matrices, respectively, W_q and W_k are learnable projection matrices, and A is the attention matrix.\n\nThe RFA architecture has several advantages over the original Transformer architecture. First, it reduces the computational complexity of the attention mechanism from O(n^2) to O(n), where n is the sequence length, which allows it to handle longer sequences of tokens. Second, it reduces the memory requirements of the attention mechanism, which makes it possible to train and deploy the model on devices with limited memory. Third, it can be trained end-to-end using the same loss function as the original Transformer architecture, which makes it easy to integrate into existing deep learning workflows.\n\nMultimodality\n\nTransformers are not limited to processing textual data, they can also be used to process other types of data such as images, audio, and video. In fact, the Transformer architecture has been extended to handle multiple modalities of data, allowing for more complex and diverse applications.\n\nOne approach to multimodal learning with Transformers is to use a single model that processes multiple modalities of data simultaneously. This can be achieved by concatenating the embeddings from each modality and passing them through a shared set of layers. For example, the Vision Transformer (ViT) model uses a Transformer architecture to process image data, while the Audio Transformer model uses a Transformer architecture to process audio data.\n\nAnother approach to multimodal learning with Transformers is to use separate models for each modality of data, and then combine their outputs in a later stage of the pipeline. For example, the CLIP (Contrastive Language-Image Pretraining) model uses two separate Transformer models, one for processing image data and one for processing textual data, and then combines their outputs in a contrastive learning objective to learn a joint representation of both modalities.\n\nMultimodal learning with Transformers has a wide range of potential applications, such as image captioning, image retrieval, and video captioning. It can also be used to improve the performance of models on tasks that involve multiple modalities of data, such as speech recognition and natural language understanding.\n\nApplications\n\nThe Transformer architecture has been applied to a wide range of natural language processing tasks, including language modeling, machine translation, text classification, sentiment analysis, question answering, and named entity recognition. It has also been applied to other domains such as computer vision and speech recognition.\n\nOne of the key advantages of the Transformer architecture is its ability to capture long-range dependencies in the input sequence. This makes it particularly effective for tasks such as machine translation and text summarization, where the relationships between tokens in the input sequence can span long distances.\n\nAnother advantage of the Transformer architecture is its ability to handle variable-length input sequences. Unlike recurrent neural networks (RNNs), which process sequences sequentially, the Transformer architecture processes all tokens in the input sequence simultaneously. This makes it more efficient than RNNs, particularly for long sequences.\n\nThe Transformer architecture has also been extended to handle multiple modalities of data, such as images and audio. This has enabled new applications such as image captioning and speech recognition.\n\nOverall, the Transformer architecture has had a significant impact on the field of natural language processing, and has led to significant improvements in the performance of state-of-the-art models on a wide range of tasks.\n\nSee also\n\n* List of artificial general intelligence topics\n* List of natural language processing topics\n* List of machine learning topics\n\nNotes\n\n* \u2191 Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N.; Kaiser, Lukasz; Polosukhin, Illia (2017). \"Attention Is All You Need\". arXiv preprint arXiv:1706.03762.\n* \u2191 \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". GitHub. Retrieved 2021-07-05.\n* \u2191 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2019). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" (PDF). Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Proceedings): 4171\u20134186. doi:10.18653/v1/N19-1442. ISBN\u00a09781945628089.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Greg; Henighan, Tom; Child, Rewon; White, Aditya; Child, Rewon; White, Aditya; Chess, Nicholas; Clark, Jack; Berner, Chris; Berner, Chris; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (2020-11-04). \"Language Models are Few-Shot Learners\". arXiv preprint arXiv:2005.14165.\n* \u2191 Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelak"}]