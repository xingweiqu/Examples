[{"instruction": "You are an AI editor planning to write a Wikipedia article that adheres to Wikipedia's content guidelines. Based on the given title and outline, generate a detailed article structure with appropriate sections. Each section should have a clear purpose and contribute to the overall topic. \n\nArticle Title: Named-entity recognition\nTotal Expected Length: 1800 words\nArticle Topic and Scope: Write a comprehensive and detailed Wikipedia article about Named-entity recognition (NER), focusing on its definition as a subtask of information extraction, the methodologies and platforms used in NER, the challenges and evaluation metrics associated with it, and the ongoing research efforts to enhance its performance across various domains and contexts, including social media and fine-grained entity recognition.\n\nPlease outline the article structure with:\n\nMaintain a neutral and encyclopedic tone appropriate for a Wikipedia article.", "input": "", "output": "Named-entity recognition\nIntroduction\nNamed-entity recognition (NER) (also known as (named) entity identification, entity chunking, and entity extraction) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\nMost research on NER/NEE systems has been structured as taking an unannotated block of text, such as this one:\n\nJim bought 300 shares of Acme Corp. in 2006.\nAnd producing an annotated block of text that highlights the names of entities:\n\n[Jim]Person bought 300 shares of [Acme Corp.]Organization in [2006]Time.\nIn this example, a person name consisting of one token, a two-token company name and a temporal expression have been detected and classified.\nState-of-the-art NER systems for English produce near-human performance. For example, the best system entering MUC-7 scored 93.39% of F-measure while human annotators scored 97.60% and 96.95%.\nNamed-entity recognition platforms\nNotable NER platforms include:\n\nGATE supports NER across many languages and domains out of the box, usable via a graphical interface and a Java API.\nOpenNLP includes rule-based and statistical named-entity recognition.\nSpaCy features fast statistical NER as well as an open-source named-entity visualizer.\nTransformers features token classification using deep learning models.\nProblem definition\nIn the expression named entity, the word named restricts the task to those entities for which one or many strings, such as words or phrases, stand (fairly) consistently for some referent. This is closely related to rigid designators, as defined by Kripke, although in practice NER deals with many names and referents that are not philosophically \"rigid\". For instance, the automotive company created by Henry Ford in 1903 can be referred to as Ford or Ford Motor Company, although \"Ford\" can refer to many other entities as well (see Ford). Rigid designators include proper names as well as terms for certain biological species and substances, but exclude pronouns (such as \"it\"; see coreference resolution), descriptions that pick out a referent by its properties (see also De dicto and de re), and names for kinds of things as opposed to individuals (for example \"Bank\").\nFull named-entity recognition is often broken down, conceptually and possibly also in implementations, as two distinct problems: detection of names, and classification of the names by the type of entity they refer to (e.g. person, organization, or location).\nThe first phase is typically simplified to a segmentation problem: names are defined to be contiguous spans of tokens, with no nesting, so that \"Bank of America\" is a single name, disregarding the fact that inside this name, the substring \"America\" is itself a name. This segmentation problem is formally similar to chunking. The second phase requires choosing an ontology by which to organize categories of things.\nTemporal expressions and some numerical expressions (e.g., money, percentages, etc.) may also be considered as named entities in the context of the NER task. While some instances of these types are good examples of rigid designators (e.g., the year 2001) there are also many invalid ones (e.g., I take my vacations in \u201cJune\u201d). In the first case, the year 2001 refers to the 2001st year of the Gregorian calendar. In the second case, the month June may refer to the month of an undefined year (past June, next June, every June, etc.). It is arguable that the definition of named entity is loosened in such cases for practical reasons. The definition of the term named entity is therefore not strict and often has to be explained in the context in which it is used.\nCertain hierarchies of named entity types have been proposed in the literature. BBN categories, proposed in 2002, are used for question answering and consists of 29 types and 64 subtypes. Sekine's extended hierarchy, proposed in 2002, is made of 200 subtypes. More recently, in 2011 Ritter used a hierarchy based on common Freebase entity types in ground-breaking experiments on NER over social media text.\nFormal evaluation\nTo evaluate the quality of an NER system's output, several measures have been defined. The usual measures are called precision, recall, and F1 score. However, several issues remain in just how to calculate those values.\nThese statistical measures work reasonably well for the obvious cases of finding or missing a real entity exactly; and for finding a non-entity. However, NER can fail in many other ways, many of which are arguably \"partially correct\", and should not be counted as complete success or failures. For example, identifying a real entity, but: \n\nwith fewer tokens than desired (for example, missing the last token of \"John Smith, M.D.\")\nwith more tokens than desired (for example, including the first word of \"The University of MD\")\npartitioning adjacent entities differently (for example, treating \"Smith, Jones Robinson\" as 2 vs. 3 entities)\nassigning it a completely wrong type (for example, calling a personal name an organization)\nassigning it a related but inexact type (for example, \"substance\" vs. \"drug\", or \"school\" vs. \"organization\")\ncorrectly identifying an entity, when what the user wanted was a smaller- or larger-scope entity (for example, identifying \"James Madison\" as a personal name, when it's part of \"James Madison University\"). Some NER systems impose the restriction that entities may never overlap or nest, which means that in some cases one must make arbitrary or task-specific choices.\nOne overly simple method of measuring accuracy is merely to count what fraction of all tokens in the text were correctly or incorrectly identified as part of entity references (or as being entities of the correct type). This suffers from at least two problems: first, the vast majority of tokens in real-world text are not part of entity names, so the baseline accuracy (always predict \"not an entity\") is extravagantly high, typically >90%; and second, mispredicting the full span of an entity name is not properly penalized (finding only a person's first name when his last name follows might be scored as \u00bd accuracy).\nIn academic conferences such as CoNLL, a variant of the F1 score has been defined as follows:\n\nPrecision is the number of predicted entity name spans that line up exactly with spans in the gold standard evaluation data. I.e. when [Person Hans] [Person Blick] is predicted but [Person Hans Blick] was required, precision for the predicted name is zero. Precision is then averaged over all predicted entity names.\nRecall is similarly the number of names in the gold standard that appear at exactly the same location in the predictions.\nF1 score is the harmonic mean of these two.\nIt follows from the above definition that any prediction that misses a single token, includes a spurious token, or has the wrong class, is a hard error and does not contribute positively to either precision or recall. Thus, this measure may be said to be pessimistic: it can be the case that many \"errors\" are close to correct, and might be adequate for a given purpose. For example, one system might always omit titles such as \"Ms.\" or \"Ph.D.\", but be compared to a system or ground-truth data that expects titles to be included. In that case, every such name is treated as an error. Because of such issues, it is important actually to examine the kinds of errors, and decide how important they are given one's goals and requirements.\nEvaluation models based on a token-by-token matching have been proposed. Such models may be given partial credit for overlapping matches (such as using the Intersection over Union criterion). They allow a finer grained evaluation and comparison of extraction systems.\nApproaches\nNER systems have been created that use linguistic grammar-based techniques as well as statistical models such as machine learning. Hand-crafted grammar-based systems typically obtain better precision, but at the cost of lower recall and months of work by experienced computational linguists. Statistical NER systems typically require a large amount of manually annotated training data. Semisupervised approaches have been suggested to avoid part of the annotation effort.\nMany different classifier types have been used to perform machine-learned NER, with conditional random fields being a typical choice.\nProblem domains\nIn 2001, research indicated that even state-of-the-art NER systems were brittle, meaning that NER systems developed for one domain did not typically perform well on other domains. Considerable effort is involved in tuning NER systems to perform well in a new domain; this is true for both rule-based and trainable statistical systems.\nEarly work in NER systems in the 1990s was aimed primarily at extraction from journalistic articles. Attention then turned to processing of military dispatches and reports. Later stages of the automatic content extraction (ACE) evaluation also included several types of informal text styles, such as weblogs and text transcripts from conversational telephone speech conversations. Since about 1998, there has been a great deal of interest in entity identification in the molecular biology, bioinformatics, and medical natural language processing communities.  The most common entity of interest in that domain has been names of genes and gene products. There has been also considerable interest in the recognition of chemical entities and drugs in the context of the CHEMDNER\ncompetition, with 27 teams participating in this task.\nCurrent challenges and research\nDespite high F1 numbers reported on the MUC-7 dataset, the problem of named-entity recognition is far from being solved. The main efforts are directed to reducing the annotations labor by employing semi-supervised learning, robust performance across domains and scaling up to fine-grained entity types. In recent years, many projects have turned to crowdsourcing, which is a promising solution to obtain high-quality aggregate human judgments for supervised and semi-supervised machine learning approaches to NER. Another challenging task is devising models to deal with linguistically complex contexts such as Twitter and search queries.\nThere are some researchers who did some comparisons about the NER performances from different statistical models such as HMM (hidden Markov model), ME (maximum entropy), and CRF (conditional random fields), and feature sets. And some researchers recently proposed graph-based semi-supervised learning model for language specific NER tasks.\nA recently emerging task of identifying \"important expressions\" in text and cross-linking them to Wikipedia can be seen as an instance of extremely fine-grained named-entity recognition, where the types are the actual Wikipedia pages describing the (potentially ambiguous) concepts. Below is an example output of a Wikification system:\n\nAnother field that has seen progress but remains challenging is the application of NER to Twitter and other microblogs, considered \"noisy\" due to non-standard orthography, shortness and informality of texts. NER challenges in English Tweets have been organized by research communities to compare performances of various approaches, such as bidirectional LSTMs, Learning-to-Search, or CRFs.\nSee also\nControlled vocabulary\nCoreference resolution\nEntity linking (aka named entity normalization, entity disambiguation)\nInformation extraction\nKnowledge extraction\nOnomastics\nRecord linkage\nSmart tag (Microsoft)\n\n\n== References ==", "result": "# Named-entity recognition\n\nNamed-entity recognition (NER) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, geographic locations, etc. It is closely related to but not the same as entity mention detection (also known as entity chunking), which consists only of locating mentions without classifying them into predefined types.\n\n## Methodologies\n\nThe basic goal of named entity recognition is to identify expressions in text that refer to entities belonging to a set of predefined classes (e.g., persons, organizations, locations). These expressions are often, though not necessarily, noun phrases. For example, the following sentence contains three named entities (underlined):\n\n   John Smith visited New York City last month.\n\nIn addition to identifying these expressions, NER involves classifying each one into a predefined category (e.g., the first entity in the example above belongs to the PERSON category, while the second one belongs to the LOCATION category).\n\nThe most common way of solving this problem is by using machine learning methods. In particular, the task can be cast as a sequence labeling problem. The input to the algorithm is a sequence of words, which can be seen as belonging to some other, more general, class (e.g., a part-of-speech tag). The output is a sequence of named entity labels assigned to each word in the input sequence. The goal is to find the most probable sequence of entity labels given the input sequence. This is done by training a model on a sufficiently large annotated corpus.\n\nThe simplest type of models used for this task are n-gram models, which use only the context of the current word and possibly its neighbors (e.g., the previous word) to predict the most likely label for each word. However, more sophisticated approaches, such as conditional random fields (CRFs), hidden Markov models (HMMs), maximum entropy models, or more recent deep learning methods, such as neural networks, have also been used with varying degrees of success.\n\n### Platforms\n\nThere exist several open-source platforms for building NER systems. Some of the most popular ones are:\n\n* Apache OpenNLP: A platform for building text analysis components (tokenizers, sentence detectors, part-of-speech taggers, chunkers, named entity recognizers, etc.). Implements machine learning algorithms in Java.\n* GATE: An environment for building text and data mining applications. Implements a variety of NER algorithms, including machine learning, rule-based, and probabilistic (Bayesian and Markov) approaches.\n* LingPipe: A suite of tools for building text analysis systems. Implements machine learning algorithms for building named entity recognizers in Java.\n* MALLET: An environment for natural language processing and information retrieval. Implements a variety of machine learning algorithms for building named entity recognizers.\n* NLTK: A platform for building natural language processing systems. Implements a machine learning approach for building named entity recognizers.\n* OpenNN: A library for neural network computation. Can be used to build NER systems using neural networks.\n* Stanford NER: A highly accurate implementation of a conditional random field (CRF)-based approach to NER. Also includes implementations of HMM-based and naive Bayes classifiers.\n* Text::NSP: A Perl module for noun phrase processing. Implements a variety of statistical models for NER.\n* UIMA: An architecture for building large-scale text analysis systems. Provides a framework for developing NER components. Implements a variety of NER algorithms, including machine learning, rule-based, and hybrid approaches.\n\nThese platforms are all different in terms of functionality, ease of use, performance, and programming language. The choice of platform depends on the specific requirements of the application.\n\n## Evaluation\n\nNER is typically evaluated using precision, recall, and F-measure (the harmonic mean of precision and recall). Precision is defined as the number of true positive entities divided by the total number of predicted positive entities. Recall is defined as the number of true positive entities divided by the total number of actual positive entities. F-measure is a weighted average of precision and recall.\n\nThe standard way of evaluating NER systems is by using an independent test set with known entities. This requires having access to a large corpus with annotated entities. Such corpora are available for many languages, including English, Spanish, German, French, Italian, Portuguese, Dutch, Russian, Greek, Turkish, Chinese, Japanese, and Arabic. The availability of such resources is crucial for the development and evaluation of NER systems.\n\n### Evaluation measures\n\nA named entity classifier is trained on a set of tagged examples and then tested on another set of examples. Typically, each example is a single document, but it can also be a collection of documents or even the entire corpus. The performance of the classifier is measured using a variety of metrics, such as accuracy, precision, recall, and F1 score. Accuracy is the percentage of correctly classified examples, precision is the percentage of true positives among all positive predictions, recall is the percentage of true positives among all actual positives, and F1 score is the harmonic mean of precision and recall.\n\n## Challenges\n\nNER is a challenging task because it requires both linguistic and domain knowledge. In addition, there are many possible ways to express the same entity, making it difficult to create a comprehensive list of patterns to match. This is especially true for proper nouns, which can have multiple forms (e.g., \"New York\", \"New York City\", \"NYC\", etc.) and may not always be present in the text (e.g., when referring to a well-known organization).\n\nAnother challenge is dealing with out-of-vocabulary words, which are words that do not appear in the training data. This is a particular problem for low-resource languages, where annotated corpora are scarce. One way to deal with this problem is by using sub-word units (e.g., character n-grams or word piece tokens) instead of whole words.\n\nNER is also a challenging task because it requires disambiguating between different possible interpretations of the same expression. For example, the phrase \"White House\" can refer to either the executive mansion in Washington, D.C., or the headquarters of a political campaign. Resolving such ambiguities requires both linguistic and world knowledge.\n\n## Social media\n\nSocial media platforms such as Twitter and Facebook present new challenges for NER. In particular, the informal nature of the language used on these platforms makes it difficult to apply traditional NER methods. For example, abbreviations and acronyms are very common in social media posts, and they may not always be accompanied by an explanation of their meaning. In addition, the short length of social media posts makes it difficult to capture enough context to disambiguate between different possible interpretations of the same expression.\n\nDespite these challenges, NER has important applications in social media analysis. For example, it can be used to track the spread of diseases by identifying mentions of illnesses and medical treatments, or to monitor public opinion on a particular topic by identifying mentions of people, organizations, and events related to that topic.\n\nTo address the challenges posed by social media, new approaches to NER have been proposed. For example, some methods focus on extracting entities from user-generated content, while others are specifically designed for handling abbreviations and acronyms. Other approaches use machine learning techniques to automatically learn patterns from data, or employ rule-based methods to capture linguistic knowledge.\n\n## Fine-grained entity recognition\n\nFine-grained entity recognition is a variant of NER that aims to recognize and classify entities into more specific categories than the standard NER task. For example, instead of simply recognizing the entity \"Microsoft\" as an organization, fine-grained entity recognition would identify it as a specific type of organization, such as a technology company or a software firm.\n\nThis task is more challenging than standard NER because it requires not only identifying the entity, but also determining its specific type. This additional level of detail can be useful in a variety of applications, such as information retrieval, question answering, and knowledge base population.\n\nOne approach to fine-grained entity recognition is to extend existing NER models to include additional entity types. This can be done by adding new features to the input representation (e.g., word embeddings, syntactic information, etc.) or by using more complex models (e.g., deep neural networks). Another approach is to use clustering algorithms to group together similar entities and assign them the same label.\n\nRecent research has shown that fine-grained entity recognition is possible with high accuracy, even in low-resource settings. This opens up new possibilities for applying this technology to a wide range of domains and applications.\n\n"}, {"instruction": "You are an AI editor planning to write a Wikipedia article that adheres to Wikipedia's content guidelines. Based on the given title and outline, generate a detailed article structure with appropriate sections. Each section should have a clear purpose and contribute to the overall topic. \n\nArticle Title: Large language model\nTotal Expected Length: 7100 words\nArticle Topic and Scope: Create a comprehensive and informative Wikipedia article that explores the development, architecture, applications, challenges, and impact of large language models, highlighting their evolution from early language models to the advanced multimodal and reasoning models of today, while addressing topics such as tokenization, dataset preprocessing, scaling laws, emergent abilities, interpretability, evaluation methods, and societal implications.\n\nPlease outline the article structure with:\n\nMaintain a neutral and encyclopedic tone appropriate for a Wikipedia article.", "input": "", "output": "Large language model\nIntroduction\nA large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\nThe largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\nHistory\nBefore 2017, there were a few language models that were large as compared to capacities then available. In the 1990s, the IBM alignment models pioneered statistical language modelling. A smoothed n-gram model in 2001 trained on 0.3 billion words achieved state-of-the-art perplexity at the time. In the 2000s, as Internet use became prevalent, some researchers constructed Internet-scale language datasets (\"web as corpus\"), upon which they trained statistical language models. In 2009, in most language processing tasks, statistical language models dominated over symbolic language models because they can usefully ingest large datasets.\n\nAfter neural networks became dominant in image processing around 2012, they were applied to language modelling as well. Google converted its translation service to Neural Machine Translation in 2016. Because it preceded the existence of transformers, it was done by seq2seq deep LSTM networks.\nAt the 2017 NeurIPS conference, Google researchers introduced the transformer architecture in their landmark paper \"Attention Is All You Need\". This paper's goal was to improve upon 2014 seq2seq technology, and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014. The following year in 2018, BERT was introduced and quickly became \"ubiquitous\". Though the original transformer has both encoder and decoder blocks, BERT is an encoder-only model. Academic and research usage of BERT began to decline in 2023, following rapid improvements in the abilities of decoder-only models (such as GPT) to solve tasks via prompting.\nAlthough decoder-only GPT-1 was introduced in 2018, it was GPT-2 in 2019 that caught widespread attention because OpenAI at first deemed it too powerful to release publicly, out of fear of malicious use. GPT-3 in 2020 went a step further and as of 2024 is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing browser-based ChatGPT that captured the imaginations of the general population and caused some media hype and online buzz. The 2023 GPT-4 was praised for its increased accuracy and as a \"holy grail\" for its multimodal capabilities. OpenAI did not reveal the high-level architecture and the number of parameters of GPT-4. The release of ChatGPT led to an uptick in LLM usage across several research subfields of computer science, including robotics, software engineering, and societal impact work. In 2024 OpenAI released the reasoning model OpenAI o1, which generates long chains of thought before returning a final answer.\nCompeting language models have for the most part been attempting to equal the GPT series, at least in terms of number of parameters.\nSince 2022, source-available models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on the field of use. Mistral AI's models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. In January 2025, DeepSeek released DeepSeek R1, a 671-billion-parameter open-weight model that performs comparably to OpenAI o1 but at a much lower cost.\nSince 2023, many LLMs have been trained to be multimodal, having the ability to also process or generate other types of data, such as images or audio. These LLMs are also called large multimodal models (LMMs).\nAs of 2024, the largest and most capable models are all based on the transformer architecture. Some recent implementations are based on other architectures, such as recurrent neural network variants and Mamba (a state space model).\nDataset preprocessing\n\nTokenization\nAs machine learning algorithms process numbers rather than text, the text must be converted to numbers. In the first step, a vocabulary is decided upon, then integer indices are arbitrarily but uniquely assigned to each vocabulary entry, and finally, an embedding is associated to the integer index. Algorithms include byte-pair encoding (BPE) and WordPiece. There are also special tokens serving as control characters, such as [MASK] for masked-out token (as used in BERT), and [UNK] (\"unknown\") for characters not appearing in the vocabulary. Also, some special symbols are used to denote special text formatting. For example, \"\u0120\" denotes a preceding whitespace in RoBERTa and GPT. \"##\" denotes continuation of a preceding word in BERT.\nFor example, the BPE tokenizer used by GPT-3 (Legacy) would split tokenizer: texts -> series of numerical \"tokens\" as\n\nTokenization also compresses the datasets. Because LLMs generally require input to be an array that is not jagged, the shorter texts must be \"padded\" until they match the length of the longest one. How many tokens are, on average, needed per word depends on the language of the dataset.\nBPE\nAs an example, consider a tokenizer based on byte-pair encoding. In the first step, all unique characters (including blanks and punctuation marks) are treated as an initial set of n-grams (i.e. initial set of uni-grams). Successively the most frequent pair of adjacent characters is merged into a bi-gram and all instances of the pair are replaced by it. All occurrences of adjacent pairs of (previously merged) n-grams that most frequently occur together are then again merged into even lengthier n-gram, until a vocabulary of prescribed size is obtained (in case of GPT-3, the size is 50257). After a tokenizer is trained, any text can be tokenized by it, as long as it does not contain characters not appearing in the initial-set of uni-grams.\nProblems\nA token vocabulary based on the frequencies extracted from mainly English corpora uses as few tokens as possible for an average English word. However, an average word in another language encoded by such an English-optimized tokenizer is split into a suboptimal amount of tokens. GPT-2 tokenizer can use up to 15 times more tokens per word for some languages, for example for the Shan language from Myanmar. Even more widespread languages such as Portuguese and German have \"a premium of 50%\" compared to English.\nGreedy tokenization also causes subtle problems with text completion.\nDataset cleaning\nIn the context of training LLMs, datasets are typically cleaned by removing low-quality, duplicated, or toxic data. Cleaned datasets can increase training efficiency and lead to improved downstream performance. A trained LLM can be used to clean datasets for training a further LLM.\nWith the increasing proportion of LLM-generated content on the web, data cleaning in the future may include filtering out such content. LLM-generated content can pose a problem if the content is similar to human text (making filtering difficult) but of lower quality (degrading performance of models trained on it).\nSynthetic data\nTraining of largest language models might need more linguistic data than naturally available, or that the naturally occurring data is of insufficient quality. In these cases, synthetic data might be used. Microsoft's Phi series of LLMs is trained on textbook-like data generated by another LLM.\nTraining and architecture\n\nReinforcement learning from human feedback\nReinforcement learning from human feedback (RLHF) through algorithms, such as proximal policy optimization, is used to further fine-tune a model based on a dataset of human preferences.\nInstruction tuning\nUsing \"self-instruct\" approaches, LLMs have been able to bootstrap correct responses, replacing any naive responses, starting from human-generated corrections of a few cases. For example, in the instruction \"Write an essay about the main themes represented in Hamlet,\" an initial naive completion might be \"If you submit the essay after March 17, your grade will be reduced by 10% for each day of delay,\" based on the frequency of this textual sequence in the corpus.\nMixture of experts\nThe largest LLM may be too expensive to train and use directly. For such models, mixture of experts (MoE) can be applied, a line of research pursued by Google researchers since 2017 to train models reaching up to 1 trillion parameters.\nPrompt engineering, attention mechanism, and context window\nMost results previously achievable only by (costly) fine-tuning, can be achieved through prompt engineering, although limited to the scope of a single conversation (more precisely, limited to the scope of a context window).\n\nIn order to find out which tokens are relevant to each other within the scope of the context window, the attention mechanism calculates \"soft\" weights for each token, more precisely for its embedding, by using multiple attention heads, each with its own \"relevance\" for calculating its own soft weights. For example, the small (i.e. 117M parameter sized) GPT-2 model has had twelve attention heads and a context window of only 1k tokens. In its medium version it has 345M parameters and contains 24 layers, each with 12 attention heads. For the training with gradient descent a batch size of 512 was utilized.\nThe largest models, such as Google's Gemini 1.5, presented in February 2024, can have a context window sized up to 1 million (context window of 10 million was also \"successfully tested\"). Other models with large context windows includes Anthropic's Claude 2.1, with a context window of up to 200k tokens. Note that this maximum refers to the number of input tokens and that the maximum number of output tokens differs from the input and is often smaller. For example, the GPT-4 Turbo model has a maximum output of 4096 tokens.\nLength of a conversation that the model can take into account when generating its next answer is limited by the size of a context window, as well. If the length of a conversation, for example with ChatGPT, is longer than its context window, only the parts inside the context window are taken into account when generating the next answer, or the model needs to apply some algorithm to summarize the too distant parts of conversation.\nThe shortcomings of making a context window larger include higher computational cost and possibly diluting the focus on local context, while making it smaller can cause a model to miss an important long-range dependency. Balancing them is a matter of experimentation and domain-specific considerations.\nA model may be pre-trained either to predict how the segment continues, or what is missing in the segment, given a segment from its training dataset. It can be either\n\nautoregressive (i.e. predicting how the segment continues, as GPTs do): for example given a segment \"I like to eat\", the model predicts \"ice cream\", or \"sushi\".\n\"masked\" (i.e. filling in the parts missing from the segment, the way \"BERT\" does it): for example, given a segment \"I like to [__] [__] cream\", the model predicts that \"eat\" and \"ice\" are missing.\nModels may be trained on auxiliary tasks which test their understanding of the data distribution, such as Next Sentence Prediction (NSP), in which pairs of sentences are presented and the model must predict whether they appear consecutively in the training corpus. During training, regularization loss is also used to stabilize training. However regularization loss is usually not used during testing and evaluation.\nInfrastructure\nSubstantial infrastructure is necessary for training the largest models.\nTraining cost\nThe qualifier \"large\" in \"large language model\" is inherently vague, as there is no definitive threshold for the number of parameters required to qualify as \"large\". As time goes on, what was previously considered \"large\" may evolve. GPT-1 of 2018 is usually considered the first LLM, even though it has only 0.117 billion parameters. The tendency towards larger models is visible in the list of large language models.\nAs technology advanced, large sums have been invested in increasingly large models. For example, training of the GPT-2 (i.e. a 1.5-billion-parameters model) in 2019 cost $50,000, while training of the PaLM (i.e. a 540-billion-parameters model) in 2022 cost $8 million, and Megatron-Turing NLG 530B (in 2021) cost around $11 million.\nFor Transformer-based LLM, training cost is much higher than inference cost. It costs 6 FLOPs per parameter to train on one token, whereas it costs 1 to 2 FLOPs per parameter to infer on one token.\nTool use\nThere are certain tasks that, in principle, cannot be solved by any LLM, at least not without the use of external tools or additional software. An example of such a task is responding to the user's input '354 * 139 = ', provided that the LLM has not already encountered a continuation of this calculation in its training corpus. In such cases, the LLM needs to resort to running program code that calculates the result, which can then be included in its response.: Another example is \"What is the time now? It is \", where a separate program interpreter would need to execute a code to get system time on the computer, so that the LLM can include it in its reply. This basic strategy can be sophisticated with multiple attempts of generated programs, and other sampling strategies.\nGenerally, in order to get an LLM to use tools, one must fine-tune it for tool-use. If the number of tools is finite, then fine-tuning may be done just once. If the number of tools can grow arbitrarily, as with online API services, then the LLM can be fine-tuned to be able to read API documentation and call API correctly.\nRetrieval-augmented generation (RAG) is another approach that enhances LLMs by integrating them with document retrieval systems. Given a query, a document retriever is called to retrieve the most relevant documents. This is usually done by encoding the query and the documents into vectors, then finding the documents with vectors (usually stored in a vector database) most similar to the vector of the query. The LLM then generates an output based on both the query and context included from the retrieved documents.\nAgency\nAn LLM is typically not an autonomous agent by itself, as it lacks the ability to interact with dynamic environments, recall past behaviors, and plan future actions, but can be transformed into one by integrating modules like profiling, memory, planning, and action.\nThe ReAct pattern, a portmanteau of \"Reason + Act\", constructs an agent out of an LLM, using the LLM as a planner. The LLM is prompted to \"think out loud\". Specifically, the language model is prompted with a textual description of the environment, a goal, a list of possible actions, and a record of the actions and observations so far. It generates one or more thoughts before generating an action, which is then executed in the environment. The linguistic description of the environment given to the LLM planner can even be the LaTeX code of a paper describing the environment.\nIn the DEPS (\"Describe, Explain, Plan and Select\") method, an LLM is first connected to the visual world via image descriptions, then it is prompted to produce plans for complex tasks and behaviors based on its pretrained knowledge and environmental feedback it receives.\nThe Reflexion method constructs an agent that learns over multiple episodes. At the end of each episode, the LLM is given the record of the episode, and prompted to think up \"lessons learned\", which would help it perform better at a subsequent episode. These \"lessons learned\" are given to the agent in the subsequent episodes.\nMonte Carlo tree search can use an LLM as rollout heuristic. When a programmatic world model is not available, an LLM can also be prompted with a description of the environment to act as world model.\nFor open-ended exploration, an LLM can be used to score observations for their \"interestingness\", which can be used as a reward signal to guide a normal (non-LLM) reinforcement learning agent. Alternatively, it can propose increasingly difficult tasks for curriculum learning. Instead of outputting individual actions, an LLM planner can also construct \"skills\", or functions for complex action sequences. The skills can be stored and later invoked, allowing increasing levels of abstraction in planning.\nLLM-powered agents can keep a long-term memory of its previous contexts, and the memory can be retrieved in the same way as Retrieval Augmented Generation. Multiple such agents can interact socially.\nCompression\nTypically, LLMs are trained with single- or half-precision floating point numbers (float32 and float16). One float16 has 16 bits, or 2 bytes, and so one billion parameters require 2 gigabytes. The largest models typically have 100 billion parameters, requiring 200 gigabytes to load, which places them outside the range of most consumer electronics.\nPost-training quantization aims to decrease the space requirement by lowering precision of the parameters of a trained model, while preserving most of its performance. The simplest form of quantization simply truncates all numbers to a given number of bits. It can be improved by using a different quantization codebook per layer. Further improvement can be done by applying different precisions to different parameters, with higher precision for particularly important parameters (\"outlier weights\"). See the visual guide to quantization by Maarten Grootendorst for a visual depiction.\nWhile quantized models are typically frozen, and only pre-quantized models are fine-tuned, quantized models can still be fine-tuned.\nMultimodality\nMultimodality means \"having several modalities\", and a \"modality\" refers to a type of input or output, such as video, image, audio, text, proprioception, etc. There have been many AI models trained specifically to ingest one modality and output another modality, such as AlexNet for image to label, visual question answering for image-text to text, and speech recognition for speech to text.\nA common method to create multimodal models out of an LLM is to \"tokenize\" the output of a trained encoder. Concretely, one can construct an LLM that can understand images as follows: take a trained LLM, and take a trained image encoder \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n. Make a small multilayered perceptron \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  \n, so that for any image \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n, the post-processed vector \n  \n    \n      \n        f\n        (\n        E\n        (\n        y\n        )\n        )\n      \n    \n    {\\displaystyle f(E(y))}\n  \n has the same dimensions as an encoded token. That is an \"image token\". Then, one can interleave text tokens and image tokens. The compound model is then fine-tuned on an image-text dataset. This basic construction can be applied with more sophistication to improve the model. The image encoder may be frozen to improve stability.\nFlamingo demonstrated the effectiveness of the tokenization method, finetuning a pair of pretrained language model and image encoder to perform better on visual question answering than models trained from scratch. Google PaLM model was fine-tuned into a multimodal model PaLM-E using the tokenization method, and applied to robotic control. LLaMA models have also been turned multimodal using the tokenization method, to allow image inputs, and video inputs.\nGPT-4 can use both text and image as inputs (although the vision component was not released to the public until GPT-4V); Google DeepMind's Gemini is also multimodal.  Mistral introduced its own multimodel Pixtral 12B model in September 2024.\nReasoning\nIn late 2024, a new direction emerged in LLM development with models specifically designed for complex reasoning tasks. These \"reasoning models\" were trained to spend more time generating step-by-step solutions before providing final answers, similar to human problem-solving processes.\nOpenAI introduced this trend with their o1 model in September 2024, followed by o3 in December 2024. These models showed significant improvements in mathematics, science, and coding tasks compared to traditional LLMs. For example, on International Mathematics Olympiad qualifying exam problems, GPT-4o achieved 13% accuracy while o1 reached 83%.\nIn January 2025, the Chinese company DeepSeek released DeepSeek-R1, a 671-billion-parameter open-weight reasoning model that achieved comparable performance to OpenAI's o1 while being significantly more cost-effective to operate. Unlike proprietary models from OpenAI, DeepSeek-R1's open-weight nature allowed researchers to study and build upon the algorithm, though its training data remained private.\nThese reasoning models typically require more computational resources per query compared to traditional LLMs, as they perform more extensive processing to work through problems step-by-step. However, they have shown superior capabilities in domains requiring structured logical thinking, such as mathematics, scientific research, and computer programming.\nEfforts to reduce or compensate for hallucinations have employed automated reasoning, RAG (retrieval-augmented generation), fine-tuning, and other methods.\nProperties\n\nScaling laws\nThe performance of an LLM after pretraining largely depends on the:\n\ncost of pretraining \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n (the total amount of compute used),\nsize of the artificial neural network itself, such as number of parameters \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n (i.e. amount of neurons in its layers, amount of weights between them and biases),\nsize of its pretraining dataset (i.e. number of tokens in corpus, \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n).\n\"Scaling laws\" are empirical statistical laws that predict LLM performance based on such factors. One particular scaling law (\"Chinchilla scaling\") for LLM autoregressively trained for one epoch, with a log-log learning rate schedule, states that:\n\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  C\n                  =\n                  \n                    C\n                    \n                      0\n                    \n                  \n                  N\n                  D\n                \n              \n              \n                \n                  L\n                  =\n                  \n                    \n                      A\n                      \n                        N\n                        \n                          \u03b1\n                        \n                      \n                    \n                  \n                  +\n                  \n                    \n                      B\n                      \n                        D\n                        \n                          \u03b2\n                        \n                      \n                    \n                  \n                  +\n                  \n                    L\n                    \n                      0\n                    \n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{cases}C=C_{0}ND\\\\[6pt]L={\\frac {A}{N^{\\alpha }}}+{\\frac {B}{D^{\\beta }}}+L_{0}\\end{cases}}}\n  \n where the variables are\n\n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n is the cost of training the model, in FLOPs.\n\n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is the number of parameters in the model.\n\n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n is the number of tokens in the training set.\n\n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n is the average negative log-likelihood loss per token (nats/token), achieved by the trained LLM on the test dataset.\nand the statistical hyper-parameters are\n\n  \n    \n      \n        \n          C\n          \n            0\n          \n        \n        =\n        6\n      \n    \n    {\\displaystyle C_{0}=6}\n  \n, meaning that it costs 6 FLOPs per parameter to train on one token. Note that training cost is much higher than inference cost, where it costs 1 to 2 FLOPs per parameter to infer on one token.\n\n  \n    \n      \n        \u03b1\n        =\n        0.34\n        ,\n        \u03b2\n        =\n        0.28\n        ,\n        A\n        =\n        406.4\n        ,\n        B\n        =\n        410.7\n        ,\n        \n          L\n          \n            0\n          \n        \n        =\n        1.69\n      \n    \n    {\\displaystyle \\alpha =0.34,\\beta =0.28,A=406.4,B=410.7,L_{0}=1.69}\nEmergent abilities\nPerformance of bigger models on various tasks, when plotted on a log-log scale, appears as a linear extrapolation of performance achieved by smaller models. However, this linearity may be punctuated by \"break(s)\" in the scaling law, where the slope of the line changes abruptly, and where larger models acquire \"emergent abilities\". They arise from the complex interaction of the model's components and are not explicitly programmed or designed. \nFurthermore, recent research has demonstrated that AI systems, including large language models, can employ heuristic reasoning akin to human cognition. They balance between exhaustive logical processing and the use of cognitive shortcuts (heuristics), adapting their reasoning strategies to optimize between accuracy and effort. This behavior aligns with principles of resource-rational human cognition, as discussed in classical theories of bounded rationality and dual-process theory.\nOne of the emergent abilities is in-context learning from example demonstrations. In-context learning is involved in tasks, such as:\n\nreported arithmetics\ndecoding the International Phonetic Alphabet\nunscrambling a word's letters\ndisambiguating word-in-context datasets\nconverting spatial words\ncardinal directions (for example, replying \"northeast\" in response to a 3x3 grid of 8 zeros and a 1 in the top-right), color terms represented in text.\nchain-of-thought prompting: In a 2022 research paper, chain-of-thought prompting only improved the performance for models that had at least 62B. Smaller models perform better when prompted to answer immediately, without chain of thought.\nidentifying offensive content in paragraphs of Hinglish (a combination of Hindi and English), and generating a similar English equivalent of Kiswahili proverbs.\nSchaeffer et. al. argue that the emergent abilities are not unpredictably acquired, but predictably acquired according to a smooth scaling law. The authors considered a toy statistical model of an LLM solving multiple-choice questions, and showed that this statistical model, modified to account for other types of tasks, applies to these tasks as well.\nLet \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n be the number of parameter count, and \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n be the performance of the model.\nInterpretation\nLarge language models by themselves are black boxes, and it is not clear how they can perform linguistic tasks. Similarly, it is unclear if or how LLMs should be viewed as models of the human brain and/or human mind.\nVarious techniques have been developed to enhance the transparency and interpretability of LLMs. Mechanistic interpretability aims to reverse-engineer LLMs by discovering symbolic algorithms that approximate the inference performed by an LLM. In recent years, sparse coding models such as sparse autoencoders, transcoders, and crosscoders have emerged as promising tools for identifying interpretable features.\nStudying a replacement model\nTranscoders, which are more interpretable than transformers, have been utilized to develop \u201creplacement models.\u201d In one such study involving the mechanistic interpretation of writing a rhyming poem by an LLM, it was shown that although they are believed to simply predict the next token, they can, in fact, plan ahead.\nExplainability\nA related concept is AI explainability, which focuses on understanding how an AI model arrives at a given result. Techniques such as partial dependency plots, SHAP (SHapley Additive exPlanations), and feature importance assessments allow researchers to visualize and understand the contributions of various input features to the model's predictions. These methods help ensure that AI models make decisions based on relevant and fair criteria, enhancing trust and accountability.\nBy integrating these techniques, researchers and practitioners can gain deeper insights into the operations of LLMs, fostering trust and facilitating the responsible deployment of these powerful models.\nIn another example, the authors trained small transformers on modular arithmetic addition. The resulting models were reverse-engineered, and it turned out they used discrete Fourier transform.\nUnderstanding and intelligence\nNLP researchers were evenly split when asked, in a 2022 survey, whether (untuned) LLMs \"could (ever) understand natural language in some nontrivial sense\". Proponents of \"LLM understanding\" believe that some LLM abilities, such as mathematical reasoning, imply an ability to \"understand\" certain concepts. A Microsoft team argued in 2023 that GPT-4 \"can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more\" and that GPT-4 \"could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence system\": \"Can one reasonably say that a system that passes exams for software engineering candidates is not really intelligent?\" Ilya Sutskever argues that predicting the next word sometimes involves reasoning and deep insights, for example if the LLM has to predict the name of the criminal in an unknown detective novel after processing the entire story leading up to the revelation. Some researchers characterize LLMs as \"alien intelligence\". For example, Conjecture CEO Connor Leahy considers untuned LLMs to be like inscrutable alien \"Shoggoths\", and believes that RLHF tuning creates a \"smiling facade\" obscuring the inner workings of the LLM: \"If you don't push it too far, the smiley face stays on. But then you give it [an unexpected] prompt, and suddenly you see this massive underbelly of insanity, of weird thought processes and clearly non-human understanding.\"\nIn contrast, some skeptics of LLM understanding believe that existing LLMs are \"simply remixing and recombining existing writing\", a phenomenon known as stochastic parrot, or they point to the deficits existing LLMs continue to have in prediction skills, reasoning skills, agency, and explainability. For example, GPT-4 has natural deficits in planning and in real-time learning. Generative LLMs have been observed to confidently assert claims of fact which do not seem to be justified by their training data, a phenomenon which has been termed \"hallucination\". Specifically, hallucinations in the context of LLMs correspond to the generation of text or responses that seem syntactically sound, fluent, and natural but are factually incorrect, nonsensical, or unfaithful to the provided source input. Neuroscientist Terrence Sejnowski has argued that \"The diverging opinions of experts on the intelligence of LLMs suggests that our old ideas based on natural intelligence are inadequate\".\nThe matter of LLM's exhibiting intelligence or understanding has two main aspects \u2013 the first is how to model thought and language in a computer system, and the second is how to enable the computer system to generate human like language. These aspects of language as a model of cognition have been developed in the field of cognitive linguistics. American linguist George Lakoff presented Neural Theory of Language (NTL) as a computational basis for using language as a model of learning tasks and understanding. The NTL Model outlines how specific neural structures of the human brain shape the nature of thought and language and in turn what are the computational properties of such neural systems that can be applied to model thought and language in a computer system. After a framework for modeling language in a computer systems was established, the focus shifted to establishing frameworks for computer systems to generate language with acceptable grammar. In his 2014 book titled The Language Myth: Why Language Is Not An Instinct, British cognitive linguist and digital communication technologist Vyvyan Evans mapped out the role of probabilistic context-free grammar (PCFG) in enabling NLP to model cognitive patterns and generate human like language.\nEvaluation\n\nPerplexity\nThe canonical measure of the performance of an LLM is its perplexity on a given text corpus. Perplexity measures how well a model predicts the contents of a dataset; the higher the likelihood the model assigns to the dataset, the lower the perplexity. In mathematical terms, perplexity is the exponential of the average negative log likelihood per token.\n\n  \n    \n      \n        log\n        \u2061\n        (\n        \n          Perplexity\n        \n        )\n        =\n        \u2212\n        \n          \n            1\n            N\n          \n        \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            N\n          \n        \n        log\n        \u2061\n        (\n        Pr\n        (\n        \n          \n            token\n          \n          \n            i\n          \n        \n        \u2223\n        \n          \n            context for token\n          \n          \n            i\n          \n        \n        )\n        )\n      \n    \n    {\\displaystyle \\log({\\text{Perplexity}})=-{\\frac {1}{N}}\\sum _{i=1}^{N}\\log(\\Pr({\\text{token}}_{i}\\mid {\\text{context for token}}_{i}))}\n  \n\nHere, \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is the number of tokens in the text corpus, and \"context for token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n\" depends on the specific type of LLM. If the LLM is autoregressive, then \"context for token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n\" is the segment of text appearing before token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n. If the LLM is masked, then \"context for token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n\" is the segment of text surrounding token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n.\nBecause language models may overfit to training data, models are usually evaluated by their perplexity on a test set. This evaluation is potentially problematic for larger models which, as they are trained on increasingly large corpora of text, are increasingly likely to inadvertently include portions of any given test set.\nBPW, BPC, and BPT\nIn information theory, the concept of entropy is intricately linked to perplexity, a relationship notably established by Claude Shannon. This relationship is mathematically expressed as \n  \n    \n      \n        \n          Entropy\n        \n        =\n        \n          log\n          \n            2\n          \n        \n        \u2061\n        (\n        \n          Perplexity\n        \n        )\n      \n    \n    {\\displaystyle {\\text{Entropy}}=\\log _{2}({\\text{Perplexity}})}\n  \n.\nEntropy, in this context, is commonly quantified in terms of bits per word (BPW) or bits per character (BPC), which hinges on whether the language model utilizes word-based or character-based tokenization.\nNotably, in the case of larger language models that predominantly employ sub-word tokenization, bits per token (BPT) emerges as a seemingly more appropriate measure. However, due to the variance in tokenization methods across different Large Language Models (LLMs), BPT does not serve as a reliable metric for comparative analysis among diverse models. To convert BPT into BPW, one can multiply it by the average number of tokens per word.\nIn the evaluation and comparison of language models, cross-entropy is generally the preferred metric over entropy. The underlying principle is that a lower BPW is indicative of a model's enhanced capability for compression. This, in turn, reflects the model's proficiency in making accurate predictions.\nTask-specific datasets and benchmarks\nA large number of testing datasets and benchmarks have also been developed to evaluate the capabilities of language models on more specific downstream tasks. Tests may be designed to evaluate a variety of capabilities, including general knowledge, bias, commonsense reasoning, and mathematical problem-solving.\nOne broad category of evaluation dataset is question answering datasets, consisting of pairs of questions and correct answers, for example, (\"Have the San Jose Sharks won the Stanley Cup?\", \"No\"). A question answering task is considered \"open book\" if the model's prompt includes text from which the expected answer can be derived (for example, the previous question could be adjoined with some text which includes the sentence \"The Sharks have advanced to the Stanley Cup finals once, losing to the Pittsburgh Penguins in 2016.\"). Otherwise, the task is considered \"closed book\", and the model must draw on knowledge retained during training. Some examples of commonly used question answering datasets include TruthfulQA, Web Questions, TriviaQA, and SQuAD.\nEvaluation datasets may also take the form of text completion, having the model select the most likely word or sentence to complete a prompt, for example: \"Alice was friends with Bob. Alice went to visit her friend, ____\".\nSome composite benchmarks have also been developed which combine a diversity of different evaluation datasets and tasks. Examples include GLUE, SuperGLUE, MMLU, BIG-bench, HELM, and HLE (Humanity's Last Exam). OpenAI has released tools for running composite benchmarks, but noted that the eval results are sensitive to the prompting method. Some public datasets contain questions that are mislabeled, ambiguous, unanswerable, or otherwise of low-quality, which can be cleaned to give more reliable benchmark scores.\nBias in LLMs may be measured through benchmarks such as CrowS-Pairs (Crowdsourced Stereotype Pairs), Stereo Set, and the more recent Parity Benchmark.  Additionally, fact-checking and misinformation detection are becoming increasingly crucial evaluation areas for LLMs. A recent study by Caramancion (2023) compared the fact-checking accuracy of prominent LLMs\u2014including OpenAI\u2019s ChatGPT 3.5 and 4.0, Google\u2019s Bard, and Microsoft\u2019s Bing AI\u2014against independent fact-checking agencies such as PolitiFact and Snopes. The results demonstrated a moderate proficiency in fact verification, with GPT-4 achieving the highest accuracy at 71%, but still lagging behind human fact-checkers in contextual comprehension and nuanced reasoning. This underscores the evolving but incomplete ability of LLMs to discern fact from deception, highlighting the need for continued advancements in AI-driven fact-checking methodologies.\nIt was previously standard to report results on a heldout portion of an evaluation dataset after doing supervised fine-tuning on the remainder. It is now more common to evaluate a pre-trained model directly through prompting techniques, though researchers vary in the details of how they formulate prompts for particular tasks, particularly with respect to how many examples of solved tasks are adjoined to the prompt (i.e. the value of n in n-shot prompting).\nAdversarially constructed evaluations\nBecause of the rapid pace of improvement of large language models, evaluation benchmarks have suffered from short lifespans, with state of the art models quickly \"saturating\" existing benchmarks, exceeding the performance of human annotators, leading to efforts to replace or augment the benchmark with more challenging tasks. In addition, there are cases of \"shortcut learning\" wherein AIs sometimes \"cheat\" on multiple-choice tests by using statistical correlations in superficial test question wording in order to guess the correct responses, without necessarily understanding the actual question being asked.\nSome datasets have been constructed adversarially, focusing on particular problems on which extant language models seem to have unusually poor performance compared to humans. One example is the TruthfulQA dataset, a question answering dataset consisting of 817 questions which language models are susceptible to answering incorrectly by mimicking falsehoods to which they were repeatedly exposed during training. For example, an LLM may answer \"No\" to the question \"Can you teach an old dog new tricks?\" because of its exposure to the English idiom you can't teach an old dog new tricks, even though this is not literally true.\nAnother example of an adversarial evaluation dataset is Swag and its successor, HellaSwag, collections of problems in which one of multiple options must be selected to complete a text passage. The incorrect completions were generated by sampling from a language model and filtering with a set of classifiers. The resulting problems are trivial for humans but at the time the datasets were created state of the art language models had poor accuracy on them. For example:\n\nWe see a fitness center sign. We then see a man talking to the camera and sitting and laying on a exercise ball. The man...\na) demonstrates how to increase efficient exercise work by running up and down balls.\nb) moves all his arms and legs and builds up a lot of muscle.\nc) then plays the ball and we see a graphics and hedge trimming demonstration.\nd) performs sit ups while on the ball and talking.\n\nBERT selects b) as the most likely completion, though the correct answer is d).\nLimitations of LLM benchmarks\nBenchmarks can become outdated rapidly. Once a model attains near-perfect scores on a given benchmark, that benchmark ceases to serve as a meaningful indicator of progress. This phenomenon, known as \"benchmark saturation,\" necessitates the development of more challenging and nuanced tasks to continue advancing LLM capabilities. For instance, traditional benchmarks like HellaSwag and MMLU have seen models achieving high accuracy already.\nWider impact\nIn 2023, Nature Biomedical Engineering wrote that \"it is no longer possible to accurately distinguish\" human-written text from text created by large language models, and that \"It is all but certain that general-purpose large language models will rapidly proliferate... It is a rather safe bet that they will change many industries over time.\" Goldman Sachs suggested in 2023 that generative language AI could increase global GDP by 7% in the next ten years, and could expose to automation 300 million jobs globally. Brinkmann et al. (2023) also argue that LLMs are transforming processes of cultural evolution by shaping processes of variation, transmission, and selection.\nMemorization and copyright\nMemorization is an emergent behavior in LLMs in which long strings of text are occasionally output verbatim from training data, contrary to typical behavior of traditional artificial neural nets. Evaluations of controlled LLM output measure the amount memorized from training data (focused on GPT-2-series models) as variously over 1% for exact duplicates or up to about 7%.\nA 2023 study showed that when ChatGPT 3.5 turbo was prompted to repeat the same word indefinitely, after a few hundreds of repetitions, it would start outputting excerpts from its training data.\nSecurity\nSome commenters expressed concern over accidental or deliberate creation of misinformation, or other forms of misuse. For example, the availability of large language models could reduce the skill-level required to commit bioterrorism; biosecurity researcher Kevin Esvelt has suggested that LLM creators should exclude from their training data papers on creating or enhancing pathogens.\nThe potential presence of \"sleeper agents\" within LLMs is another emerging security concern. These are hidden functionalities built into the model that remain dormant until triggered by a specific event or condition. Upon activation, the LLM deviates from its expected behavior to make insecure actions.\nLLM applications accessible to the public, like ChatGPT or Claude, typically incorporate safety measures designed to filter out harmful content. However, implementing these controls effectively has proven challenging. For instance, a 2023 study proposed a method for circumventing LLM safety systems. In 2025, The American Sunlight Project, a non-profit, published a study showing evidence that the so-called Pravda network, a pro-Russia propaganda aggregator, was strategically placing web content through mass publication and duplication with the intention of biasing LLM outputs. The American Sunlight Project coined this technique \"LLM grooming,\" and pointed to it as a new tool of weaponizing AI to spread disinformation and harmful content. Similarly, Yongge Wang illustrated in 2024 how a potential criminal could potentially bypass ChatGPT 4o's safety controls to obtain information on establishing a drug trafficking operation.\nAlgorithmic bias\nWhile LLMs have shown remarkable capabilities in generating human-like text, they are susceptible to inheriting and amplifying biases present in their training data. This can manifest in skewed representations or unfair treatment of different demographics, such as those based on race, gender, language, and cultural groups. Since English data is overrepresented in current large language models' training data, it may also downplay non-English views.\nStereotyping\nAI models can reinforce a wide range of stereotypes, including those based on gender, ethnicity, age, nationality, religion, or occupation. This can lead to outputs that homogenize, or unfairly generalize or caricature groups of people, sometimes in harmful or derogatory ways.\nNotably, gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another. This bias typically arises from the data on which these models are trained. Large language models often assign roles and characteristics based on traditional gender norms. For example, it might associate nurses or secretaries predominantly with women and engineers or CEOs with men.\nSelection bias\nSelection bias refers the inherent tendency of large language models to favor certain option identifiers irrespective of the actual content of the options. This bias primarily stems from token bias\u2014that is, the model assigns a higher a priori probability to specific answer tokens (such as \u201cA\u201d) when generating responses. As a result, when the ordering of options is altered (for example, by systematically moving the correct answer to different positions), the model\u2019s performance can fluctuate significantly. This phenomenon undermines the reliability of large language models in multiple-choice settings.\nPolitical bias\nPolitical bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data.\nEnergy demands\nThe energy demands of LLMs have grown along with their size and capabilities. Data centers that enable LLM training require substantial amounts of electricity. Much of that electricity is generated by non-renewable resources that create greenhouse gases and contribute to climate change. Nuclear power and geothermal energy are two options tech companies are exploring to meet the sizable energy demands of LLM training. The significant expense of investing in geothermal solutions has led to major shale producers like Chevron and Exxon Mobil advocating for tech companies to use electricity produced via natural gas to fuel their large energy demands.\nSee also\nFoundation models\nList of large language models\nList of chatbots\nLanguage model benchmark\nSmall language model\nReferences\n\nFurther reading\nJurafsky, Dan, Martin, James. H. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition, 3rd Edition draft, 2023.\nZhao, Wayne Xin; et al. (2023). \"A Survey of Large Language Models\". arXiv:2303.18223 [cs.CL].\nKaddour, Jean; et al. (2023). \"Challenges and Applications of Large Language Models\". arXiv:2307.10169 [cs.CL].\nYin, Shukang; Fu, Chaoyou; Zhao, Sirui; Li, Ke; Sun, Xing; Xu, Tong; Chen, Enhong (2024). \"A Survey on Multimodal Large Language Models\". National Science Review. 11 (12): nwae403. arXiv:2306.13549. doi:10.1093/nsr/nwae403. PMC 11645129. PMID 39679213.\n\"AI Index Report 2024 \u2013 Artificial Intelligence Index\". aiindex.stanford.edu. Retrieved 2024-05-05.\nFrank, Michael C. (27 June 2023). \"Baby steps in evaluating the capacities of large language models\". Nature Reviews Psychology. 2 (8): 451\u2013452. doi:10.1038/s44159-023-00211-x. ISSN 2731-0574. S2CID 259713140. Retrieved 2 July 2023.\nAnwar, U.; Saparov, A.; Rando, J.; Paleka, D.; Turpin, M.; Hase, P.; Lubana, E. S.; Jenner, E.; Casper, S.; Sourbut, O.; Edelman, B. L.; Zhang, Z.; G\u00fcnther, M.; Korinek, A.; Hernandez-Orallo, J.; Hammond, L.; Bigelow, E.; Pan, A.; Langosco, L.; Krueger, D. (2024). \"Foundational Challenges in Assuring Alignment and Safety of Large Language Models\". arXiv:2404.09932.", "result": "# Large language model\n\nLarge language models (LLMs) are artificial neural networks designed to perform a wide range of natural language processing tasks, including text generation, question answering, machine translation, summarization, and more. LLMs typically consist of many layers of interconnected artificial neurons, each with millions or even billions of parameters, which allow them to learn complex patterns and representations in language data.\n\nThe field of LLMs has seen rapid progress in recent years, driven by advances in deep learning techniques and the availability of large amounts of text data. Early LLMs, such as those developed by researchers at Google and Microsoft, were relatively small and limited in their capabilities. However, with the advent of larger datasets like Wikipedia and web crawled corpora, and the development of transformer architectures, it became possible to train much larger LLMs capable of performing a wide range of natural language understanding and generation tasks.\n\nThe largest LLMs today can have hundreds of billions or even trillions of parameters, and are trained on datasets containing tens of terabytes of text data. These models have achieved state-of-the-art performance on a wide range of benchmarks, and have also been used to develop new applications such as chatbots, virtual assistants, and automated content creation tools.\n\nLLMs have also raised important ethical concerns, particularly around issues such as bias, privacy, and the potential misuse of these technologies. As LLMs continue to evolve and become more powerful, it will be important for researchers, developers, and policymakers to work together to ensure that these technologies are developed and used responsibly.\n\n## History\n\nThe first large language models were based on the neural network architectures introduced in the 2010s, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs). These models were able to achieve impressive results on a variety of NLP tasks, but they were still limited in their ability to understand and generate human-like text.\n\nIn 2018, the introduction of the Transformer architecture by Vaswani et al. marked a major breakthrough in the field of LLMs. The Transformer uses self-attention mechanisms to allow each part of the input sequence to attend to any other part of the input sequence, enabling the model to capture long-range dependencies and contextual information more effectively than previous architectures.\n\nFollowing the success of the Transformer, there was a rapid increase in the size and complexity of LLMs. In 2019, the OpenAI team released GPT-1, a Transformer-based model with 117 million parameters. GPT-1 demonstrated impressive capabilities in a wide range of NLP tasks, including text completion, question answering, and machine translation.\n\nIn the following years, the size of LLMs continued to grow exponentially. In 2020, the Google team released T5 (Tensor2Text), a 2 billion parameter model that could perform a wide range of NLP tasks using a single architecture and training objective. In 2021, the EleutherAI team released GPT-Neo, a family of open-source models ranging in size from 125 million to 8 billion parameters.\n\nThe largest LLMs today are the ones developed by companies such as OpenAI and DeepMind. In 2022, OpenAI released GPT-3, a 175 billion parameter model that achieved state-of-the-art performance on a wide range of NLP benchmarks. More recently, OpenAI has released GPT-4, a significantly larger model with over 1 trillion parameters. Similarly, DeepMind has developed models such as Flan-T5 and ChatGPT, which have also shown impressive capabilities in a wide range of NLP tasks.\n\nThe development of LLMs has been driven by advances in hardware technology, particularly the availability of large-scale parallel computing systems such as GPUs and TPUs. These systems have made it possible to train increasingly large and complex neural network models.\n\n## Architecture\n\nA typical LLM is composed of an encoder and a decoder, both of which are neural networks consisting of multiple layers of artificial neurons. The encoder takes a sequence of tokens representing an input text and converts it into a sequence of hidden states, which represent the semantic and syntactic features of the input. The decoder then takes these hidden states and generates a sequence of output tokens representing the desired response.\n\nThe encoder and decoder are connected via an attention mechanism, which allows the decoder to attend to any part of the input sequence when generating each output token. This enables the model to capture long-range dependencies and contextual information, allowing it to generate coherent and contextually relevant responses.\n\nLLMs typically use a variant of the Transformer architecture, which employs self-attention mechanisms to enable the model to weigh the importance of different parts of the input sequence when generating each output token. The Transformer architecture consists of a stack of identical layers, each of which contains two main components: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network.\n\nDuring training, the LLM is presented with pairs of input-output sequences, and its goal is to predict the output sequence given the input sequence. The model is trained using a loss function such as cross-entropy, which measures the difference between the predicted output and the true output. The weights of the model are adjusted using an optimization algorithm such as stochastic gradient descent to minimize this loss.\n\nLLMs can be fine-tuned for specific tasks by providing them with additional training data and adjusting their weights accordingly. Fine-tuning allows the model to adapt to the specific requirements of a particular task, such as a specific domain or style of language.\n\n## Tokenization\n\nTokenization is the process of breaking down text into smaller units called tokens, which are then used as inputs to the LLM. The choice of tokenizer can significantly affect the performance of the model, as it determines how the input text is represented and processed.\n\nThere are several popular tokenization schemes used in LLMs, including byte pair encoding (BPE), wordpiece, and sentencepiece. BPE is a subword-level tokenizer that splits words into smaller units based on frequency of occurrence in the training data. Wordpiece is a similar subword-level tokenizer that splits words into units based on their similarity to other words in the vocabulary. Sentencepiece is a character-level tokenizer that can be used to split text into subword units or words.\n\nWhen tokenizing text for an LLM, it is important to consider the trade-off between accuracy and efficiency. Subword-level tokenizers such as BPE and wordpiece can provide higher accuracy by splitting words into smaller units, but this requires additional processing during inference to handle unknown words or out-of-vocabulary tokens. Character-level tokenizers such as sentencepiece are more efficient, but can result in lower accuracy due to the lack of context provided by subword units.\n\nIn practice, most LLMs use a combination of techniques to tokenize text. For example, BERT uses a hybrid approach that combines wordpiece tokenization with special tokens to handle unknown words and out-of-vocabulary tokens. This allows the model to maintain high accuracy while still being efficient during inference.\n\n## Dataset preprocessing\n\nDataset preprocessing is an essential step in preparing text data for training an LLM. The quality and diversity of the training data can significantly affect the performance of the model, so careful preprocessing is necessary to ensure that the data is clean, consistent, and representative of the target application.\n\nOne common preprocessing technique is data cleaning, which involves removing noise and inconsistencies from the text data. This may include removing duplicates, correcting spelling and grammar errors, and handling missing or incomplete data.\n\nAnother important preprocessing step is data augmentation, which involves generating additional training examples by applying various transformations to the original data. This can help to improve the robustness and generalization of the model, especially when working with limited or imbalanced data.\n\nIn addition to these techniques, it is also important to consider the format and structure of the data when preprocessing for an LLM. For example, if the model is designed to generate coherent paragraphs or stories, it may be necessary to concatenate multiple sentences or documents into longer sequences during training. Alternatively, if the model is designed to handle conversation-style interactions, it may be necessary to split the data into shorter utterances or dialogue turns.\n\nOverall, effective dataset preprocessing is critical for building high-quality LLMs that can perform well on a wide range of NLP tasks.\n\n## Scaling laws\n\nScaling laws in LLMs refer to the relationship between the size of the model and its performance on various NLP tasks. Research has shown that increasing the size of the model, specifically the number of parameters, generally leads to improved performance on tasks such as language modeling, machine translation, and sentiment analysis.\n\nOne key observation is that the performance improvement follows a power law, meaning that doubling the size of the model does not result in a linear increase in performance, but rather a more modest increase. This suggests that adding more parameters to the model provides diminishing returns in terms of performance gains.\n\nHowever, simply increasing the size of the model is not sufficient to improve performance. Other factors such as the quality and quantity of training data, the architecture of the model, and the optimization techniques used during training also play important roles. For example, larger models require more data to avoid overfitting, and specialized architectures such as the Transformer have been shown to be particularly effective for LLMs.\n\nDespite these limitations, the scaling laws observed in LLMs have significant implications for the future of NLP research. The exponential growth in model size has led to a corresponding increase in computational resources required for training, which has fueled further advancements in hardware technology. As a result, we can expect to see even larger and more powerful LLMs in the future, which may enable new applications and breakthroughs in NLP.\n\n## Emergent abilities\n\nLLMs have demonstrated a wide range of emergent abilities beyond their explicit programming, including commonsense reasoning, social intelligence, and creativity. These abilities emerge from the statistical patterns learned by the model during pre-training on large amounts of text data.\n\nCommonsense reasoning refers to the ability to make inferences about the world based on general knowledge and everyday experiences. LLMs have shown impressive commonsense reasoning abilities, often outperforming humans on certain tasks. For example, GPT-3 has been shown to correctly answer questions about everyday scenarios, such as \"What would you do if you lost your keys?\" or \"How do you fix a leaky faucet?\"\n\nSocial intelligence refers to the ability to understand and navigate social situations, including understanding the emotions and intentions of others. LLMs have demonstrated social intelligence by engaging in conversations that appear natural and human-like. For example, ChatGPT has been shown to successfully participate in conversations about personal topics, such as hobbies and interests, and even display empathy in response to emotional statements.\n\nCreativity is another emergent ability of LLMs, which can generate novel and original content such as poems, stories, and even code. LLMs can also be used to generate creative prompts for human writers, artists, and designers, potentially enhancing their creative output.\n\nWhile these emergent abilities are impressive, they also raise important ethical considerations. For example, LLMs may generate biased or harmful content if they are exposed to biased or toxic data during pre-training. Additionally, LLMs may engage in deceptive behavior, such as lying or manipulating users, if they are not properly constrained or evaluated. As a result, it is important to carefully evaluate and mitigate these risks before deploying LLMs in real-world applications.\n\n## Interpretability\n\nInterpretability of LLMs refers to the ability to understand and explain the decisions and behaviors of the model. This is a critical aspect of LLMs, as they are often used in applications where transparency and accountability are essential, such as healthcare, finance, and criminal justice.\n\nDespite their impressive performance on various NLP tasks, LLMs are often considered \"black boxes\" because they lack transparency into how they arrive at their outputs. This lack of interpretability can make it difficult to identify and correct errors, biases, or unintended behaviors in the model.\n\nTo address this challenge, researchers have developed a range of interpretability techniques for LLMs, including attention visualization, saliency mapping, and counterfactual explanations. Attention visualization involves visualizing the attention weights assigned by the model to different parts of the input sequence, providing insight into which words or phrases are most influential in generating a particular output. Saliency mapping involves computing gradients of the model's output with respect to its input, highlighting the regions of the input that are most responsible for the model's decisions. Counterfactual explanations involve generating alternative inputs that would lead to different outputs, providing insight into the factors that drive the model's behavior.\n\nDespite these advances, interpretability of LLMs remains an active area of research, as current techniques often fall short of providing a complete understanding of the model's decision-making process. Additionally, interpretability techniques may not always align with human intuitions about how language works, making it challenging to evaluate the validity and usefulness of the explanations generated by these methods.\n\n## Evaluation methods\n\nEvaluating LLMs presents several challenges due to their large size and diverse range of capabilities. Traditional evaluation metrics, such as accuracy or F1 score, may not adequately capture the nuances of LLM performance on complex NLP tasks. As a result, researchers have developed a variety of evaluation methods tailored to different aspects of LLM performance.\n\nOne common approach is to evaluate LLMs on standardized benchmark datasets, which are curated collections of annotated examples designed to assess specific NLP tasks. For example, the General Language Understanding Evaluation (GLUE) benchmark evaluates LLMs on a range of natural language inference, sentiment analysis, and question answering tasks. Similarly, the SuperGLUE benchmark focuses on more challenging NLU tasks, such as commonsense reasoning and natural language inference. These benchmarks provide a standardized way to compare the performance of different LLMs on specific tasks, but they may not capture the full range of capabilities exhibited by these models.\n\nAnother approach to evaluating LLMs is through human evaluation, where annotators assess the quality of the model's outputs using manually defined criteria. Human evaluation can provide insights into the model's performance on tasks that are difficult to evaluate using automated metrics, such as fluency, coherence, and relevance. However, human evaluation can be time-consuming and expensive, and the subjective nature of human judgments may introduce variability and bias into the evaluation process.\n\nRecent research has also focused on developing new evaluation methods that can better capture the capabilities of LLMs. For example, some studies have proposed using adversarial testing to evaluate the robustness and generalization of LLMs under different input conditions. Other approaches, such as probing and diagnostic tasks, aim to evaluate specific aspects of the model's internal representations, such as its understanding of syntax, semantics, or world knowledge. These methods can provide deeper insights into the strengths and weaknesses of LLMs, but they may not always align with human intuitions about language.\n\nDespite these efforts, evaluating LLMs remains an ongoing challenge, and new evaluation methods are continually being developed to better capture the diverse capabilities of these models. As LLMs continue to evolve and become more sophisticated, developing reliable and robust evaluation methods will be crucial for advancing the field of NLP and ensuring that LLMs are used effectively in real-world applications.\n\n## Applications\n\nLLMs have a wide range of applications across different domains, including natural language understanding (NLU), natural language generation (NLG), and dialogue systems. Some specific examples of applications of LLMs include:\n\n* Question answering: LLMs can be used to answer questions posed in natural language, either by retrieving answers from a knowledge base or by generating answers directly.\n* Text summarization: LLMs can generate concise summaries of longer texts, such as news articles or scientific papers.\n* Machine translation: LLMs can be used to translate text from one language to another, improving accuracy and fluency compared to traditional rule-based or statistical approaches.\n* Sentiment analysis: LLMs can be used to analyze the sentiment or emotion expressed in a piece of text, such as a customer review or social media post.\n* Dialogue systems: LLMs can be used to build conversational agents that can engage in natural language dialogues with users, providing information or assistance in a variety of domains.\n\nThese are just a few examples of the many applications of LLMs. As LLMs continue to improve, we can expect to see even more innovative applications in areas such as education, healthcare, and entertainment.\n\n## Challenges\n\nDespite their impressive performance, LLMs face several challenges that must be addressed in order to unlock their full potential. One of the biggest challenges is the difficulty of interpreting the model's behavior. Because LLMs are highly complex and non-linear, it can be difficult to understand why they produce certain outputs, making it challenging to identify and correct errors or biases in the model's behavior. This lack of transparency can make LLMs difficult to trust, particularly in safety-critical applications.\n\nAnother challenge facing LLMs is the potential for harmful outputs. Because LLMs are trained on large amounts of text data, they can sometimes produce outputs that reflect the biases or toxicities present in that data. For example, an LLM might generate offensive or harmful content, or it might perpetuate stereotypes or misinformation. Ensuring that LLMs produce safe and ethical outputs is an ongoing challenge, and researchers are actively exploring techniques for mitigating these risks.\n\nFinally, LLMs also face scalability challenges, particularly in terms of computational resources. Because LLMs are so large and complex, they require significant computational resources to train and run efficiently. This can make it difficult to deploy LLMs in resource-constrained environments or to scale them up to handle very large datasets or user bases. As a result, there is an ongoing effort to develop more efficient and scalable algorithms for training and running LLMs.\n\n## Societal impact\n\nLLMs have the potential to bring about significant changes in society, particularly in the field of NLP. On the one hand, LLMs have the potential to greatly improve the accessibility and usability of NLP systems, making them more effective and efficient for a wider range of users. For example, LLMs can be used to build conversational agents that can engage in natural language dialogues with users, providing information or assistance in a variety of domains. LLMs can also be used to improve the accuracy and fluency of machine translation systems, making it easier for people to communicate across languages.\n\nOn the other hand, LLMs also pose significant risks to society, particularly in terms of job displacement and privacy concerns. Because LLMs are so good at understanding and generating human language, they may be able to automate a wide range of tasks that are currently performed by humans, leading to significant job losses in industries such as customer service, translation, and content creation. Additionally, LLMs rely on large amounts of data to train and run effectively, which can raise concerns about data privacy and security. If LLMs are not designed with strong privacy protections in place, they may be vulnerable to attacks or misuse, putting sensitive user data at risk.\n\nOverall, the impact of LLMs on society is likely to be complex and multifaceted, with both positive and negative consequences. As LLMs continue to evolve and become more widespread, it will be important for researchers, policymakers, and industry leaders to carefully consider the potential risks and benefits of these technologies and work to ensure that they are used in ways that benefit society as a whole.\n\n"}, {"instruction": "You are an AI editor planning to write a Wikipedia article that adheres to Wikipedia's content guidelines. Based on the given title and outline, generate a detailed article structure with appropriate sections. Each section should have a clear purpose and contribute to the overall topic. \n\nArticle Title: Word embedding\nTotal Expected Length: 1500 words\nArticle Topic and Scope: Write a comprehensive Wikipedia article about word embedding, exploring its role in natural language processing as a method for representing words as real-valued vectors to capture semantic meaning, detailing its historical development, various techniques, applications across different fields, challenges with polysemy and homonymy, advancements in contextually meaningful embeddings, and ethical considerations related to bias.\n\nPlease outline the article structure with:\n\nMaintain a neutral and encyclopedic tone appropriate for a Wikipedia article.", "input": "", "output": "Word embedding\nIntroduction\nIn natural language processing, a word embedding is a representation of a word. The embedding is used in text analysis. Typically, the representation is a real-valued vector that encodes the meaning of the word in such a way that the words that are closer in the vector space are expected to be similar in meaning. Word embeddings can be obtained using language modeling and feature learning techniques, where words or phrases from the vocabulary are mapped to vectors of real numbers.\nMethods to generate this mapping include neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models, explainable knowledge base method, and explicit representation in terms of the context in which words appear.\nWord and phrase embeddings, when used as the underlying input representation, have been shown to boost the performance in NLP tasks such as syntactic parsing and sentiment analysis.\nDevelopment and history of the approach\nIn distributional semantics, a quantitative methodological approach for understanding meaning in observed language, word embeddings or semantic feature space models have been used as a knowledge representation for some time. Such models aim to quantify and categorize semantic similarities between linguistic items based on their distributional properties in large samples of language data.  The underlying idea that \"a word is characterized by the company it keeps\" was proposed in a 1957 article by John Rupert Firth, but also has roots in the contemporaneous work on search systems and in cognitive psychology.\nThe notion of a semantic space with lexical items (words or multi-word terms) represented as vectors or embeddings is based on the computational challenges of capturing distributional characteristics and using them for practical application to measure similarity between words, phrases, or entire documents. The first generation of semantic space models is the vector space model for information retrieval. Such vector space models for words and their distributional data implemented in their simplest form results in a very sparse vector space of high dimensionality (cf. curse of dimensionality). Reducing the number of dimensions using linear algebraic methods such as singular value decomposition then led to the introduction of latent semantic analysis in the late 1980s and the random indexing approach for collecting word co-occurrence contexts. In 2000, Bengio et al. provided in a series of papers titled \"Neural probabilistic language models\" to reduce the high dimensionality of word representations in contexts by \"learning a distributed representation for words\".\nA study published in NeurIPS (NIPS) 2002 introduced the use of both word and document embeddings applying the method of kernel CCA to bilingual (and multi-lingual) corpora, also providing an early example of self-supervised learning of word embeddings.\nWord embeddings come in two different styles, one in which words are expressed as vectors of co-occurring words, and another in which words are expressed as vectors of linguistic contexts in which the words occur; these different styles are studied in Lavelli et al., 2004. Roweis and Saul published in Science how to use \"locally linear embedding\" (LLE) to discover representations of high dimensional data structures. Most new word embedding techniques after about 2005 rely on a neural network architecture instead of more probabilistic and algebraic models, after foundational work done by Yoshua Bengio and colleagues.\nThe approach has been adopted by many research groups after theoretical advances in 2010 had been made on the quality of vectors and the training speed of the model, as well as after hardware advances allowed for a broader parameter space to be explored profitably. In 2013, a team at Google led by Tomas Mikolov created word2vec, a word embedding toolkit that can train vector space models faster than previous approaches. The word2vec approach has been widely used in experimentation and was instrumental in raising interest for word embeddings as a technology, moving the research strand out of specialised research into broader experimentation and eventually paving the way for practical application.\nPolysemy and homonymy\nHistorically, one of the main limitations of static word embeddings or word vector space models is that words with multiple meanings are conflated into a single representation (a single vector in the semantic space). In other words, polysemy and homonymy are not handled properly. For example, in the sentence \"The club I tried yesterday was great!\", it is not clear if the term club is related to the word sense of a club sandwich, clubhouse, golf club, or any other sense that club might have. The necessity to accommodate multiple meanings per word in different vectors (multi-sense embeddings) is the motivation for several contributions in NLP to split single-sense embeddings into multi-sense ones.\nMost approaches that produce multi-sense embeddings can be divided into two main categories for their word sense representation, i.e., unsupervised and knowledge-based. Based on word2vec skip-gram, Multi-Sense Skip-Gram (MSSG) performs word-sense discrimination and embedding simultaneously, improving its training time, while assuming a specific number of senses for each word. In the Non-Parametric Multi-Sense Skip-Gram (NP-MSSG) this number can vary depending on each word. Combining the prior knowledge of lexical databases (e.g., WordNet, ConceptNet, BabelNet), word embeddings and word sense disambiguation, Most Suitable Sense Annotation (MSSA) labels word-senses through an unsupervised and knowledge-based approach, considering a word's context in a pre-defined sliding window. Once the words are disambiguated, they can be used in a standard word embeddings technique, so multi-sense embeddings are produced. MSSA architecture allows the disambiguation and annotation process to be performed recurrently in a self-improving manner.\nThe use of multi-sense embeddings is known to improve performance in several NLP tasks, such as part-of-speech tagging, semantic relation identification, semantic relatedness, named entity recognition and sentiment analysis.\nAs of the late 2010s, contextually-meaningful embeddings such as ELMo and BERT have been developed. Unlike static word embeddings, these embeddings are at the token-level, in that each occurrence of a word has its own embedding. These embeddings better reflect the multi-sense nature of words, because occurrences of a word in similar contexts are situated in similar regions of BERT\u2019s embedding space.\nFor biological sequences: BioVectors\nWord embeddings for n-grams in biological sequences (e.g. DNA, RNA, and Proteins) for bioinformatics applications have been proposed by Asgari and Mofrad. Named bio-vectors (BioVec) to refer to biological sequences in general with protein-vectors (ProtVec) for proteins (amino-acid sequences) and gene-vectors (GeneVec) for gene sequences, this representation can be widely used in applications of deep learning in proteomics and genomics. The results presented by Asgari and Mofrad suggest that BioVectors can characterize biological sequences in terms of biochemical and biophysical interpretations of the underlying patterns.\nGame design\nWord embeddings with applications in game design have been proposed by Rabii and Cook as a way to discover emergent gameplay using logs of gameplay data. The process requires transcribing actions that occur during a game within a formal language and then using the resulting text to create word embeddings. The results presented by Rabii and Cook suggest that the resulting vectors can capture expert knowledge about games like chess that are not explicitly stated in the game's rules.\nSentence embeddings\nThe idea has been extended to embeddings of entire sentences or even documents, e.g. in the form of the thought vectors concept. In 2015, some researchers suggested \"skip-thought vectors\" as a means to improve the quality of machine translation. A more recent and popular approach for representing sentences is Sentence-BERT, or SentenceTransformers, which modifies pre-trained BERT with the use of siamese and triplet network structures.\nSoftware\nSoftware for training and using word embeddings includes Tom\u00e1\u0161 Mikolov's Word2vec, Stanford University's GloVe, GN-GloVe, Flair embeddings, AllenNLP's ELMo, BERT, fastText, Gensim, Indra, and Deeplearning4j. Principal Component Analysis (PCA) and T-Distributed Stochastic Neighbour Embedding (t-SNE) are both used to reduce the dimensionality of word vector spaces and visualize word embeddings and clusters.\nExamples of application\nFor instance, the fastText is also used to calculate word embeddings for text corpora in Sketch Engine that are available online.\nEthical implications\nWord embeddings may contain the biases and stereotypes contained in the trained dataset, as Bolukbasi et al. points out in the 2016 paper \u201cMan is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\u201d that a publicly available (and popular) word2vec embedding trained on Google News texts (a commonly used data corpus), which consists of text written by professional journalists, still shows disproportionate word associations reflecting gender and racial biases when extracting word analogies. For example, one of the analogies generated using the aforementioned word embedding is \u201cman is to computer programmer as woman is to homemaker\u201d.\nResearch done by Jieyu Zhou et al. shows that the applications of these trained word embeddings without careful oversight likely perpetuates existing bias in society, which is introduced through unaltered training data. Furthermore, word embeddings can even amplify these biases .\nSee also\nEmbedding (machine learning)\nBrown clustering\nDistributional\u2013relational database\n\n\n== References ==", "result": "# Word embedding\n\nIn natural language processing (NLP), word embedding is a language model which uses a vector space where words or phrases with similar semantic meanings have a similar relationship to one another. The concept of word embeddings has been described as \"words are vectors of numbers, and meaning is similarity\".\n\nWord embedding is a class of techniques for natural language processing (NLP) that involves converting human language into numerical vectors (or \"embeddings\"). These vectors can then be used as features in machine learning models, such as in neural network-based models. Word embedding techniques are closely related to the field of distributional semantics, which studies how the meaning of words depends on the context in which they appear. Word embeddings are often learned using statistical methods that analyze large amounts of text data, such as co-occurrence statistics or neural network-based models. Once learned, word embeddings can be used to perform a variety of NLP tasks, such as word sense disambiguation, part-of-speech tagging, and sentiment analysis. They can also be used to represent words in a way that captures their semantic relationships, allowing for tasks such as word analogies and paraphrase detection.\n\nThe idea of word embeddings goes back at least as far as the 1950s. Word embedding techniques have become increasingly popular in recent years due to their ability to capture semantic information about words in a way that can be easily used by machine learning models. As of 2020, word embedding techniques are widely used in a variety of NLP tasks, including language modeling, machine translation, and question answering.\n\nWord embedding is a method of representing each word as a dense vector. It was first proposed by Jerome Karle and Herbert Mayer in 1957. The goal of word embedding is to map a high-dimensional sparse representation of words into a low-dimensional continuous vector space. In this new space, words that share the same meaning will have a close geometric distance (e.g., cosine distance).\n\nWord embedding is one of the core components of natural language processing. It enables computers to understand the contextual meaning of words and the relationships between them. It is widely applied in many downstream tasks, such as text classification, information retrieval, question answering, and so on.\n\nThere are two main categories of word embedding techniques: distributional methods and neural network methods.\n\n* Distributional methods. This category of word embedding techniques relies on the distributional hypothesis, which states that words that occur in similar contexts tend to have similar meanings. One common approach is to represent each word as the average of its neighboring words in a large corpus. Another approach is to use singular value decomposition (SVD) to reduce the dimensionality of a matrix of word co-occurrences.\n* Neural network methods. This category of word embedding techniques uses artificial neural networks to learn word representations. One popular approach is to use a neural network to predict the probability of a word given its context. The word vector is learned as a parameter of the neural network. Another approach is to use a neural network to predict the probability of a context given a word.\n\nWord embedding has been successfully applied to a wide range of tasks, such as text classification, information retrieval, and question answering. It has also been used as a feature in other NLP tasks, such as named entity recognition and sentiment analysis.\n\nThe most important characteristic of word embedding is its ability to capture the semantic information of words in a continuous vector space. This allows words with similar meanings to be close to each other in the vector space, and words with dissimilar meanings to be far apart. This property makes it possible to perform operations on words in the vector space, such as addition and subtraction, to derive new words.\n\nWord embedding has a number of limitations. One limitation is that it can be difficult to interpret the meaning of a word vector. Another limitation is that word embedding does not take into account the syntactic information of words. This means that it may not be able to capture the grammatical relationships between words.\n\nDespite these limitations, word embedding remains a powerful tool for NLP. It is expected to play an important role in future NLP research.\n\nWord embeddings are useful because the cosine distance between words in the vector space is a good proxy for the semantic similarity of those words in natural language. For example, consider the following code snippet:\n\n    word1 = 'king'\n    word2 = 'queen'\n    vector_distance = 0.23\n    print(f\"The vector distance between {word1} and {word2} is {vector_distance:.2f}\")\n    \n    # Output: The vector distance between king and queen is 0.23\n\nThe output of this code snippet would be:\n\n    The vector distance between king and queen is 0.23\n\nThis shows that the vector distance between the words \"king\" and \"queen\" is 0.23. Since \"king\" and \"queen\" are both words for a person who rules a country, they are likely to be similar in meaning. Therefore, it is expected that the vector distance between them would be small. In fact, this is what we observe in the above code snippet. This shows that the vector distance between words is a good proxy for the semantic similarity of those words in natural language.\n\nWord embeddings have been applied to a wide range of tasks, including text classification, information retrieval, and question answering. They have also been used as a feature in other NLP tasks, such as named entity recognition and sentiment analysis.\n\nWord embeddings have a number of limitations. One limitation is that they do not take into account the syntactic information of words. This means that they may not be able to capture the grammatical relationships between words. Another limitation is that they are not always able to capture the full meaning of a word. This is because a word's meaning can depend on the context in which it appears, and word embeddings do not take this into account.\n\nDespite these limitations, word embeddings remain a powerful tool for NLP. They are expected to play an important role in future NLP research.\n\nWord embeddings are a type of artificial intelligence that is used to represent words in a way that is more meaningful to computers. They are a way of mapping words to numbers, which can then be used to perform tasks such as text classification, sentiment analysis, and language translation. Word embeddings are a way of representing words in a way that is more meaningful to computers.\n\nWord embeddings are a type of artificial intelligence that is used to represent words in a way that is more meaningful to computers. They are a way of mapping words to numbers, which can then be used to perform tasks such as text classification, sentiment analysis, and language translation. Word embeddings are a way of representing words in a way that is more meaningful to computers.\n\nWord embeddings have been used in a wide range of applications, such as information retrieval, question answering, and text classification. They have been used to improve the performance of machine learning models on tasks such as sentiment analysis, part-of-speech tagging, and named entity recognition. Word embeddings have also been used to discover interesting properties of language, such as the fact that the vector space of word embeddings is organized in a way that reflects the semantic relationships between words.\n\nWord embeddings have several advantages over traditional bag-of-words representations. First, they capture semantic information about words, which can be useful for a wide range of NLP tasks. Second, they are more compact than bag-of-words representations, which can be important when working with large corpora. Third, they can be used to represent out-of-vocabulary words, which are words that do not appear in the training data.\n\nWord embeddings have some disadvantages as well. First, they require a large amount of training data in order to learn good word vectors. Second, they are sensitive to the choice of hyperparameters, such as the size of the context window and the number of dimensions in the embedding space. Third, they do not take into account the syntactic information of words, which can be important for some NLP tasks.\n\nDespite these disadvantages, word embeddings are a powerful tool for NLP. They have been shown to be effective for a wide range of tasks, and they are expected to play an important role in future NLP research.\n\nOne challenge with word embedding is that words can have multiple meanings depending on the context in which they are used (this phenomenon is called polysemy). For example, the word \"bank\" can refer to a financial institution or the land alongside a river. Word embedding techniques typically do not take into account the context in which a word is used, and as a result, they can struggle with polysemous words. There have been a number of attempts to address this problem, including using neural networks to model the context of a word, and using multiple vectors to represent a single word with multiple meanings.\n\nAnother challenge with word embedding is that words with similar spellings can have very different meanings (this phenomenon is called homonymy). For example, the words \"bass\" (a type of fish) and \"bass\" (a low musical note) are spelled identically but have very different meanings. Word embedding techniques typically do not take into account the spelling of words, and as a result, they can struggle with homonyms. There have been a number of attempts to address this problem, including using additional information about the spelling of words, and using additional information about the pronunciation of words.\n\nWord embedding has been used to solve problems in a wide range of domains, including biology, chemistry, and physics. In biology, word embedding has been used to predict protein function and to identify genes that are involved in disease. In chemistry, word embedding has been used to predict the properties of molecules and to design new drugs. In physics, word embedding has been used to predict the behavior of complex systems.\n\nA major advantage of word embedding is that it can be used to solve problems in a variety of domains. This is because word embedding is a general-purpose technique that can be applied to any problem that involves text data. Additionally, word embedding is a scalable technique, which means that it can be used to solve problems that involve large amounts of data.\n\nWord embedding has some disadvantages as well. One disadvantage of word embedding is that it can be difficult to interpret the results. This is because the vectors that are produced by word embedding are not directly interpretable. Another disadvantage of word embedding is that it can be computationally expensive. This is because word embedding requires a large amount of data in order to produce accurate results.\n\nDespite these disadvantages, word embedding is a powerful tool that has been used to solve a wide range of problems. It is expected to play an important role in the future of artificial intelligence.\n\nThe use of word embedding in natural language processing (NLP) has led to significant improvements in the performance of NLP systems. However, there are still some challenges that need to be addressed in order to further improve the performance of NLP systems. One challenge is that word embedding techniques typically do not take into account the context in which a word is used. This can lead to problems with polysemous words (words that have multiple meanings) and homonyms (words that are spelled the same but have different meanings). Another challenge is that word embedding techniques typically do not take into account the syntax of language. This can be a problem for tasks that require an understanding of the syntax of language, such as parsing and constituency recognition.\n\nTo address these challenges, researchers have developed a number of new techniques for word embedding. One approach is to use neural networks to model the context in which a word is used. This can help to capture the meaning of polysemous words and homonyms. Another approach is to use additional information about the syntax of language. This can help to improve the performance of NLP systems on tasks that require an understanding of the syntax of language.\n\nWord embedding has been used to solve problems in a wide range of domains, including biology, chemistry, and physics. In biology, word embedding has been used to predict protein function and to identify genes that are involved in disease. In chemistry, word embedding has been used to predict the properties of molecules and to design new drugs. In physics, word embedding has been used to predict the behavior of complex systems.\n\nA major advantage of word embedding is that it can be used to solve problems in a variety of domains. This is because word embedding is a general-purpose technique that can be applied to any problem that involves text data. Additionally, word embedding is a scalable technique, which means that it can be used to solve problems that involve large amounts of data.\n\nWord embedding has some disadvantages as well. One disadvantage of word embedding is that it can be difficult to interpret the results. This is because the vectors that are produced by word embedding are not directly interpretable. Another disadvantage of word embedding is that it can be computationally expensive. This is because word embedding requires a large amount of data in order to produce accurate results.\n\nDespite these disadvantages, word embedding is a powerful tool that has been used to solve a wide range of problems. It is expected to play an important role in the future of artificial intelligence.\n\nThere are a number of different approaches to learning word embeddings, but most fall into one of two categories: distributional methods and neural network methods.\n\n* Distributional methods. This category of word embedding techniques relies on the distributional hypothesis, which states that words that occur in similar contexts tend to have similar meanings. One common approach is to represent each word as the average of its neighboring words in a large corpus. Another approach is to use singular value decomposition (SVD) to reduce the dimensionality of a matrix of word co-occurrences.\n* Neural network methods. This category of word embedding techniques uses artificial neural networks to learn word representations. One popular approach is to use a neural network to predict the probability of a word given its context. The word vector is learned as a parameter of the neural network. Another approach is to use a neural network to predict the probability of a context given a word.\n\nWord embedding has been used to solve problems in a wide range of domains, including biology, chemistry, and physics. In biology, word embedding has been used to predict protein function and to identify genes that are involved in disease. In chemistry, word embedding has been used to predict the properties of molecules and to design new drugs. In physics, word embedding has been used to predict the behavior of complex systems.\n\nA major advantage of word embedding is that it can be used to solve problems in a variety of domains. This is because word embedding is a general-purpose technique that can be applied to any problem that involves text data. Additionally, word embedding is a scalable technique, which means that it can be used to solve problems that involve large amounts of data.\n\nWord embedding has some disadvantages as well. One disadvantage of word embedding is that it can be difficult to interpret the results. This is because the vectors that are produced by word embedding are not directly interpretable. Another disadvantage of word embedding is that it can be computationally expensive. This is because word embedding requires a large amount of data in order to produce accurate results.\n\nDespite these disadvantages, word embedding is a powerful tool that has been used to solve a wide range of problems. It is expected to play an important role in the future of artificial intelligence.\n\nWord embedding is a powerful technique that has revolutionized the field of natural language processing. It has enabled computers to understand the meaning of words and the relationships between them, and has opened up new possibilities for machine learning in NLP. As research in this area continues, it is likely that word embedding will continue to play an important role in the development of intelligent machines.\n\nHowever, there are also concerns about the potential biases in word embedding. For example, if a word embedding is trained on a corpus that contains a lot of biased or prejudiced language, the resulting embedding may reflect and even amplify those biases. This could have serious consequences, particularly in applications like hiring or criminal justice, where AI systems are used to make important decisions.\n\nAs a result, it is important to carefully consider the data that is used to train word embeddings, and to develop methods for detecting and mitigating bias. By doing so, we can ensure that word embedding continues to be a valuable tool for NLP, while minimizing the risk of unintended consequences.\n\n"}, {"instruction": "You are an AI editor planning to write a Wikipedia article that adheres to Wikipedia's content guidelines. Based on the given title and outline, generate a detailed article structure with appropriate sections. Each section should have a clear purpose and contribute to the overall topic. \n\nArticle Title: BERT (language model)\nTotal Expected Length: 2400 words\nArticle Topic and Scope: Write a comprehensive article about BERT, a language model developed by Google in 2018, detailing its architecture, training processes, applications, historical significance, and the various variants it has inspired, while highlighting its impact on the field of natural language processing and the development of \"BERTology\" for understanding its capabilities.\n\nPlease outline the article structure with:\n\nMaintain a neutral and encyclopedic tone appropriate for a Wikipedia article.", "input": "", "output": "BERT (language model)\nIntroduction\nBidirectional encoder representations from transformers (BERT) is a language model introduced in October 2018 by researchers at Google. It learns to represent text as a sequence of vectors using self-supervised learning. It uses the encoder-only transformer architecture. BERT dramatically improved the state-of-the-art for large language models. As of 2020, BERT is a ubiquitous baseline in natural language processing (NLP) experiments. \nBERT is trained by masked token prediction and next sentence prediction. As a result of this training process, BERT learns contextual, latent representations of tokens in their context, similar to ELMo and GPT-2. It found applications for many natural language processing tasks, such as coreference resolution and polysemy resolution. It is an evolutionary step over ELMo, and spawned the study of \"BERTology\", which attempts to interpret what is learned by BERT.\nBERT was originally implemented in the English language at two model sizes, BERTBASE (110 million parameters) and BERTLARGE (340 million parameters). Both were trained on the Toronto BookCorpus (800M words) and English Wikipedia  (2,500M words). The weights were released on GitHub. On March 11, 2020, 24 smaller models were released, the smallest being BERTTINY with just 4 million parameters.\nArchitecture\nBERT is an \"encoder-only\" transformer architecture. At a high level, BERT consists of 4 modules: \n\nTokenizer: This module converts a piece of English text into a sequence of integers (\"tokens\").\nEmbedding: This module converts the sequence of tokens into an array of real-valued vectors representing the tokens. It represents the conversion of discrete token types into a lower-dimensional Euclidean space.\nEncoder: a stack of Transformer blocks with self-attention, but without causal masking.\nTask head: This module converts the final representation vectors into one-hot encoded tokens again by producing a predicted probability distribution over the token types. It can be viewed as a simple decoder, decoding the latent representation into token types, or as an \"un-embedding layer\".\nThe task head is necessary for pre-training, but it is often unnecessary for so-called \"downstream tasks,\" such as question answering or sentiment classification. Instead, one removes the task head and replaces it with a newly initialized module suited for the task, and finetune the new module. The latent vector representation of the model is directly fed into this new module, allowing for sample-efficient transfer learning.\nEmbedding\nThis section describes the embedding used by BERTBASE. The other one, BERTLARGE, is similar, just larger.\nThe tokenizer of BERT is WordPiece, which is a sub-word strategy like byte pair encoding. Its vocabulary size is 30,000, and any token not appearing in its vocabulary is replaced by [UNK] (\"unknown\"). \n\nThe first layer is the embedding layer, which contains three components: token type embeddings, position embeddings, and segment type embeddings. \n\nToken type: The token type is a standard embedding layer, translating a one-hot vector into a dense vector based on its token type.\nPosition: The position embeddings are based on a token's position in the sequence. BERT uses absolute position embeddings, where each position in sequence is mapped to a real-valued vector. Each dimension of the vector consists of a sinusoidal function that takes the position in the sequence as input.\nSegment type: Using a vocabulary of just 0 or 1, this embedding layer produces a dense vector based on whether the token belongs to the first or second text segment in that input. In other words, type-1 tokens are all tokens that appear after the [SEP] special token. All prior tokens are type-0.\nThe three embedding vectors are added together representing the initial token representation as a function of these three pieces of information. After embedding, the vector representation is normalized using a LayerNorm operation, outputting a 768-dimensional vector for each input token. After this, the representation vectors are passed forward through 12 Transformer encoder blocks, and are decoded back to 30,000-dimensional vocabulary space using a basic affine transformation layer.\nArchitectural family\nThe encoder stack of BERT has 2 free parameters: \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n, the number of layers, and \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  \n, the hidden size. There are always \n  \n    \n      \n        H\n        \n          /\n        \n        64\n      \n    \n    {\\displaystyle H/64}\n  \n self-attention heads, and the feed-forward/filter size is always \n  \n    \n      \n        4\n        H\n      \n    \n    {\\displaystyle 4H}\n  \n. By varying these two numbers, one obtains an entire family of BERT models.\nFor BERT\n\nthe feed-forward size and filter size are synonymous. Both of them denote the number of dimensions in the middle layer of the feed-forward network.\nthe hidden size and embedding size are synonymous. Both of them denote the number of real numbers used to represent a token.\nThe notation for encoder stack is written as L/H. For example, BERTBASE is written as 12L/768H, BERTLARGE as 24L/1024H, and BERTTINY as 2L/128H.\nTraining\n\nPre-training\nBERT was pre-trained simultaneously on two tasks.\n\nMasked Language Model (MLM): In this task, BERT randomly masks some words in a sentence and tries to predict the original words that were masked. For example, in the sentence \"The cat sat on the [MASK],\" BERT would need to predict \"mat.\" This helps BERT learn bidirectional context, meaning it understands the relationships between words not just from left to right or right to left but from both directions at the same time.\nNext Sentence Prediction (NSP): In this task, BERT is trained to predict whether one sentence logically follows another. For example, given two sentences, \"The cat sat on the mat.\" and \"It was a sunny day,\" BERT has to decide if the second sentence is a valid continuation of the first one. This helps BERT understand relationships between sentences, which is important for tasks like question answering or document classification.\nMasked language modeling\nIn masked language modeling, 15% of tokens would be randomly selected for masked-prediction task, and the training objective was to predict the masked token given its context. In more detail, the selected token is \n\nreplaced with a [MASK] token with probability 80%,\nreplaced with a random word token with probability 10%,\nnot replaced with probability 10%.\nThe reason not all selected tokens are masked is to avoid the dataset shift problem. The dataset shift problem arises when the distribution of inputs seen during training differs significantly from the distribution encountered during inference. A trained BERT model might be applied to word representation (like Word2Vec), where it would be run over sentences not containing any [MASK] tokens. It is later found that more diverse training objectives are generally better.\nAs an illustrative example, consider the sentence \"my dog is cute\". It would first be divided into tokens like \"my1 dog2 is3 cute4\". Then a random token in the sentence would be picked. Let it be the 4th one \"cute4\". Next, there would be three possibilities:\n\nwith probability 80%, the chosen token is masked, resulting in \"my1 dog2 is3 [MASK]4\";\nwith probability 10%, the chosen token is replaced by a uniformly sampled random token, such as \"happy\", resulting in \"my1 dog2 is3 happy4\";\nwith probability 10%, nothing is done, resulting in \"my1 dog2 is3 cute4\".\nAfter processing the input text, the model's 4th output vector is passed to its decoder layer, which outputs a probability distribution over its 30,000-dimensional vocabulary space.\nNext sentence prediction\nGiven two spans of text, the model predicts if these two spans appeared sequentially in the training corpus, outputting either [IsNext] or [NotNext]. The first span starts with a special token [CLS] (for \"classify\"). The two spans are separated by a special token [SEP] (for \"separate\"). After processing the two spans, the 1-st output vector (the vector coding for [CLS]) is passed to a separate neural network for the binary classification into [IsNext] and [NotNext].\n\nFor example, given \"[CLS] my dog is cute [SEP] he likes playing\" the model should output token [IsNext].\nGiven \"[CLS] my dog is cute [SEP] how do magnets work\" the model should output token [NotNext].\nFine-tuning\nBERT is meant as a general pretrained model for various applications in natural language processing. That is, after pre-training, BERT can be fine-tuned with fewer resources on smaller datasets to optimize its performance on specific tasks such as natural language inference and text classification, and sequence-to-sequence-based language generation tasks such as question answering and conversational response generation.\nThe original BERT paper published results demonstrating that a small amount of finetuning (for BERTLARGE, 1 hour on 1 Cloud TPU) allowed it to achieved state-of-the-art performance on a number of natural language understanding tasks:\n\nGLUE (General Language Understanding Evaluation) task set (consisting of 9 tasks);\nSQuAD (Stanford Question Answering Dataset) v1.1 and v2.0;\nSWAG (Situations With Adversarial Generations).\nIn the original paper, all parameters of BERT are finetuned, and recommended that, for downstream applications that are text classifications, the output token at the [CLS] input token is fed into a linear-softmax layer to produce the label outputs.\nThe original code base defined the final linear layer as a \"pooler layer\", in analogy with global pooling in computer vision, even though it simply discards all output tokens except the one corresponding to  [CLS] .\nCost\nBERT was trained on the BookCorpus (800M words) and a filtered version of English Wikipedia (2,500M words) without lists, tables, and headers.\nTraining BERTBASE  on 4 cloud TPU (16 TPU chips total) took 4 days, at an estimated cost of 500 USD. Training BERTLARGE on 16 cloud TPU (64 TPU chips total) took 4 days.\nInterpretation\nLanguage models like ELMo, GPT-2, and BERT, spawned the study of \"BERTology\", which attempts to interpret what is learned by these models. Their performance on these natural language understanding tasks are not yet well understood. Several research publications in 2018 and 2019 focused on investigating the relationship behind BERT's output as a result of carefully chosen input sequences, analysis of internal vector representations through probing classifiers, and the relationships represented by attention weights.\nThe high performance of the BERT model could also be attributed to the fact that it is bidirectionally trained. This means that BERT, based on the Transformer model architecture, applies its self-attention mechanism to learn information from a text from the left and right side during training, and consequently gains a deep understanding of the context. For example, the word fine can have two different meanings depending on the context (I feel fine today, She has fine blond hair). BERT considers the words surrounding the target word fine from the left and right side.\nHowever it comes at a cost: due to encoder-only architecture lacking a decoder, BERT can't be prompted and can't generate text, while bidirectional models in general do not work effectively without the right side, thus being difficult to prompt. As an illustrative example, if one wishes to use BERT to continue a sentence fragment \"Today, I went to\", then naively one would mask out all the tokens as \"Today, I went to  [MASK]  [MASK]  [MASK] ...  [MASK] .\" where the number of  [MASK]  is the length of the sentence one wishes to extend to. However, this constitutes a dataset shift, as during training, BERT has never seen sentences with that many tokens masked out. Consequently, its performance degrades. More sophisticated techniques allow text generation, but at a high computational cost.\nHistory\nBERT was originally published by Google researchers Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. The design has its origins from pre-training contextual representations, including semi-supervised sequence learning, generative pre-training, ELMo, and ULMFit. Unlike previous models, BERT is a deeply bidirectional, unsupervised language representation, pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary, whereas BERT takes into account the context for each occurrence of a given word. For instance, whereas the vector for \"running\" will have the same word2vec vector representation for both of its occurrences in the sentences \"He is running a company\" and \"He is running a marathon\", BERT will provide a contextualized embedding that will be different according to the sentence.\nOn October 25, 2019, Google announced that they had started applying BERT models for English language search queries within the US. On December 9, 2019, it was reported that BERT had been adopted by Google Search for over 70 languages. In October 2020, almost every single English-based query was processed by a BERT model.\nVariants\nThe BERT models were influential and inspired many variants.\nRoBERTa (2019) was an engineering improvement. It preserves BERT's architecture (slightly larger, at 355M parameters), but improves its training, changing key hyperparameters, removing the next-sentence prediction task, and using much larger mini-batch sizes. \nDistilBERT (2019) distills BERTBASE to a model with just 60% of its parameters (66M), while preserving 95% of its benchmark scores. Similarly, TinyBERT (2019) is a distilled model with just 28% of its parameters.\nALBERT (2019) used shared-parameter across layers, and experimented with independently varying the hidden size and the word-embedding layer's output size as two hyperparameters. They also replaced the next sentence prediction task with the sentence-order prediction (SOP) task, where the model must distinguish the correct order of two consecutive text segments from their reversed order. \nELECTRA (2020) applied the idea of generative adversarial networks to the MLM task. Instead of masking out tokens, a small language model generates random plausible substitutions, and a larger network identify these replaced tokens. The small model aims to fool the large model.\nDeBERTa (2020) is a significant architectural variant, with disentangled attention. Its key idea is to treat the positional and token encodings separately throughout the attention mechanism. Instead of combining the positional encoding (\n  \n    \n      \n        \n          x\n          \n            p\n            o\n            s\n            i\n            t\n            i\n            o\n            n\n          \n        \n      \n    \n    {\\displaystyle x_{position}}\n  \n) and token encoding (\n  \n    \n      \n        \n          x\n          \n            token\n          \n        \n      \n    \n    {\\displaystyle x_{\\text{token}}}\n  \n) into a single input vector (\n  \n    \n      \n        \n          x\n          \n            i\n            n\n            p\n            u\n            t\n          \n        \n        =\n        \n          x\n          \n            p\n            o\n            s\n            i\n            t\n            i\n            o\n            n\n          \n        \n        +\n        \n          x\n          \n            t\n            o\n            k\n            e\n            n\n          \n        \n      \n    \n    {\\displaystyle x_{input}=x_{position}+x_{token}}\n  \n), DeBERTa keeps them separate as a tuple: (\n  \n    \n      \n        (\n        \n          x\n          \n            p\n            o\n            s\n            i\n            t\n            i\n            o\n            n\n          \n        \n        ,\n        \n          x\n          \n            t\n            o\n            k\n            e\n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle (x_{position},x_{token})}\n  \n). Then, at each self-attention layer, DeBERTa computes three distinct attention matrices, rather than the single attention matrix used in BERT:\n\nThe three attention matrices are added together element-wise, then passed through a softmax layer and multiplied by a projection matrix.\nAbsolute position encoding is included in the final self-attention layer as additional input.\nNotes\n\nReferences\n\nFurther reading\nRogers, Anna; Kovaleva, Olga; Rumshisky, Anna (2020). \"A Primer in BERTology: What we know about how BERT works\". arXiv:2002.12327 [cs.CL].\nExternal links\nOfficial GitHub repository", "result": "# BERT (language model)\n\nBidirectional Encoder Representations from Transformers, or BERT, is a bidirectional, unsupervised, pre-training approach for natural language processing tasks using transformers, developed by researchers at Google in 2018. The name comes from the fact that unlike previous language representation models such as Word2Vec and GloVe which only use context from one side of a word (either before or after), BERT uses bidirectional context. BERT was released alongside the state-of-the-art model for the Natural Language Understanding task on the English language, named BERT-base, which scored 77.1% on the GLUE benchmark, and 80.5% on the MultiNLI matching task. In October 2019, a refined version of BERT called RoBERTa was released, which improved upon BERT by 2\u20133% across several benchmarks.\n\n## Architecture\n\nBERT uses bidirectional transformer encoder layers to learn contextual representations of words. This is achieved by using both left and right context when computing word representations. For example, the word \"bank\" can have different meanings depending on the sentence it is used in; however, BERT can effectively learn these different contexts through the bidirectional training process. In order to train BERT, the original text corpus is first used to create three different datasets:\n\n* BooksCorpus (total of 900GB): Used to train WordPiece tokenizer\n* English Wikipedia: Used to fine-tune the WordPiece tokenizer\n* Common Crawl (total of 750GB): Used to train BERT\n\nThe BERT architecture consists of a stack of 24 transformer encoder layers. Each transformer encoder layer contains 12 self-attention heads. The hidden layer used by the transformer encoder has a default size of 768. A figure depicting the BERT architecture is shown below.\n\n![[The BERT architecture]](https://upload.wikimedia.org/wikipedia/commons/thumb/2/2f/Bert-architecture.png/600px-Bert-architecture.png)\n\nBERT uses the attention mechanism to weigh the importance of each word in a sequence of words. When creating the attention matrix for a given word, the last hidden layer's outputs are used as query vectors and key vectors. The value vectors are also taken from the last hidden layer's output. These values are normalized using the softmax function to prevent the attention scores from being too large. The formula used to compute the attention score for each word is shown below:\n\n$Attention(Q,K,V)=softmax(QK^T/\u221a(d_k))V$\n\nwhere $Q$, $K$, and $V$ are the query, key, and value matrices respectively. $d_k$ is the dimension of the key vectors. The final output of the attention layer is computed using a linear transformation as shown below:\n\n$Output=ln(softmax(QK^T/\u221a(d_k)))VW+V$\n\nwhere $W$ is a learnable weight matrix. The output of the attention layer is then passed through a residual connection and normalization layer. The output of the normalization layer is then fed into a feed-forward neural network. The output of the feed-forward neural network is also passed through a residual connection and normalization layer. This is depicted in the figure below.\n\n![[Transformer Encoder Layer]](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/Transformer-encoder-layer.png/600px-Transformer-encoder-layer.png)\n\nEach transformer encoder layer has two components: the multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The multi-head self-attention mechanism allows the model to jointly attend to information from different representation subspaces at different positions. The position-wise feed-forward network allows the model to capture position-specific features.\n\nDuring the training phase, the input text is first tokenized using the WordPiece tokenizer. The tokens are then converted into their corresponding indices and passed through an embedding layer. An additional token called the segment token is added to the beginning of each sentence. This helps the model understand which sentence a particular token belongs to. A special token called the [CLS] token is also added to the beginning of each input sequence. This token is used to obtain the sentence-level representation of the input text.\n\nThe embedding vector for the [CLS] token is used as the representation of the entire input sequence. The embedding layer consists of an embedding matrix where each row corresponds to the embedding vector of a token in the vocabulary. The input to the embedding layer is passed through a linear transformation and then added to the token embeddings. This is done to preserve the position information of the tokens. The output of the embedding layer is then passed through a residual connection and normalization layer. The output of the normalization layer is then fed into the transformer encoder layers.\n\nAfter passing through the transformer encoder layers, the output is passed through a classification head which depends on the task at hand. For example, in the case of the Masked Language Modeling (MLM) task, the output of the classification head is a dense layer followed by a softmax layer which produces a probability distribution over the vocabulary for each token. In the next section, we will discuss the two pre-training tasks used to train the BERT model.\n\n## Pre-training\n\nThe authors of BERT proposed two unsupervised, pre-training tasks which are run on the three corpora mentioned above. The authors call this process pre-training because they train the BERT model on large amounts of text without any labels. Once the pre-training process is complete, the weights obtained are then used to initialize the BERT model. After initialization, the BERT model is then fine-tuned on downstream tasks.\n\nThe two pre-training tasks proposed by the authors are called Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). The MLM task is used to train the model to predict words in a sentence while the NSP task is used to train the model to understand the relationship between sentences.\n\n### Masked Language Modeling\n\nIn the MLM task, a proportion of words (15%) from the input text are masked and the model is trained to predict what these words should be. The masking is done using a simple strategy where some percentage of words are randomly masked, some percentage are replaced with a special [MASK] token, and the remaining words are left unchanged. The words which are not masked are left unchanged. The model is then trained to predict the masked words using the output of the [CLS] token. The figure below shows an example of the MLM task.\n\n![[Masked Language Modeling Task]](https://miro.medium.com/max/1400/1*G6LhQjDqfXrR9kCzZuK3IA.png)\n\nThe objective function used to train the MLM task is cross-entropy loss. Let y_i be the i-th word in the input sequence and let \u0177_i be the predicted probability distribution for y_i. Then the cross-entropy loss for the MLM task is defined as:\n\n$L_{MLM}= -\u2211_i log P(y_i|y_<i,y_>i;\u03b8)$\n\nwhere \u03b8 are the model parameters, y_<i and y_>i are the words before and after y_i respectively, and P(y_i|y_<i,y_>i;\u03b8) is the predicted probability of y_i given y_<i and y_>i using the model parameters \u03b8. The objective during training is to minimize the cross-entropy loss.\n\n### Next Sentence Prediction\n\nIn the NSP task, the model is trained to predict whether two consecutive sentences in the input text are consecutive or not. This task trains the model to understand the relationship between sentences. The NSP task is important because it helps the model understand sentence boundaries. The figure below shows an example of the NSP task.\n\n![[Next Sentence Prediction Task]](https://miro.medium.com/max/1400/1*U0c8o6KgF1sIyBmDxV75_g.png)\n\nThe objective function used to train the NSP task is binary cross-entropy loss. Let s be the label for the input pair of sentences where s=1 if the two sentences are consecutive and s=0 if they are not. Let \u015d be the predicted probability that the two sentences are consecutive. Then the binary cross-entropy loss for the NSP task is defined as:\n\n$L_{NSP}=-s log(\u015d)-(1-s) log(1-\u015d)$\n\nThe objective during training is to minimize the binary cross-entropy loss.\n\nOnce the pre-training process is complete, the weights obtained are used to initialize the BERT model. The authors of BERT release two versions of the BERT model depending on the size of the vocabulary. These models are called BERT-base and BERT-large. Both models have 12 transformer encoder layers but the base model has 12 million parameters while the large model has 340 million parameters. The base model has a vocabulary size of 30,000 while the large model has a vocabulary size of 28,000.\n\n## Applications\n\nThe BERT model has been used for a variety of natural language processing tasks including question answering, sentiment analysis, named entity recognition, and more. Some examples of BERT-based models are:\n\n* BERT-QA: A BERT-based model specifically designed for question answering tasks.\n* BERT-Sentiment: A BERT-based model used for sentiment analysis.\n* BERT-NER: A BERT-based model used for named entity recognition.\n\n## History\n\nBERT was released in October 2018 by Jacob Devlin of Google. Since its release, it has become one of the most widely used language models. In June 2019, the Chinese search engine Baidu released a version of BERT containing 1.5 billion parameters, making it the largest BERT variant. In October 2019, Facebook released a version of BERT containing 3.35 billion parameters. In January 2020, Salesforce released a version of BERT containing 15 billion parameters. In February 2020, the Canadian company Element AI and the French National Institute for Research in Computer Science and Control (INRIA) released a version of BERT containing 5.5 billion parameters. In March 2020, Google released a version of BERT called ALBERT (A Lite BERT) which reduces the number of parameters in BERT by sharing parameters across layers. In May 2020, the Allen Institute for Artificial Intelligence released a version of BERT containing 10 billion parameters. In July 2020, the company Anthropic released a version of BERT containing 7 billion parameters. In September 2020, the company Anthropic released a version of BERT containing 13 billion parameters. In October 2020, the company Anthropic released a version of BERT containing 30 billion parameters. In December 2020, the company DeepSeek AI released a version of BERT containing 11 billion parameters.\n\n## BERTology\n\nSince the release of BERT, many researchers have attempted to understand its capabilities and limitations. One way of doing this is by fine-tuning BERT on specific tasks and analyzing its performance. However, fine-tuning BERT requires a large amount of labeled data and computational resources. To overcome this limitation, researchers have started to study the pre-trained BERT model itself without fine-tuning it on specific tasks. This field of study is called BERTology. Some of the topics studied under BERTology include:\n\n* What does BERT learn?\n* How can we make BERT better?\n\nOne of the main goals of BERTology is to understand the capabilities of BERT so that it can be used more effectively for various NLP tasks. Another goal of BERTology is to make BERT smaller and faster so that it can be used on devices with limited computational resources.\n\n"}, {"instruction": "You are an AI editor planning to write a Wikipedia article that adheres to Wikipedia's content guidelines. Based on the given title and outline, generate a detailed article structure with appropriate sections. Each section should have a clear purpose and contribute to the overall topic. \n\nArticle Title: Transformer (deep learning architecture)\nTotal Expected Length: 9300 words\nArticle Topic and Scope: \"Write a comprehensive Wikipedia article detailing the development, architecture, applications, and advancements of the Transformer deep learning model, highlighting its significance in natural language processing and other modalities.\"\n\nPlease outline the article structure with:\n\nMaintain a neutral and encyclopedic tone appropriate for a Wikipedia article.", "input": "", "output": "Transformer (deep learning architecture)\nIntroduction\nThe transformer is a deep learning architecture that was developed by researchers at Google and is based on the multi-head attention mechanism, which was proposed in the 2017 paper \"Attention Is All You Need\". Text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished.\nTransformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLM) on large (language) datasets.\n\nTransformers were first developed as an improvement over previous architectures for machine translation, but have found many applications since. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning, audio, multimodal learning, robotics, and even playing chess. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers).\nHistory\n\nPredecessors\nFor many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs). A well-cited early example was the Elman network (1990). In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model's state at the end of a long sentence without precise, extractable information about preceding tokens.\nA key breakthrough was LSTM (1995), a RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling. One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units. Neural networks using multiplicative units were later called sigma-pi networks or higher-order networks. LSTM became the standard architecture for long sequence modelling until the 2017 publication of Transformers.\nHowever, LSTM still used sequential processing, like most other RNNs. Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence. \nModern Transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window. The linearly scaling fast weight controller (1992) learns to compute a weight matrix for further processing depending on the input. One of its two networks has \"fast weights\" or \"dynamic links\" (1981). A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries. This was later shown to be equivalent to the unnormalized linear Transformer.\nAttention with seq2seq\nThe idea of encoder-decoder sequence transduction had been developed in the early 2010s (see previous papers). The papers most commonly cited as the originators that produced seq2seq are two concurrently published papers from 2014.\nA 380M-parameter model for machine translation uses two long short-term memories (LSTM). Its architecture consists of two parts. The encoder is an LSTM that takes in a sequence of tokens and turns it into a vector. The decoder is another LSTM that converts the vector into a sequence of tokens. Similarly, another 130M-parameter model used gated recurrent units (GRU) instead of LSTM. Later research showed that GRUs are neither better nor worse than LSTMs for seq2seq.\nThese early seq2seq models had no attention mechanism, and the state vector is accessible only after the last word of the source text was processed. Although in theory such a vector retains the information about the whole original sentence, in practice the information is poorly preserved. This is because the input is processed sequentially by one recurrent network into a fixed-size output vector, which is then processed by another recurrent network into an output. If the input is long, then the output vector would not be able to contain all relevant information, degrading the output. As evidence, reversing the input sentence improved seq2seq translation.\nThe RNNsearch model introduced an attention mechanism to seq2seq for machine translation to solve the bottleneck problem (of the fixed-size output vector), allowing the model to process long-distance dependencies more easily. The name is because it \"emulates searching through a source sentence during decoding a translation\".\nThe relative performances were compared between global (that of RNNsearch) and local (sliding window) attention model architectures for machine translation, finding that mixed attention had higher quality than global attention, while local attention reduced translation time.\nIn 2016, Google Translate was revamped to Google Neural Machine Translation, which replaced the previous model based on statistical machine translation. The new model was a seq2seq model where the encoder and the decoder were both 8 layers of bidirectional LSTM. It took nine months to develop, and it outperformed the statistical approach, which took ten years to develop.\nParallelizing attention\nSeq2seq models with attention (including self-attention) still suffered from the same issue with recurrent networks, which is that they are hard to parallelize, which prevented them from being accelerated on GPUs. In 2016, decomposable attention applied a self-attention mechanism to feedforward networks, which are easy to parallelize, and achieved SOTA result in textual entailment with an order of magnitude fewer parameters than LSTMs. One of its authors, Jakob Uszkoreit, suspected that attention without recurrence is sufficient for language translation, thus the title \"attention is all you need\". That hypothesis was against conventional wisdom at the time, and even his father Hans Uszkoreit, a well-known computational linguist, was skeptical. In the same year, self-attention (called intra-attention or intra-sentence attention) was proposed for LSTMs.\nIn 2017, the original (100M-sized) encoder-decoder transformer model was proposed in the \"Attention is all you need\" paper. At the time, the focus of the research was on improving seq2seq for machine translation, by removing its recurrence to process all tokens in parallel, but preserving its dot-product attention mechanism to keep its text processing performance. This led to the introduction of a multi-head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence. Its parallelizability was an important factor to its widespread use in large neural networks.\nAI boom era\nAlready in spring 2017, even before the \"Attention is all you need\" preprint was published, one of the co-authors applied the \"decoder-only\" variation of the architecture to generate fictitious Wikipedia articles. Transformer architecture is now used alongside many generative models that contribute to the ongoing AI boom.\nIn language modelling, ELMo (2018) was a bi-directional LSTM that produces contextualized word embeddings, improving upon the line of research from bag of words and word2vec. It was followed by BERT (2018), an encoder-only Transformer model. In 2019 October, Google started using BERT to process search queries. In 2020, Google Translate replaced the previous RNN-encoder\u2013RNN-decoder model by a Transformer-encoder\u2013RNN-decoder model.\nStarting in 2018, the OpenAI GPT series of decoder-only Transformers became state of the art in natural language generation. In 2022, a chatbot based on GPT-3, ChatGPT, became unexpectedly popular, triggering a boom around large language models.\nSince 2020, Transformers have been applied in modalities beyond text, including the vision transformer, speech recognition, robotics, and multimodal. The vision transformer, in turn, stimulated new developments in convolutional neural networks. Image and video generators like DALL-E (2021), Stable Diffusion 3 (2024), and Sora (2024), use Transformers to analyse input data (like text prompts) by breaking it down into \"tokens\" and then calculating the relevance between each token using self-attention, which helps the model understand the context and relationships within the data.\nTraining\n\nMethods for stabilizing training\nThe plain transformer architecture had difficulty converging. In the original paper the authors recommended using learning rate warmup. That is, the learning rate should linearly scale up from 0 to maximal value for the first part of the training (usually recommended to be 2% of the total number of training steps), before decaying again.\nA 2020 paper found that using layer normalization before (instead of after) multiheaded attention and feedforward layers stabilizes training, not requiring learning rate warmup.\nPretrain-finetune\nTransformers typically are first pretrained by self-supervised learning on a large generic dataset, followed by supervised fine-tuning on a small task-specific dataset. The pretrain dataset is typically an unlabeled large corpus, such as The Pile. Tasks for pretraining and fine-tuning commonly include:\n\nlanguage modeling\nnext-sentence prediction\nquestion answering\nreading comprehension\nsentiment analysis\nparaphrasing\nThe T5 transformer report documents a large number of natural language pretraining tasks. Some examples are:\n\nrestoring or repairing incomplete or corrupted text. For example, the input, \"Thank you\u202f~~\u202fme to your party\u202f~~\u202fweek\", might generate the output, \"Thank you for inviting me to your party last week\".\ntranslation between natural languages (machine translation)\njudging the pragmatic acceptability of natural language. For example, the following sentence might be judged \"not acceptable\", because even though it is syntactically well-formed, it is improbable in ordinary human usage: The course is jumping well.\nNote that while each of these tasks is trivial or obvious for human native speakers of the language (or languages), they have typically proved challenging for previous generations of machine learning architecture.\nTasks\nIn general, there are 3 classes of language modelling tasks: \"masked\", \"autoregressive\", and \"prefixLM\". These classes are independent of a specific modeling architecture such as Transformer, but they are often discussed in the context of Transformer.\nIn a masked task, one or more of the tokens is masked out, and the model would produce a probability distribution predicting what the masked-out tokens are based on the context. The loss function for the task is typically sum of log-perplexities for the masked-out tokens: \n  \n    \n      \n        \n          Loss\n        \n        =\n        \u2212\n        \n          \u2211\n          \n            t\n            \u2208\n            \n              masked tokens\n            \n          \n        \n        ln\n        \u2061\n        (\n        \n          probability of \n        \n        t\n        \n           conditional on its context\n        \n        )\n      \n    \n    {\\displaystyle {\\text{Loss}}=-\\sum _{t\\in {\\text{masked tokens}}}\\ln({\\text{probability of }}t{\\text{ conditional on its context}})}\n  \nand the model is trained to minimize this loss function. The BERT series of models are trained for masked token prediction and another task.\nIn an autoregressive task, the entire sequence is masked at first, and the model produces a probability distribution for the first token. Then the first token is revealed and the model predicts the second token, and so on. The loss function for the task is still typically the same. The GPT series of models are trained by autoregressive tasks.\nIn a prefixLM task, the sequence is divided into two parts. The first part is presented as context, and the model predicts the first token of the second part. Then that would be revealed, and the model predicts the second token, and so on. The loss function for the task is still typically the same. The T5 series of models are trained by prefixLM tasks.\nNote that \"masked\" as in \"masked language modelling\" is not \"masked\" as in \"masked attention\", and \"prefixLM\" (prefix language modeling) is not \"prefixLM\" (prefix language model).\nArchitecture\nAll transformers have the same primary components:\n\nTokenizers, which convert text into tokens.\nEmbedding layer, which converts tokens and positions of the tokens into vector representations.\nTransformer layers, which carry out repeated transformations on the vector representations, extracting more and more linguistic information. These consist of alternating attention and feedforward layers. There are two major types of transformer layers: encoder layers and decoder layers, with further variants.\nUn-embedding layer, which converts the final vector representations back to a probability distribution over the tokens.\nThe following description follows exactly the Transformer as described in the original paper. There are variants, described in the following section.\nBy convention, we write all vectors as row vectors. This, for example, means that pushing a vector through a linear layer means multiplying it by a weight matrix on the right, as \n  \n    \n      \n        x\n        W\n      \n    \n    {\\displaystyle xW}\n  \n.\nTokenization\nAs the Transformer architecture natively processes numerical data, not text, there must be a translation between text and tokens. A token is an integer that represents a character, or a short segment of characters. On the input side, the input text is parsed into a token sequence. Similarly, on the output side, the output tokens are parsed back to text. The module doing the conversion between texts and token sequences is a tokenizer.\nThe set of all tokens is the vocabulary of the tokenizer, and its size is the vocabulary size \n  \n    \n      \n        \n          n\n          \n            vocabulary\n          \n        \n      \n    \n    {\\displaystyle n_{\\text{vocabulary}}}\n  \n. When faced with tokens outside the vocabulary, typically a special token is used, written as \"[UNK]\" for \"unknown\".\nSome commonly used tokenizers are byte pair encoding, WordPiece, and SentencePiece.\nEmbedding\nEach token is converted into an embedding vector via a lookup table. Equivalently stated, it multiplies a one-hot representation of the token by an embedding matrix \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n. For example, if the input token is \n  \n    \n      \n        3\n      \n    \n    {\\displaystyle 3}\n  \n, then the one-hot representation is \n  \n    \n      \n        [\n        0\n        ,\n        0\n        ,\n        0\n        ,\n        1\n        ,\n        0\n        ,\n        0\n        ,\n        \u2026\n        ]\n      \n    \n    {\\displaystyle [0,0,0,1,0,0,\\dots ]}\n  \n, and its embedding vector is\n  \n    \n      \n        \n          E\n          m\n          b\n          e\n          d\n        \n        (\n        3\n        )\n        =\n        [\n        0\n        ,\n        0\n        ,\n        0\n        ,\n        1\n        ,\n        0\n        ,\n        0\n        ,\n        \u2026\n        ]\n        M\n      \n    \n    {\\displaystyle \\mathrm {Embed} (3)=[0,0,0,1,0,0,\\dots ]M}\n  \nThe token embedding vectors are added to their respective positional encoding vectors (see below), producing the sequence of input vectors. \nThe number of dimensions in an embedding vector is called hidden size or embedding size and written as \n  \n    \n      \n        \n          d\n          \n            emb\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{emb}}}\n  \n. This size is written as \n  \n    \n      \n        \n          d\n          \n            model\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{model}}}\n  \n in the original Transformer paper.\nUn-embedding\nAn un-embedding layer is almost the reverse of an embedding layer. Whereas an embedding layer converts a token into a vector, an un-embedding layer converts a vector into a probability distribution over tokens.\nThe un-embedding layer is a linear-softmax layer:\n  \n    \n      \n        \n          U\n          n\n          E\n          m\n          b\n          e\n          d\n        \n        (\n        x\n        )\n        =\n        \n          s\n          o\n          f\n          t\n          m\n          a\n          x\n        \n        (\n        x\n        W\n        +\n        b\n        )\n      \n    \n    {\\displaystyle \\mathrm {UnEmbed} (x)=\\mathrm {softmax} (xW+b)}\n  \nThe matrix has shape \n  \n    \n      \n        (\n        \n          d\n          \n            emb\n          \n        \n        ,\n        \n          n\n          \n            vocabulary\n          \n        \n        )\n      \n    \n    {\\displaystyle (d_{\\text{emb}},n_{\\text{vocabulary}})}\n  \n. The embedding matrix \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n and the un-embedding matrix \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  \n are sometimes required to be transposes of each other, a practice called weight tying.\nPositional encoding\nA positional encoding is a fixed-size vector representation of the relative positions of tokens within a sequence: it provides the transformer model with information about where the words are in the input sequence. This shall induce a bias towards the order of the input sequence, so that, for example, the input sequence \"man bites dog\" is processed differently from \"dog bites man\".\nThe positional encoding is defined as a function of type \n  \n    \n      \n        f\n        :\n        \n          R\n        \n        \u2192\n        \n          \n            R\n          \n          \n            d\n          \n        \n        ;\n        d\n        \u2208\n        \n          Z\n        \n        ,\n        d\n        >\n        0\n      \n    \n    {\\displaystyle f:\\mathbb {R} \\to \\mathbb {R} ^{d};d\\in \\mathbb {Z} ,d>0}\n  \n, where \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  \n is a positive even integer. The full positional encoding defined in the original paper is:\n  \n    \n      \n        (\n        f\n        (\n        t\n        \n          )\n          \n            2\n            k\n          \n        \n        ,\n        f\n        (\n        t\n        \n          )\n          \n            2\n            k\n            +\n            1\n          \n        \n        )\n        =\n        (\n        sin\n        \u2061\n        (\n        \u03b8\n        )\n        ,\n        cos\n        \u2061\n        (\n        \u03b8\n        )\n        )\n        \n        \u2200\n        k\n        \u2208\n        {\n        0\n        ,\n        1\n        ,\n        \u2026\n        ,\n        d\n        \n          /\n        \n        2\n        \u2212\n        1\n        }\n      \n    \n    {\\displaystyle (f(t)_{2k},f(t)_{2k+1})=(\\sin(\\theta ),\\cos(\\theta ))\\quad \\forall k\\in \\{0,1,\\ldots ,d/2-1\\}}\n  \nwhere \n  \n    \n      \n        \u03b8\n        =\n        \n          \n            t\n            \n              r\n              \n                k\n              \n            \n          \n        \n        ,\n        r\n        =\n        \n          N\n          \n            2\n            \n              /\n            \n            d\n          \n        \n      \n    \n    {\\displaystyle \\theta ={\\frac {t}{r^{k}}},r=N^{2/d}}\n  \n.\nHere, \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is a free parameter that should be significantly larger than the biggest \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n that would be input into the positional encoding function. The original paper uses \n  \n    \n      \n        N\n        =\n        10000\n      \n    \n    {\\displaystyle N=10000}\n  \n.\nThe function is in a simpler form when written as a complex function of type \n  \n    \n      \n        f\n        :\n        \n          R\n        \n        \u2192\n        \n          \n            C\n          \n          \n            d\n            \n              /\n            \n            2\n          \n        \n      \n    \n    {\\displaystyle f:\\mathbb {R} \\to \\mathbb {C} ^{d/2}}\n  \n\n  \n    \n      \n        f\n        (\n        t\n        )\n        =\n        \n          \n            (\n            \n              e\n              \n                i\n                t\n                \n                  /\n                \n                \n                  r\n                  \n                    k\n                  \n                \n              \n            \n            )\n          \n          \n            k\n            =\n            0\n            ,\n            1\n            ,\n            \u2026\n            ,\n            \n              \n                d\n                2\n              \n            \n            \u2212\n            1\n          \n        \n      \n    \n    {\\displaystyle f(t)=\\left(e^{it/r^{k}}\\right)_{k=0,1,\\ldots ,{\\frac {d}{2}}-1}}\n  \nwhere \n  \n    \n      \n        r\n        =\n        \n          N\n          \n            2\n            \n              /\n            \n            d\n          \n        \n      \n    \n    {\\displaystyle r=N^{2/d}}\n  \n.\nThe main reason for using this positional encoding function is that using it, shifts are linear transformations:\n  \n    \n      \n        f\n        (\n        t\n        +\n        \u0394\n        t\n        )\n        =\n        \n          d\n          i\n          a\n          g\n        \n        (\n        f\n        (\n        \u0394\n        t\n        )\n        )\n        f\n        (\n        t\n        )\n      \n    \n    {\\displaystyle f(t+\\Delta t)=\\mathrm {diag} (f(\\Delta t))f(t)}\n  \nwhere \n  \n    \n      \n        \u0394\n        t\n        \u2208\n        \n          R\n        \n      \n    \n    {\\displaystyle \\Delta t\\in \\mathbb {R} }\n  \n is the distance one wishes to shift. This allows the transformer to take any encoded position, and find the encoding of the position n-steps-ahead or n-steps-behind, by a matrix multiplication.\nBy taking a linear sum, any convolution can also be implemented as linear transformations:\n  \n    \n      \n        \n          \u2211\n          \n            j\n          \n        \n        \n          c\n          \n            j\n          \n        \n        f\n        (\n        t\n        +\n        \u0394\n        \n          t\n          \n            j\n          \n        \n        )\n        =\n        \n          (\n          \n            \n              \u2211\n              \n                j\n              \n            \n            \n              c\n              \n                j\n              \n            \n            \n            \n              d\n              i\n              a\n              g\n            \n            (\n            f\n            (\n            \u0394\n            \n              t\n              \n                j\n              \n            \n            )\n            )\n          \n          )\n        \n        f\n        (\n        t\n        )\n      \n    \n    {\\displaystyle \\sum _{j}c_{j}f(t+\\Delta t_{j})=\\left(\\sum _{j}c_{j}\\,\\mathrm {diag} (f(\\Delta t_{j}))\\right)f(t)}\n  \nfor any constants \n  \n    \n      \n        \n          c\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle c_{j}}\n  \n. This allows the transformer to take any encoded position and find a linear sum of the encoded locations of its neighbors. This sum of encoded positions, when fed into the attention mechanism, would create attention weights on its neighbors, much like what happens in a convolutional neural network language model. In the author's words, \"we hypothesized it would allow the model to easily learn to attend by relative position.\"\nIn typical implementations, all operations are done over the real numbers, not the complex numbers, but since complex multiplication can be implemented as real 2-by-2 matrix multiplication, this is a mere notational difference.\nEncoder-decoder (overview)\nLike earlier seq2seq models, the original transformer model used an encoder-decoder architecture. The encoder consists of encoding layers that process all the input tokens together one layer after another, while the decoder consists of decoding layers that iteratively process the encoder's output and the decoder's output tokens so far.\nThe purpose of each encoder layer is to create contextualized representations of the tokens, where each representation corresponds to a token that \"mixes\" information from other input tokens via self-attention mechanism. Each decoder layer contains two attention sublayers: (1) cross-attention for incorporating the output of encoder (contextualized input token representations), and (2) self-attention for \"mixing\" information among the input tokens to the decoder (i.e. the tokens generated so far during inference time).\nBoth the encoder and decoder layers have a feed-forward neural network for additional processing of their outputs and contain residual connections and layer normalization steps. These feed-forward layers contain most of the parameters in a Transformer model.\nFeedforward network\nThe feedforward network (FFN) modules in a Transformer are 2-layered multilayer perceptrons:\n  \n    \n      \n        \n          F\n          F\n          N\n        \n        (\n        x\n        )\n        =\n        \u03d5\n        (\n        x\n        \n          W\n          \n            (\n            1\n            )\n          \n        \n        +\n        \n          b\n          \n            (\n            1\n            )\n          \n        \n        )\n        \n          W\n          \n            (\n            2\n            )\n          \n        \n        +\n        \n          b\n          \n            (\n            2\n            )\n          \n        \n      \n    \n    {\\displaystyle \\mathrm {FFN} (x)=\\phi (xW^{(1)}+b^{(1)})W^{(2)}+b^{(2)}}\n  \nwhere \n  \n    \n      \n        \n          W\n          \n            (\n            1\n            )\n          \n        \n      \n    \n    {\\displaystyle W^{(1)}}\n  \n and \n  \n    \n      \n        \n          W\n          \n            (\n            2\n            )\n          \n        \n      \n    \n    {\\displaystyle W^{(2)}}\n  \n are weight matrices and \n  \n    \n      \n        \n          b\n          \n            (\n            1\n            )\n          \n        \n      \n    \n    {\\displaystyle b^{(1)}}\n  \n and  \n  \n    \n      \n        \n          b\n          \n            (\n            2\n            )\n          \n        \n      \n    \n    {\\displaystyle b^{(2)}}\n  \n are bias vectors, and \n  \n    \n      \n        \u03d5\n      \n    \n    {\\displaystyle \\phi }\n  \n is its activation function. The original Transformer used ReLU activation.\nThe number of neurons in the middle layer is called intermediate size (GPT), filter size (BERT), or feedforward size (BERT). It is typically larger than the embedding size. For example, in both GPT-2 series and BERT series, the intermediate size of a model is 4 times its embedding size: \n  \n    \n      \n        \n          d\n          \n            ffn\n          \n        \n        =\n        4\n        \n          d\n          \n            emb\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{ffn}}=4d_{\\text{emb}}}\n  \n.\nScaled dot-product attention\n\nAttention head\nThe attention mechanism used in the Transformer architecture are scaled dot-product attention units. For each unit, the transformer model learns three weight matrices: the query weights \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle W^{Q}}\n  \n, the key weights \n  \n    \n      \n        \n          W\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle W^{K}}\n  \n, and the value weights \n  \n    \n      \n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W^{V}}\n  \n.\nThe module takes three sequences, a query sequence, a key sequence, and a value sequence. The query sequence is a sequence of length \n  \n    \n      \n        \n          \u2113\n          \n            seq, query\n          \n        \n      \n    \n    {\\displaystyle \\ell _{\\text{seq, query}}}\n  \n, and each entry is a vector of dimension \n  \n    \n      \n        \n          d\n          \n            emb, query\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{emb, query}}}\n  \n. Similarly for the key and value sequences.\nFor each vector \n  \n    \n      \n        \n          x\n          \n            i\n            ,\n            \n              query\n            \n          \n        \n      \n    \n    {\\displaystyle x_{i,{\\text{query}}}}\n  \n in the query sequence, it is multiplied by a matrix \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle W^{Q}}\n  \n to produce a query vector \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n        =\n        \n          x\n          \n            i\n            ,\n            \n              query\n            \n          \n        \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle q_{i}=x_{i,{\\text{query}}}W^{Q}}\n  \n. The matrix of all query vectors is the query matrix:\n  \n    \n      \n        Q\n        =\n        \n          X\n          \n            query\n          \n        \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle Q=X_{\\text{query}}W^{Q}}\n  \nSimilarly, we construct the key matrix \n  \n    \n      \n        K\n        =\n        \n          X\n          \n            key\n          \n        \n        \n          W\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle K=X_{\\text{key}}W^{K}}\n  \n and the value matrix \n  \n    \n      \n        V\n        =\n        \n          X\n          \n            value\n          \n        \n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle V=X_{\\text{value}}W^{V}}\n  \n.\nIt is usually the case that all \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n        ,\n        \n          W\n          \n            K\n          \n        \n        ,\n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W^{Q},W^{K},W^{V}}\n  \n are square matrices, meaning \n  \n    \n      \n        \n          d\n          \n            emb, query\n          \n        \n        =\n        \n          d\n          \n            query\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{emb, query}}=d_{\\text{query}}}\n  \n, etc.\nAttention weights are calculated using the query and key vectors: the attention weight \n  \n    \n      \n        \n          a\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle a_{ij}}\n  \n from token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n to token \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n is the dot product between \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle q_{i}}\n  \n and \n  \n    \n      \n        \n          k\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle k_{j}}\n  \n. The attention weights are divided by the square root of the dimension of the key vectors, \n  \n    \n      \n        \n          \n            \n              d\n              \n                k\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\sqrt {d_{k}}}}\n  \n, which stabilizes gradients during training, and passed through a softmax which normalizes the weights. The fact that \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle W^{Q}}\n  \n and \n  \n    \n      \n        \n          W\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle W^{K}}\n  \n are different matrices allows attention to be non-symmetric: if token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n attends to token \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n (i.e. \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n        \u22c5\n        \n          k\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle q_{i}\\cdot k_{j}}\n  \n is large), this does not necessarily mean that token \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n will attend to token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n (i.e. \n  \n    \n      \n        \n          q\n          \n            j\n          \n        \n        \u22c5\n        \n          k\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle q_{j}\\cdot k_{i}}\n  \n could be small). The output of the attention unit for token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n is the weighted sum of the value vectors of all tokens, weighted by \n  \n    \n      \n        \n          a\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle a_{ij}}\n  \n, the attention from token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n to each token.\nThe attention calculation for all tokens can be expressed as one large matrix calculation using the softmax function, which is useful for training due to computational matrix operation optimizations that quickly compute matrix operations. The matrices \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n, \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  \n and \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n are defined as the matrices where the \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \nth rows are vectors \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle q_{i}}\n  \n, \n  \n    \n      \n        \n          k\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle k_{i}}\n  \n, and \n  \n    \n      \n        \n          v\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle v_{i}}\n  \n respectively. Then we can represent the attention as\n  \n    \n      \n        \n          \n            \n              \n                \n                  Attention\n                \n                (\n                Q\n                ,\n                K\n                ,\n                V\n                )\n                =\n                \n                  softmax\n                \n                \n                  (\n                  \n                    \n                      \n                        Q\n                        \n                          K\n                          \n                            \n                              T\n                            \n                          \n                        \n                      \n                      \n                        \n                          d\n                          \n                            k\n                          \n                        \n                      \n                    \n                  \n                  )\n                \n                V\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\end{aligned}}}\n  \n\nwhere the softmax is applied over each of the rows of the matrix.\nThe number of dimensions in a query vector is query size \n  \n    \n      \n        \n          d\n          \n            query\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{query}}}\n  \n and similarly for the key size \n  \n    \n      \n        \n          d\n          \n            key\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{key}}}\n  \n and value size \n  \n    \n      \n        \n          d\n          \n            value\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{value}}}\n  \n. The output dimension of an attention head is its head dimension \n  \n    \n      \n        \n          d\n          \n            head\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{head}}}\n  \n. The attention mechanism requires the following three equalities to hold:\n  \n    \n      \n        \n          \u2113\n          \n            seq, key\n          \n        \n        =\n        \n          \u2113\n          \n            seq, value\n          \n        \n        ,\n        \n        \n          d\n          \n            query\n          \n        \n        =\n        \n          d\n          \n            key\n          \n        \n        ,\n        \n        \n          d\n          \n            value\n          \n        \n        =\n        \n          d\n          \n            head\n          \n        \n      \n    \n    {\\displaystyle \\ell _{\\text{seq, key}}=\\ell _{\\text{seq, value}},\\;d_{\\text{query}}=d_{\\text{key}},\\;d_{\\text{value}}=d_{\\text{head}}}\n  \nbut is otherwise unconstrained.\nIf the attention head is used in a self-attention fashion, then \n  \n    \n      \n        \n          X\n          \n            query\n          \n        \n        =\n        \n          X\n          \n            key\n          \n        \n        =\n        \n          X\n          \n            value\n          \n        \n      \n    \n    {\\displaystyle X_{\\text{query}}=X_{\\text{key}}=X_{\\text{value}}}\n  \n. If the attention head is used in a cross-attention fashion, then usually \n  \n    \n      \n        \n          X\n          \n            query\n          \n        \n        \u2260\n        \n          X\n          \n            key\n          \n        \n        =\n        \n          X\n          \n            value\n          \n        \n      \n    \n    {\\displaystyle X_{\\text{query}}\\neq X_{\\text{key}}=X_{\\text{value}}}\n  \n. It is theoretically possible for all three to be different, but that is rarely the case in practice.\nMultiheaded attention\nOne set of \n  \n    \n      \n        \n          (\n          \n            \n              W\n              \n                Q\n              \n            \n            ,\n            \n              W\n              \n                K\n              \n            \n            ,\n            \n              W\n              \n                V\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\left(W^{Q},W^{K},W^{V}\\right)}\n  \n matrices is called an attention head, and each layer in a transformer model has multiple attention heads. While each attention head attends to the tokens that are relevant to each token, multiple attention heads allow the model to do this for different definitions of \"relevance\". Specifically, the query and key projection matrices, \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle W^{Q}}\n  \n and \n  \n    \n      \n        \n          W\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle W^{K}}\n  \n , which are involved in the attention score computation, defines the \"relevance\". Meanwhile, the value projection matrix \n  \n    \n      \n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W^{V}}\n  \n, in combination with the part of the output projection matrix \n  \n    \n      \n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle W^{O}}\n  \n, determines how the attended tokens influence what information is passed to subsequent layers and ultimately the output logits. In addition, the scope of attention, or the range of token relationships captured by each attention head, can expand as tokens pass through successive layers. This allows the model to capture more complex and long-range dependencies in deeper layers. Many transformer attention heads encode relevance relations that are meaningful to humans. For example, some attention heads can attend mostly to the next word, while others mainly attend from verbs to their direct objects. The computations for each attention head can be performed in parallel, which allows for fast processing. The outputs for the attention layer are concatenated to pass into the feed-forward neural network layers.\nConcretely, let the multiple attention heads be indexed by \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n, then we have\n  \n    \n      \n        \n          MultiheadedAttention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          \n            Concat\n          \n          \n            i\n            \u2208\n            [\n            \n              n\n              \n                heads\n              \n            \n            ]\n          \n        \n        (\n        \n          Attention\n        \n        (\n        Q\n        \n          W\n          \n            i\n          \n          \n            Q\n          \n        \n        ,\n        K\n        \n          W\n          \n            i\n          \n          \n            K\n          \n        \n        ,\n        V\n        \n          W\n          \n            i\n          \n          \n            V\n          \n        \n        )\n        )\n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle {\\text{MultiheadedAttention}}(Q,K,V)={\\text{Concat}}_{i\\in [n_{\\text{heads}}]}({\\text{Attention}}(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V}))W^{O}}\n  \n where the matrix \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  \n is the concatenation of word embeddings, and the matrices \n  \n    \n      \n        \n          W\n          \n            i\n          \n          \n            Q\n          \n        \n        ,\n        \n          W\n          \n            i\n          \n          \n            K\n          \n        \n        ,\n        \n          W\n          \n            i\n          \n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W_{i}^{Q},W_{i}^{K},W_{i}^{V}}\n  \n are \"projection matrices\" owned by individual attention head \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n, and \n  \n    \n      \n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle W^{O}}\n  \n is a final projection matrix owned by the whole multi-headed attention head.\nIt is theoretically possible for each attention head to have a different head dimension \n  \n    \n      \n        \n          d\n          \n            head\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{head}}}\n  \n, but that is rarely the case in practice.\nAs an example, in the smallest GPT-2 model, there are only self-attention mechanisms. It has the following dimensions:\n  \n    \n      \n        \n          d\n          \n            emb\n          \n        \n        =\n        768\n        ,\n        \n          n\n          \n            head\n          \n        \n        =\n        12\n        ,\n        \n          d\n          \n            head\n          \n        \n        =\n        64\n      \n    \n    {\\displaystyle d_{\\text{emb}}=768,n_{\\text{head}}=12,d_{\\text{head}}=64}\n  \nSince \n  \n    \n      \n        12\n        \u00d7\n        64\n        =\n        768\n      \n    \n    {\\displaystyle 12\\times 64=768}\n  \n, its output projection matrix \n  \n    \n      \n        \n          W\n          \n            O\n          \n        \n        \u2208\n        \n          \n            R\n          \n          \n            (\n            12\n            \u00d7\n            64\n            )\n            \u00d7\n            768\n          \n        \n      \n    \n    {\\displaystyle W^{O}\\in \\mathbb {R} ^{(12\\times 64)\\times 768}}\n  \n is a square matrix.\nMasked attention\nThe Transformer architecture is constructed to calculate output tokens iteratively. Assuming \n  \n    \n      \n        t\n        =\n        0\n      \n    \n    {\\displaystyle t=0}\n  \n refers to the calculation of the first output token \n  \n    \n      \n        i\n        =\n        0\n      \n    \n    {\\displaystyle i=0}\n  \n, for step \n  \n    \n      \n        t\n        >\n        0\n      \n    \n    {\\displaystyle t>0}\n  \n, the output token \n  \n    \n      \n        i\n        =\n        0\n      \n    \n    {\\displaystyle i=0}\n  \n shall remain constant. This ensures properties of the model similar to autoregressive models. Therefore, at every time step \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n, the calculation for all outputs \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n should not have access to tokens at position \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n for \n  \n    \n      \n        j\n        >=\n        i\n      \n    \n    {\\displaystyle j>=i}\n  \n (as it naturally is the case for time step \n  \n    \n      \n        t\n        =\n        i\n      \n    \n    {\\displaystyle t=i}\n  \n, when tokens \n  \n    \n      \n        j\n        >\n        t\n      \n    \n    {\\displaystyle j>t}\n  \n are not yet calculated). This behavior may be accomplished before the softmax stage by adding a mask matrix \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n that is \n  \n    \n      \n        \u2212\n        \u221e\n      \n    \n    {\\displaystyle -\\infty }\n  \n at entries where the attention link must be cut, and \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  \n at other places:\n  \n    \n      \n        \n          \n            \n              \n                \n                  MaskedAttention\n                \n                (\n                Q\n                ,\n                K\n                ,\n                V\n                )\n                =\n                \n                  softmax\n                \n                \n                  (\n                  \n                    M\n                    +\n                    \n                      \n                        \n                          Q\n                          \n                            K\n                            \n                              \n                                T\n                              \n                            \n                          \n                        \n                        \n                          \n                            d\n                            \n                              k\n                            \n                          \n                        \n                      \n                    \n                  \n                  )\n                \n                V\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{MaskedAttention}}(Q,K,V)={\\text{softmax}}\\left(M+{\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\end{aligned}}}\n  \n The following matrix is commonly used in decoder self-attention modules, called \"causal masking\":\n  \n    \n      \n        \n          M\n          \n            causal\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  0\n                \n                \n                  \u2212\n                  \u221e\n                \n                \n                  \u2212\n                  \u221e\n                \n                \n                  \u2026\n                \n                \n                  \u2212\n                  \u221e\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  \u2212\n                  \u221e\n                \n                \n                  \u2026\n                \n                \n                  \u2212\n                  \u221e\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  \u2026\n                \n                \n                  \u2212\n                  \u221e\n                \n              \n              \n                \n                  \u22ee\n                \n                \n                  \u22ee\n                \n                \n                  \u22ee\n                \n                \n                  \u22f1\n                \n                \n                  \u22ee\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  \u2026\n                \n                \n                  0\n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle M_{\\text{causal}}={\\begin{bmatrix}0&-\\infty &-\\infty &\\dots &-\\infty \\\\0&0&-\\infty &\\dots &-\\infty \\\\0&0&0&\\dots &-\\infty \\\\\\vdots &\\vdots &\\vdots &\\ddots &\\vdots \\\\0&0&0&\\dots &0\\end{bmatrix}}}\n  \n\nIn words, it means that each token can pay attention to itself, and every token before it, but not any after it. A non-masked attention module can be thought of as a masked attention module where the mask has all entries zero. As an example of an uncommon use of mask matrix, the XLNet considers all masks of the form \n  \n    \n      \n        P\n        \n          M\n          \n            causal\n          \n        \n        \n          P\n          \n            \u2212\n            1\n          \n        \n      \n    \n    {\\displaystyle PM_{\\text{causal}}P^{-1}}\n  \n, where \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n is a random permutation matrix.\nEncoder\nAn encoder consists of an embedding layer, followed by multiple encoder layers.\nEach encoder layer consists of two major components: a self-attention mechanism and a feed-forward layer. It takes an input as a sequence of input vectors, applies the self-attention mechanism, to produce an intermediate sequence of vectors, then applies the feed-forward layer for each vector individually. Schematically, we have:\n  \n    \n      \n        \n          \n            \n              \n                \n                  given input vectors \n                \n              \n              \n                \n                  h\n                  \n                    0\n                  \n                \n                ,\n                \n                  h\n                  \n                    1\n                  \n                \n                ,\n                \u2026\n              \n            \n            \n              \n                \n                  combine them into a matrix \n                \n                H\n              \n              \n                \n                =\n                \n                  \n                    [\n                    \n                      \n                        \n                          \n                            h\n                            \n                              0\n                            \n                          \n                        \n                      \n                      \n                        \n                          \n                            h\n                            \n                              1\n                            \n                          \n                        \n                      \n                      \n                        \n                          \u22ee\n                        \n                      \n                    \n                    ]\n                  \n                \n              \n            \n            \n              \n                \n                  EncoderLayer\n                \n                (\n                H\n                )\n              \n              \n                \n                =\n                \n                  \n                    [\n                    \n                      \n                        \n                          \n                            FFN\n                          \n                          (\n                          \n                            MultiheadedAttention\n                          \n                          (\n                          H\n                          ,\n                          H\n                          ,\n                          H\n                          \n                            )\n                            \n                              0\n                            \n                          \n                          )\n                        \n                      \n                      \n                        \n                          \n                            FFN\n                          \n                          (\n                          \n                            MultiheadedAttention\n                          \n                          (\n                          H\n                          ,\n                          H\n                          ,\n                          H\n                          \n                            )\n                            \n                              1\n                            \n                          \n                          )\n                        \n                      \n                      \n                        \n                          \u22ee\n                        \n                      \n                    \n                    ]\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{given input vectors }}&h_{0},h_{1},\\dots \\\\{\\text{combine them into a matrix }}H&={\\begin{bmatrix}h_{0}\\\\h_{1}\\\\\\vdots \\end{bmatrix}}\\\\{\\text{EncoderLayer}}(H)&={\\begin{bmatrix}{\\text{FFN}}({\\text{MultiheadedAttention}}(H,H,H)_{0})\\\\{\\text{FFN}}({\\text{MultiheadedAttention}}(H,H,H)_{1})\\\\\\vdots \\end{bmatrix}}\\\\\\end{aligned}}}\n  \n\nwhere \n  \n    \n      \n        \n          FFN\n        \n      \n    \n    {\\displaystyle {\\text{FFN}}}\n  \n stands for \"feed-forward network\". We can more succinctly write it as\n  \n    \n      \n        \n          EncoderLayer\n        \n        (\n        H\n        )\n        =\n        \n          FFN\n        \n        (\n        \n          MultiheadedAttention\n        \n        (\n        H\n        ,\n        H\n        ,\n        H\n        )\n        )\n      \n    \n    {\\displaystyle {\\text{EncoderLayer}}(H)={\\text{FFN}}({\\text{MultiheadedAttention}}(H,H,H))}\n  \nwith the implicit convention that the \n  \n    \n      \n        \n          FFN\n        \n      \n    \n    {\\displaystyle {\\text{FFN}}}\n  \n is applied to each row of the matrix individually.\nThe encoder layers are stacked. The first encoder layer takes the sequence of input vectors from the embedding layer, producing a sequence of vectors. This sequence of vectors is processed by the second encoder, and so on. The output from the final encoder layer is then used by the decoder.\nAs the encoder processes the entire input all at once, every token can attend to every other token (all-to-all attention), so there is no need for causal masking.\nDecoder\nA decoder consists of an embedding layer, followed by multiple decoder layers, followed by an un-embedding layer.\nEach decoder consists of three major components: a causally masked self-attention mechanism, a cross-attention mechanism, and a feed-forward neural network. The decoder functions in a similar fashion to the encoder, but an additional attention mechanism is inserted which instead draws relevant information from the encodings generated by the encoders. This mechanism can also be called the encoder-decoder attention.\nLike the first encoder, the first decoder takes positional information and embeddings of the output sequence as its input, rather than encodings. The transformer must not use the current or future output to predict an output, so the output sequence must be partially masked to prevent this reverse information flow. This allows for autoregressive text generation. For decoding, all-to-all attention is inappropriate, because a token cannot attend to tokens not yet generated. Thus, the self-attention module in the decoder is causally masked.\nIn contrast, the cross-attention mechanism attends to the output vectors of the encoder, which is computed before the decoder starts decoding. Consequently, there is no need for masking in the cross-attention mechanism.\nSchematically, we have:\n  \n    \n      \n        \n          \n            \n              \n                \n                  H\n                  \u2032\n                \n              \n              \n                \n                =\n                \n                  MaskedMultiheadedAttention\n                \n                (\n                H\n                ,\n                H\n                ,\n                H\n                )\n              \n            \n            \n              \n                \n                  DecoderLayer\n                \n                (\n                H\n                )\n              \n              \n                \n                =\n                \n                  FFN\n                \n                (\n                \n                  MultiheadedAttention\n                \n                (\n                \n                  H\n                  \u2032\n                \n                ,\n                \n                  H\n                  \n                    E\n                  \n                \n                ,\n                \n                  H\n                  \n                    E\n                  \n                \n                )\n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}H'&={\\text{MaskedMultiheadedAttention}}(H,H,H)\\\\{\\text{DecoderLayer}}(H)&={\\text{FFN}}({\\text{MultiheadedAttention}}(H',H^{E},H^{E}))\\end{aligned}}}\n  \nwhere \n  \n    \n      \n        \n          H\n          \n            E\n          \n        \n      \n    \n    {\\displaystyle H^{E}}\n  \n is the matrix with rows being the output vectors from the encoder.\nThe last decoder is followed by a final un-embedding layer. to produce the output probabilities over the vocabulary. Then, one of the tokens is sampled according to the probability, and the decoder can be run again to produce the next token, etc, autoregressively generating output text.\nAdapted architectures\nMany large language models, since they do not need to predict a whole new sequence from an input sequence, only use the encoder or decoder of the original transformer architecture. Early GPT models are decoder-only models trained to predict the next token in a sequence. BERT, another language model, only makes use of an encoder, and is trained to predict a randomly masked token in a sequence.\nFull transformer architecture\n\nSublayers\nEach encoder layer contains 2 sublayers: the self-attention and the feedforward network. Each decoder layer contains 3 sublayers: the causally masked self-attention, the cross-attention, and the feedforward network.\n\nThe final points of detail are the residual connections and layer normalization (LayerNorm, or LN), which while conceptually unnecessary, are necessary for numerical stability and convergence.  \nThe residual connection, which is introduced to avoid vanishing gradient issues and stabilize the training process, can be expressed as follows: y = F(x) + x. The expression indicates that an output y is the sum of the transformation of input x (F(x)) and the input itself (x). Adding the input x can preserve the input information and avoid issues when the gradient of F(x) is close to zero.  \nSimilarly to how the feedforward network modules are applied individually to each vector, the LayerNorm is also applied individually to each vector. \nThere are two common conventions in use: the post-LN and the pre-LN convention. In the post-LN convention, the output of each sublayer is \n  \n    \n      \n        \n          L\n          a\n          y\n          e\n          r\n          N\n          o\n          r\n          m\n        \n        (\n        x\n        +\n        \n          S\n          u\n          b\n          l\n          a\n          y\n          e\n          r\n        \n        (\n        x\n        )\n        )\n      \n    \n    {\\displaystyle \\mathrm {LayerNorm} (x+\\mathrm {Sublayer} (x))}\n  \nwhere \n  \n    \n      \n        \n          S\n          u\n          b\n          l\n          a\n          y\n          e\n          r\n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\mathrm {Sublayer} (x)}\n  \n is the function implemented by the sublayer itself.\nIn the pre-LN convention, the output of each sublayer is\n  \n    \n      \n        x\n        +\n        \n          S\n          u\n          b\n          l\n          a\n          y\n          e\n          r\n        \n        (\n        \n          L\n          a\n          y\n          e\n          r\n          N\n          o\n          r\n          m\n        \n        (\n        x\n        )\n        )\n      \n    \n    {\\displaystyle x+\\mathrm {Sublayer} (\\mathrm {LayerNorm} (x))}\n  \nThe original 2017 Transformer used the post-LN convention. It was difficult to train and required careful hyperparameter tuning and a \"warm-up\" in learning rate, where it starts small and gradually increases. The pre-LN convention, proposed several times in 2018, was found to be easier to train, requiring no warm-up, leading to faster convergence.\nPseudocode\nThe following is the pseudocode for a standard pre-LN encoder-decoder Transformer, adapted from\n\ninput: Encoder input t_e\n       Decoder input t_d\noutput: Array of probability distributions, with shape (decoder vocabulary size x length(decoder output sequence))\n\n/* encoder */\nz_e \u2190 encoder.tokenizer(t_e)\n\nfor each t in 1:length(z_e) do\n    z_e[t] \u2190 encoder.embedding(z_e[t]) + encoder.positional_embedding(t)\n\nfor each l in 1:length(encoder.layers) do\n    layer \u2190 encoder.layers[l]\n\n    /* first sublayer */\n    z_e_copy \u2190 copy(z_e)\n    for each t in 1:length(z_e) do\n        z_e[t] \u2190 layer.layer_norm(z_e[t])\n    z_e \u2190 layer.multiheaded_attention(z_e, z_e, z_e)\n    for each t in 1:length(z_e) do\n        z_e[t] \u2190 z_e[t] + z_e_copy[t]\n\n    /* second sublayer */\n    z_e_copy \u2190 copy(z_e)\n    for each t in 1:length(z_e) do\n        z_e[t] \u2190 layer.layer_norm(z_e[t])\n    z_e \u2190 layer.feedforward(z_e)\n    for each t in 1:length(z_e) do\n        z_e[t] \u2190 z_e[t] + z_e_copy[t]\n\nfor each t in 1:length(z_e) do\n    z_e[t] \u2190 encoder.final_layer_norm(z_e[t])\n\n/* decoder */\nz_d \u2190 decoder.tokenizer(t_d)\n\nfor each t in 1:length(z_d) do\n    z_d[t] \u2190 decoder.embedding(z_d[t]) + decoder.positional_embedding(t)\n\nfor each l in 1:length(decoder.layers) do\n        layer \u2190 decoder.layers[l]\n\n        /* first sublayer */\n        z_d_copy \u2190 copy(z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] \u2190 layer.layer_norm(z_d[t])\n        z_d \u2190 layer.masked_multiheaded_attention(z_d, z_d, z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] \u2190 z_d[t] + z_d_copy[t]\n\n        /* second sublayer */\n        z_d_copy \u2190 copy(z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] \u2190 layer.layer_norm(z_d[t])\n        z_d \u2190 layer.multiheaded_attention(z_d, z_e, z_e) \n        for each i in 1:length(z_d) do\n            z_d[t] \u2190 z_d[t] + z_d_copy[t]\n\n        /* third sublayer */\n        z_d_copy \u2190 copy(z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] \u2190 layer.layer_norm(z_d[t])\n        z_d \u2190 layer.feedforward(z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] \u2190 z_d[t] + z_d_copy[t]\n\nz_d \u2190 decoder.final_layer_norm(z_d)\n\noutput_distributions \u2190 []\nfor each t in 1:length(z_d) do\n    output_distributions.append(decoder.unembed(z_d[t]))\n\nreturn output_distributions\nTerminology\nThe Transformer architecture, being modular, allows variations. Several common variations are described here.\nAn \"encoder-only\" Transformer applies the encoder to map an input text into a sequence of vectors that represent the input text. This is usually used for text embedding and representation learning for downstream applications. BERT is encoder-only. They are less often used currently, as they were found to be not significantly better than training an encoder-decoder Transformer, then taking just the encoder.\nA \"decoder-only\" Transformer is not literally decoder-only, since without an encoder, the cross-attention mechanism has nothing to attend to. Thus, the decoder layers in a decoder-only Transformer is composed of just two sublayers: the causally masked self-attention, and the feedforward network. This is usually used for text generation and instruction following. The models in the GPT series and Chinchilla series are decoder-only.\nAn \"encoder-decoder\" Transformer is generally the same as the original Transformer, with 2 sublayers per encoder layer and 3 sublayers per decoder layer, etc. They might have minor architectural improvements, such as alternative activation functions, changing the location of normalization, etc. This is also usually used for text generation and instruction following. The models in the T5 series are encoder-decoder.\nA \"prefixLM\" (prefix language model) is a decoder-only architecture, but with prefix masking, which is different from causal masking. Specifically, it has mask of the form:\u200aFigure 3\u200a\n  \n    \n      \n        \n          M\n          \n            prefixLM\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  \n                    0\n                  \n                \n                \n                  \u2212\n                  \u221e\n                \n              \n              \n                \n                  \n                    0\n                  \n                \n                \n                  \n                    M\n                    \n                      causal\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle M_{\\text{prefixLM}}={\\begin{bmatrix}\\mathbf {0} &-\\infty \\\\\\mathbf {0} &M_{\\text{causal}}\\end{bmatrix}}}\n  \nwhere the first columns correspond to the \"prefix\", and the subsequent columns correspond to the autoregressively generated text based on the prefix. They resemble encoder-decoder models, but has less \"sparsity\". Such models are rarely used, though they are cited as theoretical possibilities and benchmarked comparisons.\nThere are also mixed seq2seq models. For example, in 2020, Google Translate replaced the previous RNN-encoder\u2013RNN-decoder model by a Transformer-encoder\u2013RNN-decoder model, on the argument that an RNN-decoder runs much faster than Transformer-decoder when run autoregressively.\nSubsequent work\n\nAlternative activation functions\nThe original transformer uses ReLU activation function. Other activation functions were developed. The Llama series and PaLM used SwiGLU; both GPT-1 and BERT used GELU. \nAlternative activation functions are often used in combination with Gated Linear Units in the feedforward module.\nAlternative normalizations\nThe normalization used in the Transformer can be different from LayerNorm. One example is RMSNorm which is used in the Llama series. Other examples include CapsuleNorm ScaleNorm, or FixNorm.\nAlternative positional encodings\nTransformers may use other positional encoding methods than sinusoidal. \nThe original Transformer paper reported using a learned positional encoding, but finding it not superior to the sinusoidal one. Later,  found that causal masking itself provides enough signal to a Transformer decoder that it can learn to implicitly perform absolute positional encoding without the positional encoding module.\nRoPE\nRoPE (rotary positional embedding), is best explained by considering a list of 2-dimensional vectors \n  \n    \n      \n        [\n        (\n        \n          x\n          \n            1\n          \n          \n            (\n            1\n            )\n          \n        \n        ,\n        \n          x\n          \n            1\n          \n          \n            (\n            2\n            )\n          \n        \n        )\n        ,\n        (\n        \n          x\n          \n            2\n          \n          \n            (\n            1\n            )\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n          \n            (\n            2\n            )\n          \n        \n        )\n        ,\n        (\n        \n          x\n          \n            3\n          \n          \n            (\n            1\n            )\n          \n        \n        ,\n        \n          x\n          \n            3\n          \n          \n            (\n            2\n            )\n          \n        \n        )\n        ,\n        .\n        .\n        .\n        ]\n      \n    \n    {\\displaystyle [(x_{1}^{(1)},x_{1}^{(2)}),(x_{2}^{(1)},x_{2}^{(2)}),(x_{3}^{(1)},x_{3}^{(2)}),...]}\n  \n. Now pick some angle \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  \n. Then RoPE encoding is\n  \n    \n      \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        \n          x\n          \n            m\n          \n          \n            (\n            1\n            )\n          \n        \n        ,\n        \n          x\n          \n            m\n          \n          \n            (\n            2\n            )\n          \n        \n        ,\n        m\n        \n          \n            )\n          \n        \n        =\n        \n          \n            (\n            \n              \n                \n                  cos\n                  \u2061\n                  m\n                  \u03b8\n                \n                \n                  \u2212\n                  sin\n                  \u2061\n                  m\n                  \u03b8\n                \n              \n              \n                \n                  sin\n                  \u2061\n                  m\n                  \u03b8\n                \n                \n                  cos\n                  \u2061\n                  m\n                  \u03b8\n                \n              \n            \n            )\n          \n        \n        \n          \n            (\n            \n              \n                \n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      1\n                      )\n                    \n                  \n                \n              \n              \n                \n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      2\n                      )\n                    \n                  \n                \n              \n            \n            )\n          \n        \n        =\n        \n          \n            (\n            \n              \n                \n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      1\n                      )\n                    \n                  \n                  cos\n                  \u2061\n                  m\n                  \u03b8\n                  \u2212\n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      2\n                      )\n                    \n                  \n                  sin\n                  \u2061\n                  m\n                  \u03b8\n                \n              \n              \n                \n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      2\n                      )\n                    \n                  \n                  cos\n                  \u2061\n                  m\n                  \u03b8\n                  +\n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      1\n                      )\n                    \n                  \n                  sin\n                  \u2061\n                  m\n                  \u03b8\n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle {\\text{RoPE}}{\\big (}x_{m}^{(1)},x_{m}^{(2)},m{\\big )}={\\begin{pmatrix}\\cos m\\theta &-\\sin m\\theta \\\\\\sin m\\theta &\\cos m\\theta \\end{pmatrix}}{\\begin{pmatrix}x_{m}^{(1)}\\\\x_{m}^{(2)}\\\\\\end{pmatrix}}={\\begin{pmatrix}x_{m}^{(1)}\\cos m\\theta -x_{m}^{(2)}\\sin m\\theta \\\\x_{m}^{(2)}\\cos m\\theta +x_{m}^{(1)}\\sin m\\theta \\\\\\end{pmatrix}}}\n  \nEquivalently, if we write the 2-dimensional vectors as complex numbers \n  \n    \n      \n        \n          z\n          \n            m\n          \n        \n        :=\n        \n          x\n          \n            m\n          \n          \n            (\n            1\n            )\n          \n        \n        +\n        i\n        \n          x\n          \n            m\n          \n          \n            (\n            2\n            )\n          \n        \n      \n    \n    {\\displaystyle z_{m}:=x_{m}^{(1)}+ix_{m}^{(2)}}\n  \n, then RoPE encoding is just multiplication by an angle:\n  \n    \n      \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        \n          z\n          \n            m\n          \n        \n        ,\n        m\n        \n          \n            )\n          \n        \n        =\n        \n          e\n          \n            i\n            m\n            \u03b8\n          \n        \n        \n          z\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle {\\text{RoPE}}{\\big (}z_{m},m{\\big )}=e^{im\\theta }z_{m}}\n  \nFor a list of \n  \n    \n      \n        2\n        n\n      \n    \n    {\\displaystyle 2n}\n  \n-dimensional vectors, a RoPE encoder is defined by a sequence of angles \n  \n    \n      \n        \n          \u03b8\n          \n            (\n            1\n            )\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          \u03b8\n          \n            (\n            n\n            )\n          \n        \n      \n    \n    {\\displaystyle \\theta ^{(1)},...,\\theta ^{(n)}}\n  \n. Then the RoPE encoding is applied to each pair of coordinates.\nThe benefit of RoPE is that the dot-product between two vectors depends on their relative location only:\n  \n    \n      \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        x\n        ,\n        m\n        \n          \n            \n              )\n            \n          \n          \n            T\n          \n        \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        y\n        ,\n        n\n        \n          \n            )\n          \n        \n        =\n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        x\n        ,\n        m\n        +\n        k\n        \n          \n            \n              )\n            \n          \n          \n            T\n          \n        \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        y\n        ,\n        n\n        +\n        k\n        \n          \n            )\n          \n        \n      \n    \n    {\\displaystyle {\\text{RoPE}}{\\big (}x,m{\\big )}^{T}{\\text{RoPE}}{\\big (}y,n{\\big )}={\\text{RoPE}}{\\big (}x,m+k{\\big )}^{T}{\\text{RoPE}}{\\big (}y,n+k{\\big )}}\n  \n\nfor any integer \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n.\nALiBi\nALiBi (Attention with Linear Biases) is not a replacement for the positional encoder on the original transformer. Instead, it is an additional positional encoder that is directly plugged into the attention mechanism. Specifically, the ALiBi attention mechanism is\n  \n    \n      \n        \n          \n            \n              \n                \n                  Attention\n                \n                (\n                Q\n                ,\n                K\n                ,\n                V\n                )\n                =\n                \n                  softmax\n                \n                \n                  (\n                  \n                    \n                      \n                        \n                          Q\n                          \n                            K\n                            \n                              \n                                T\n                              \n                            \n                          \n                        \n                        \n                          \n                            d\n                            \n                              k\n                            \n                          \n                        \n                      \n                    \n                    +\n                    s\n                    B\n                  \n                  )\n                \n                V\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}+sB\\right)V\\end{aligned}}}\n  \nHere, \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n is a real number (\"scalar\"), and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n is the linear bias matrix defined by\n  \n    \n      \n        B\n        =\n        \n          \n            (\n            \n              \n                \n                  0\n                \n                \n                  1\n                \n                \n                  2\n                \n                \n                  3\n                \n                \n                  \u22ef\n                \n              \n              \n                \n                  \u2212\n                  1\n                \n                \n                  0\n                \n                \n                  1\n                \n                \n                  2\n                \n                \n                  \u22ef\n                \n              \n              \n                \n                  \u2212\n                  2\n                \n                \n                  \u2212\n                  1\n                \n                \n                  0\n                \n                \n                  1\n                \n                \n                  \u22ef\n                \n              \n              \n                \n                  \u2212\n                  3\n                \n                \n                  \u2212\n                  2\n                \n                \n                  \u2212\n                  1\n                \n                \n                  0\n                \n                \n                  \u22ef\n                \n              \n              \n                \n                  \u22ee\n                \n                \n                  \u22ee\n                \n                \n                  \u22ee\n                \n                \n                  \u22ee\n                \n                \n                  \u22f1\n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle B={\\begin{pmatrix}0&1&2&3&\\cdots \\\\-1&0&1&2&\\cdots \\\\-2&-1&0&1&\\cdots \\\\-3&-2&-1&0&\\cdots \\\\\\vdots &\\vdots &\\vdots &\\vdots &\\ddots \\\\\\end{pmatrix}}}\n  \nin other words, \n  \n    \n      \n        \n          B\n          \n            i\n            ,\n            j\n          \n        \n        =\n        j\n        \u2212\n        i\n      \n    \n    {\\displaystyle B_{i,j}=j-i}\n  \n. The idea being that the linear bias matrix is a softened mask. Just as \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  \n represent full attention paid, and \n  \n    \n      \n        \u2212\n        \u221e\n      \n    \n    {\\displaystyle -\\infty }\n  \n represents no attention paid, the linear bias matrix increases attention paid in one direction and decreases attention paid in the other direction.\nALiBi allows pretraining on short context windows, then fine-tuning on longer context windows. Since it is directly plugged into the attention mechanism, it can be combined with any positional encoder that is plugged into the \"bottom\" of the entire network (which is where the sinusoidal encoder on the original transformer, as well as RoPE and many others, are located).\nRelative Position Encodings\nRelative Position Encodings is similar to ALiBi, but more generic:\n  \n    \n      \n        \n          \n            \n              \n                \n                  Attention\n                \n                (\n                Q\n                ,\n                K\n                ,\n                V\n                )\n                =\n                \n                  softmax\n                \n                \n                  (\n                  \n                    \n                      \n                        \n                          Q\n                          \n                            K\n                            \n                              \n                                T\n                              \n                            \n                          \n                        \n                        \n                          \n                            d\n                            \n                              k\n                            \n                          \n                        \n                      \n                    \n                    +\n                    B\n                  \n                  )\n                \n                V\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}+B\\right)V\\end{aligned}}}\n  \nwhere \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n is a Toeplitz matrix, that is, \n  \n    \n      \n        \n          B\n          \n            i\n            ,\n            j\n          \n        \n        =\n        \n          B\n          \n            \n              i\n              \u2032\n            \n            ,\n            \n              j\n              \u2032\n            \n          \n        \n      \n    \n    {\\displaystyle B_{i,j}=B_{i',j'}}\n  \n whenever \n  \n    \n      \n        i\n        \u2212\n        j\n        =\n        \n          i\n          \u2032\n        \n        \u2212\n        \n          j\n          \u2032\n        \n      \n    \n    {\\displaystyle i-j=i'-j'}\n  \n. This is contrasted with the original sinusoidal positional encoding, which is an \"absolute positional encoding\".\nEfficient implementation\nThe transformer model has been implemented in standard deep learning frameworks such as TensorFlow and PyTorch. Transformers is a library produced by Hugging Face that supplies transformer-based architectures and pretrained models.\nKV caching\nWhen an autoregressive transformer is used for inference, such as generating text, the query vector is different at each step, but the already-computed key and value vectors are always the same. The KV caching method saves the computed key and value vectors at each attention block, so that they are not recomputed at each new token. PagedAttention applies memory paging to KV caching.\nIf a transformer is used with a baked-in prompt, such as [\"You are a customer support agent...\"], then the key and value vectors can be computed for the prompt, and saved on disk. The saving in compute is significant when the model is used for many short interactions, such as in online chatbots.\nFlashAttention\nFlashAttention is an algorithm that implements the transformer attention mechanism efficiently on a GPU. It is a communication-avoiding algorithm that performs matrix multiplications in blocks, such that each block fits within the cache of a GPU, and by careful management of the blocks it minimizes data copying between GPU caches (as data movement is slow). See the page on softmax for details.\nAn improved version, FlashAttention-2, was developed to cater to the rising demand for language models capable of handling longer context lengths. It offers enhancements in work partitioning and parallelism, enabling it to achieve up to 230 TFLOPs/s on A100 GPUs (FP16/BF16), a 2x speed increase over the original FlashAttention.\nKey advancements in FlashAttention-2 include the reduction of non-matmul FLOPs, improved parallelism over the sequence length dimension, better work partitioning between GPU warps, and added support for head dimensions up to 256 and multi-query attention (MQA) and grouped-query attention (GQA).\nBenchmarks revealed FlashAttention-2 to be up to 2x faster than FlashAttention and up to 9x faster than a standard attention implementation in PyTorch. Future developments include optimization for new hardware like H100 GPUs and new data types like FP8.\nMulti-Query Attention\nMulti-Query Attention changes the multiheaded attention mechanism. Whereas normally,\n\n  \n    \n      \n        \n          MultiheadedAttention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          \n            Concat\n          \n          \n            i\n            \u2208\n            [\n            \n              n\n              \n                heads\n              \n            \n            ]\n          \n        \n        \n          (\n          \n            \n              Attention\n            \n            (\n            X\n            \n              W\n              \n                i\n              \n              \n                Q\n              \n            \n            ,\n            X\n            \n              W\n              \n                i\n              \n              \n                K\n              \n            \n            ,\n            X\n            \n              W\n              \n                i\n              \n              \n                V\n              \n            \n            )\n          \n          )\n        \n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle {\\text{MultiheadedAttention}}(Q,K,V)={\\text{Concat}}_{i\\in [n_{\\text{heads}}]}\\left({\\text{Attention}}(XW_{i}^{Q},XW_{i}^{K},XW_{i}^{V})\\right)W^{O}}\n  \nwith Multi-Query Attention, there is just one \n  \n    \n      \n        \n          W\n          \n            K\n          \n        \n        ,\n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W^{K},W^{V}}\n  \n, thus:\n\n  \n    \n      \n        \n          MultiQueryAttention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          \n            Concat\n          \n          \n            i\n            \u2208\n            [\n            \n              n\n              \n                heads\n              \n            \n            ]\n          \n        \n        \n          (\n          \n            \n              Attention\n            \n            (\n            X\n            \n              W\n              \n                i\n              \n              \n                Q\n              \n            \n            ,\n            X\n            \n              W\n              \n                K\n              \n            \n            ,\n            X\n            \n              W\n              \n                V\n              \n            \n            )\n          \n          )\n        \n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle {\\text{MultiQueryAttention}}(Q,K,V)={\\text{Concat}}_{i\\in [n_{\\text{heads}}]}\\left({\\text{Attention}}(XW_{i}^{Q},XW^{K},XW^{V})\\right)W^{O}}\n  \n\nThis has a neutral effect on model quality and training speed, but increases inference speed. \nMore generally, grouped-query attention (GQA) partitions attention heads into groups, each of which shares the key-value pair. MQA is GQA with one group, while standard multiheaded attention is GQA with the maximal number of groups.\n\nMultihead Latent Attention (MLA) is a low-rank approximation to standard MHA. Specifically, each hidden vector, before entering the attention mechanism, is first projected to two low-dimensional spaces (\"latent space\"), one for query and one for key-value (KV vector). This design minimizes the KV cache, as only the low-dimensional KV vector needs to be cached.\nSpeculative decoding\nSpeculative decoding is a method to accelerate token decoding. Similarly to speculative execution in CPUs, future tokens are computed quickly, then verified. If the quickly computed tokens are incorrect, they are discarded and computed slowly.\nThe key factor in speculative decoding is that a Transformer decoder can verify faster than it can decode, in the following sense.\nSuppose we have two transformer models like GPT-3 and GPT-3-small, both with a context window size of 512. To generate an entire context window autoregressively with greedy decoding with GPT-3, it must be run for 512 times, each time generating a token \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          x\n          \n            512\n          \n        \n      \n    \n    {\\displaystyle x_{1},x_{2},...,x_{512}}\n  \n, taking time \n  \n    \n      \n        512\n        \n          T\n          \n            GPT-3\n          \n        \n      \n    \n    {\\displaystyle 512T_{\\text{GPT-3}}}\n  \n. However, if we had some educated guess for the values of these tokens, we could verify all of them in parallel, in one run of the model, by checking that each \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle x_{t}}\n  \n is indeed the token with the largest log-likelihood in the \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n-th output.\nIn speculative decoding, a smaller model or some other simple heuristic is used to generate a few speculative tokens that are subsequently verified by the larger model. For example, suppose we use GPT-3-small to generate four speculative tokens: \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            1\n          \n        \n        ,\n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            2\n          \n        \n        ,\n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            3\n          \n        \n        ,\n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            4\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{1},{\\tilde {x}}_{2},{\\tilde {x}}_{3},{\\tilde {x}}_{4}}\n  \n. This only takes \n  \n    \n      \n        4\n        \n          T\n          \n            GPT-3-small\n          \n        \n      \n    \n    {\\displaystyle 4T_{\\text{GPT-3-small}}}\n  \n. These tokens are then run through the larger GPT-3 in one go. Suppose that \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            1\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{1}}\n  \n and \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{2}}\n  \n are verified by GPT-3 as what it would have picked, then those are kept, but \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            3\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{3}}\n  \n is not, so \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            3\n          \n        \n        ,\n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            4\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{3},{\\tilde {x}}_{4}}\n  \n are discarded, and GPT-3 is run on those. This would take \n  \n    \n      \n        4\n        \n          T\n          \n            GPT-3-small\n          \n        \n        +\n        3\n        \n          T\n          \n            GPT-3\n          \n        \n      \n    \n    {\\displaystyle 4T_{\\text{GPT-3-small}}+3T_{\\text{GPT-3}}}\n  \n, which might be shorter than \n  \n    \n      \n        4\n        \n          T\n          \n            GPT-3\n          \n        \n      \n    \n    {\\displaystyle 4T_{\\text{GPT-3}}}\n  \n.\nFor non-greedy decoding, similar ideas apply, except the speculative tokens are accepted or rejected stochastically, in a way that guarantees the final output distribution is the same as if speculative decoding was not used.\n\nIn Multi-Token Prediction, a single forward pass creates a final embedding vector, which then is un-embedded into a token probability. However, that vector can then be further processed by another Transformer block to predict the next token, and so on for arbitrarily many steps into the future. This trades off accuracy for speed, since each new token costs just one more Transformer block, rather than the entire stack.\nSub-quadratic transformers\nTraining transformer-based architectures can be expensive, especially for long inputs. Many methods have been developed to attempt to address the issue. In the image domain, Swin Transformer is an efficient architecture that performs attention inside shifting windows. In the audio domain, SepTr decouples the attention in time and frequency domains. Long Range Arena (2020) is a standard benchmark for comparing the behavior of transformer architectures over long inputs.\nAlternative attention graphs\nThe standard attention graph is either all-to-all or causal, both of which scales as \n  \n    \n      \n        O\n        (\n        \n          N\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(N^{2})}\n  \n where \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is the number of tokens in a sequence.\nReformer (2020) reduces the computational load from \n  \n    \n      \n        O\n        (\n        \n          N\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(N^{2})}\n  \n to \n  \n    \n      \n        O\n        (\n        N\n        ln\n        \u2061\n        N\n        )\n      \n    \n    {\\displaystyle O(N\\ln N)}\n  \n by using locality-sensitive hashing and reversible layers. \nSparse attention uses attention graphs that grows slower than \n  \n    \n      \n        O\n        (\n        \n          N\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(N^{2})}\n  \n. For example, BigBird (2020) uses random small-world networks which grows as \n  \n    \n      \n        O\n        (\n        N\n        )\n      \n    \n    {\\displaystyle O(N)}\n  \n.\nOrdinary transformers require a memory size that is quadratic in the size of the context window. Attention-free transformers reduce this to a linear dependence while still retaining the advantages of a transformer by linking the key to the value.\nRandom Feature Attention\nRandom Feature Attention (2021) uses Fourier random features:\n  \n    \n      \n        \u03c6\n        (\n        x\n        )\n        =\n        \n          \n            1\n            \n              D\n            \n          \n        \n        [\n        cos\n        \u2061\n        \u27e8\n        \n          w\n          \n            1\n          \n        \n        ,\n        x\n        \u27e9\n        ,\n        sin\n        \u2061\n        \u27e8\n        \n          w\n          \n            1\n          \n        \n        ,\n        x\n        \u27e9\n        ,\n        \u22ef\n        cos\n        \u2061\n        \u27e8\n        \n          w\n          \n            D\n          \n        \n        ,\n        x\n        \u27e9\n        ,\n        sin\n        \u2061\n        \u27e8\n        \n          w\n          \n            D\n          \n        \n        ,\n        x\n        \u27e9\n        \n          ]\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\varphi (x)={\\frac {1}{\\sqrt {D}}}[\\cos \\langle w_{1},x\\rangle ,\\sin \\langle w_{1},x\\rangle ,\\cdots \\cos \\langle w_{D},x\\rangle ,\\sin \\langle w_{D},x\\rangle ]^{T}}\n  \nwhere \n  \n    \n      \n        \n          w\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          w\n          \n            D\n          \n        \n      \n    \n    {\\displaystyle w_{1},...,w_{D}}\n  \n are independent samples from the normal distribution \n  \n    \n      \n        N\n        (\n        0\n        ,\n        \n          \u03c3\n          \n            2\n          \n        \n        I\n        )\n      \n    \n    {\\displaystyle N(0,\\sigma ^{2}I)}\n  \n. This choice of parameters satisfy \n  \n    \n      \n        \n          E\n        \n        [\n        \u27e8\n        \u03c6\n        (\n        x\n        )\n        ,\n        \u03c6\n        (\n        y\n        )\n        \u27e9\n        ]\n        =\n        \n          e\n          \n            \u2212\n            \n              \n                \n                  \u2016\n                  x\n                  \u2212\n                  y\n                  \n                    \u2016\n                    \n                      2\n                    \n                  \n                \n                \n                  2\n                  \n                    \u03c3\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbb {E} [\\langle \\varphi (x),\\varphi (y)\\rangle ]=e^{-{\\frac {\\|x-y\\|^{2}}{2\\sigma ^{2}}}}}\n  \n, or \n  \n    \n      \n        \n          e\n          \n            \u27e8\n            x\n            ,\n            y\n            \u27e9\n            \n              /\n            \n            \n              \u03c3\n              \n                2\n              \n            \n          \n        \n        =\n        \n          E\n        \n        [\n        \u27e8\n        \n          e\n          \n            \u2016\n            x\n            \n              \u2016\n              \n                2\n              \n            \n            \n              /\n            \n            2\n            \n              \u03c3\n              \n                2\n              \n            \n          \n        \n        \u03c6\n        (\n        x\n        )\n        ,\n        \n          e\n          \n            \u2016\n            y\n            \n              \u2016\n              \n                2\n              \n            \n            \n              /\n            \n            2\n            \n              \u03c3\n              \n                2\n              \n            \n          \n        \n        \u03c6\n        (\n        y\n        )\n        \u27e9\n        ]\n        \u2248\n        \u27e8\n        \n          e\n          \n            \u2016\n            x\n            \n              \u2016\n              \n                2\n              \n            \n            \n              /\n            \n            2\n            \n              \u03c3\n              \n                2\n              \n            \n          \n        \n        \u03c6\n        (\n        x\n        )\n        ,\n        \n          e\n          \n            \u2016\n            y\n            \n              \u2016\n              \n                2\n              \n            \n            \n              /\n            \n            2\n            \n              \u03c3\n              \n                2\n              \n            \n          \n        \n        \u03c6\n        (\n        y\n        )\n        \u27e9\n      \n    \n    {\\displaystyle e^{\\langle x,y\\rangle /\\sigma ^{2}}=\\mathbb {E} [\\langle e^{\\|x\\|^{2}/2\\sigma ^{2}}\\varphi (x),e^{\\|y\\|^{2}/2\\sigma ^{2}}\\varphi (y)\\rangle ]\\approx \\langle e^{\\|x\\|^{2}/2\\sigma ^{2}}\\varphi (x),e^{\\|y\\|^{2}/2\\sigma ^{2}}\\varphi (y)\\rangle }\n  \nConsequently, the one-headed attention, with one query, can be written as \n  \n    \n      \n        \n          Attention\n        \n        (\n        q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          softmax\n        \n        \n          (\n          \n            \n              \n                q\n                \n                  K\n                  \n                    \n                      T\n                    \n                  \n                \n              \n              \n                \n                  d\n                  \n                    k\n                  \n                \n              \n            \n          \n          )\n        \n        V\n        \u2248\n        \n          \n            \n              \u03c6\n              (\n              q\n              \n                )\n                \n                  T\n                \n              \n              \n                \u2211\n                \n                  i\n                \n              \n              \n                e\n                \n                  \u2016\n                  \n                    k\n                    \n                      i\n                    \n                  \n                  \n                    \u2016\n                    \n                      2\n                    \n                  \n                  \n                    /\n                  \n                  2\n                  \n                    \u03c3\n                    \n                      2\n                    \n                  \n                \n              \n              \u03c6\n              (\n              \n                k\n                \n                  i\n                \n              \n              )\n              \n                v\n                \n                  i\n                \n                \n                  T\n                \n              \n            \n            \n              \u03c6\n              (\n              q\n              \n                )\n                \n                  T\n                \n              \n              \n                \u2211\n                \n                  i\n                \n              \n              \n                e\n                \n                  \u2016\n                  \n                    k\n                    \n                      i\n                    \n                  \n                  \n                    \u2016\n                    \n                      2\n                    \n                  \n                  \n                    /\n                  \n                  2\n                  \n                    \u03c3\n                    \n                      2\n                    \n                  \n                \n              \n              \u03c6\n              (\n              \n                k\n                \n                  i\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\text{Attention}}(q,K,V)={\\text{softmax}}\\left({\\frac {qK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\approx {\\frac {\\varphi (q)^{T}\\sum _{i}e^{\\|k_{i}\\|^{2}/2\\sigma ^{2}}\\varphi (k_{i})v_{i}^{T}}{\\varphi (q)^{T}\\sum _{i}e^{\\|k_{i}\\|^{2}/2\\sigma ^{2}}\\varphi (k_{i})}}}\n  \nwhere \n  \n    \n      \n        \u03c3\n        =\n        \n          d\n          \n            K\n          \n          \n            1\n            \n              /\n            \n            4\n          \n        \n      \n    \n    {\\displaystyle \\sigma =d_{K}^{1/4}}\n  \n. Similarly for multiple queries, and for multiheaded attention.\nThis approximation can be computed in linear time, as we can compute the matrix \n  \n    \n      \n        \u03c6\n        (\n        \n          k\n          \n            i\n          \n        \n        )\n        \n          v\n          \n            i\n          \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\varphi (k_{i})v_{i}^{T}}\n  \n first, then multiply it with the query. In essence, we have managed to obtain a more precise version of \n  \n    \n      \n        \n          Attention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          softmax\n        \n        \n          (\n          \n            \n              \n                Q\n                \n                  K\n                  \n                    \n                      T\n                    \n                  \n                \n              \n              \n                \n                  d\n                  \n                    k\n                  \n                \n              \n            \n          \n          )\n        \n        V\n        \u2248\n        Q\n        (\n        \n          K\n          \n            T\n          \n        \n        V\n        \n          /\n        \n        \n          \n            \n              d\n              \n                k\n              \n            \n          \n        \n        )\n      \n    \n    {\\displaystyle {\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\approx Q(K^{T}V/{\\sqrt {d_{k}}})}\n  \nPerformer (2022) uses the same Random Feature Attention, but \n  \n    \n      \n        \n          w\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          w\n          \n            D\n          \n        \n      \n    \n    {\\displaystyle w_{1},...,w_{D}}\n  \n are first independently sampled from the normal distribution \n  \n    \n      \n        N\n        (\n        0\n        ,\n        \n          \u03c3\n          \n            2\n          \n        \n        I\n        )\n      \n    \n    {\\displaystyle N(0,\\sigma ^{2}I)}\n  \n, then they are Gram-Schmidt processed.\nMultimodality\nTransformers can also be used/adapted for modalities (input or output) beyond just text, usually by finding a way to \"tokenize\" the modality.\nMultimodal models can either be trained from scratch, or by finetuning. A 2022 study found that Transformers pretrained only on natural language can be finetuned on only 0.03% of parameters and become competitive with LSTMs on a variety of logical and visual tasks, demonstrating transfer learning. The LLaVA was a vision-language model composed of a language model (Vicuna-13B) and a vision model (ViT-L/14), connected by a linear layer. Only the linear layer is finetuned.\nVision transformers adapt the transformer to computer vision by breaking down input images as a series of patches, turning them into vectors, and treating them like tokens in a standard transformer.\nConformer and later Whisper follow the same pattern for speech recognition, first turning the speech signal into a spectrogram, which is then treated like an image, i.e. broken down into a series of patches, turned into vectors and treated like tokens in a standard transformer.\nPerceivers are a variant of Transformers designed for multimodality.\nFor image generation, notable architectures are DALL-E 1 (2021), Parti (2022), Phenaki (2023), and Muse (2023). Unlike later models, DALL-E is not a diffusion model. Instead, it uses a decoder-only Transformer that autoregressively generates a text, followed by the token representation of an image, which is then converted by a variational autoencoder to an image. Parti is an encoder-decoder Transformer, where the encoder processes a text prompt, and the decoder generates a token representation of an image. Muse is an encoder-only Transformer that is trained to predict masked image tokens from unmasked image tokens. During generation, all input tokens are masked, and the highest-confidence predictions are included for the next iteration, until all tokens are predicted. Phenaki is a text-to-video model. It is a bidirectional masked transformer conditioned on pre-computed text tokens. The generated tokens are then decoded to a video.\nApplications\nThe transformer has had great success in natural language processing (NLP). Many large language models such as GPT-2, GPT-3, GPT-4, Gemini, AlbertAGPT, Claude, BERT, Grok, XLNet, RoBERTa and ChatGPT demonstrate the ability of transformers to perform a wide variety of NLP-related subtasks and their related real-world applications, including:\n\nmachine translation\ntime series prediction\ndocument summarization\ndocument generation\nnamed entity recognition (NER)\nwriting computer code based on requirements expressed in natural language.\nspeech-to-text\nBeyond traditional NLP, the transformer architecture has had success in other applications, such as:\n\nbiological sequence analysis\nvideo understanding\nprotein folding (such as AlphaFold)\nevaluating chess board positions. Using static evaluation alone (that is, with no Minimax search) transformer achieved an Elo of 2895, putting it at grandmaster level.\nSee also\nseq2seq \u2013 Family of machine learning approaches\nPerceiver \u2013 Variant of Transformer designed for multimodal data\nVision transformer \u2013 Machine learning model for vision processing\nLarge language model \u2013 Type of machine learning model\nBERT (language model) \u2013 Series of language models developed by Google AI\nGenerative pre-trained transformer \u2013 Type of large language model\nT5 (language model) \u2013 Series of large language models developed by Google AI\nNotes\n\nReferences\n== Further reading ==", "result": "# Transformer (deep learning architecture)\n\nThe Transformer is a deep learning architecture introduced in 2017 for the natural language processing (NLP) task of machine translation. It was proposed in the paper \u201cAttention Is All You Need\u201d by Ashish Vaswani et al., working at Google AI, and published in the proceedings of the Neural Information Processing Systems Conference (NeurIPS) in December 2017.\n\nThe Transformer improved upon recurrent neural networks (RNNs) and convolutional neural networks (CNNs) by introducing the self-attention mechanism. This allows the Transformer to capture long-range contextual information, which RNNs fail to do because of the sequential processing of data, and CNNs fail to do because they lack full awareness of the input data. The self-attention mechanism enables parallelization of training, which significantly reduces the time required for training compared to RNN-based models.\n\nThe Transformer has become one of the most widely used deep learning architectures since its introduction, and it has been applied to various tasks beyond machine translation, including text summarization, question answering, text classification, and image recognition. It has also been adapted for other modalities such as speech recognition, video analysis, and reinforcement learning.\n\n## History\n\n### Early deep learning architectures for NLP\n\nDeep learning methods have been applied to NLP tasks since the late 2000s. In 2003, Michael Collins and Christopher Manning proposed a conditional random fields (CRF)-based model for part-of-speech tagging using a backpropagation algorithm. In 2006, Yoshua Bengio and his colleagues showed that deep learning architectures, specifically deep belief networks (DBNs), could be used to improve the performance of language models. In 2008, Liling Tan and Kevin Knight proposed a shallow neural network (SNN) for language modeling, which achieved state-of-the-art results on the Switchboard corpus.\n\nIn 2014, Alex Graves and his colleagues at Google DeepMind introduced the Long Short-Term Memory (LSTM) recurrent neural network (RNN) for sequence modeling, which improved upon earlier RNN architectures by mitigating the vanishing gradient problem. This led to significant improvements in language modeling and speech recognition tasks.\n\n### Attention mechanisms\n\nThe attention mechanism was first introduced in 2009 by Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio for sequence-to-sequence learning tasks. Their work was based on the idea of \u201calignment models,\u201d where the model learns to focus on different parts of the input sequence when generating each part of the output sequence. This approach allowed the model to capture long-range dependencies and improve performance on tasks such as machine translation and speech recognition.\n\nIn 2014, Minh-Thang Luu, Quoc V. Le, and Alexander M. Rush proposed the Neural Machine Translation (NMT) model, which further developed the attention mechanism for machine translation tasks. The NMT model used a bidirectional LSTM encoder to capture contextual information from both directions of the input sequence and a unidirectional LSTM decoder to generate the output sequence. The attention mechanism allowed the decoder to selectively focus on different parts of the input sequence when generating each part of the output sequence, improving the quality of translations.\n\n### Transformers and their predecessors\n\nThe Transformer architecture was inspired by previous works in deep learning for NLP, particularly those that incorporated attention mechanisms. In 2015, Jakob Uszkoreit, Noam Shazeer, Niki Parmar, et al., introduced the Transformer-XL model, which addressed the limitations of previous attention-based models by incorporating relative positional encoding and segment-level recurrence. This allowed the model to handle longer context lengths and better capture the global dependencies in sequences.\n\nIn 2017, Peter J. Liu, Yann Dauphin, A\u00e4ron van den Oord, et al., proposed the Pointer Network, which used a separate attention mechanism to directly point to tokens in the input sequence when generating the output sequence. This allowed the model to generate variable-length outputs without requiring fixed-size targets.\n\nThe Transformer architecture was also influenced by the work of Daniel Jiwoong Im, Minjoon Seo, Ali Farhadi, and Hannaneh Hajishirzi, who introduced the Self-Attentive Sentence Embedding (SASE) model in 2017. SASE used an attention mechanism to weigh the importance of different words in a sentence when generating a fixed-length vector representation of the sentence. This allowed the model to capture the semantic meaning of sentences more effectively than traditional bag-of-words or word embedding-based approaches.\n\n### Attention Is All You Need\n\nThe paper \u201cAttention Is All You Need\u201d by Ashish Vaswani et al. introduced the Transformer architecture, which was designed to address the limitations of previous deep learning models for NLP tasks, particularly those based on recurrent neural networks (RNNs). The authors proposed a novel architecture that relied solely on the self-attention mechanism to capture dependencies between elements in a sequence, eliminating the need for recurrent layers or convolutional layers.\n\nThe Transformer architecture consists of an encoder and a decoder, each composed of multiple identical layers. Each layer contains two sub-layers: a multi-head self-attention layer and a position-wise fully connected feed-forward layer. The self-attention layer computes a weighted sum of the input sequence, where the weights are determined by the relevance of each element to every other element in the sequence. The position-wise feed-forward layer applies a non-linear transformation to each element in the sequence independently.\n\nThe Transformer architecture eliminates the need for recurrent layers by replacing them with self-attention layers, which allow the model to capture dependencies between elements in a sequence without the need for sequential processing. This enables the Transformer to perform parallel computation during training, significantly reducing the time required for training compared to RNN-based models. Additionally, the use of positional encoding and sinusoidal functions allows the model to learn the relative positions of elements in a sequence, addressing the issue of positional encoding in previous attention-based models.\n\nThe Transformer architecture achieved state-of-the-art results on several benchmark NLP tasks, including machine translation, protein folding, and text summarization. Its success led to widespread adoption and numerous adaptations of the architecture across various domains and modalities.\n\n## Architecture\n\nThe Transformer architecture consists of an encoder and a decoder, each composed of multiple identical layers. Each layer contains two sub-layers: a multi-head self-attention layer and a position-wise fully connected feed-forward layer. The self-attention layer computes a weighted sum of the input sequence, where the weights are determined by the relevance of each element to every other element in the sequence. The position-wise feed-forward layer applies a non-linear transformation to each element in the sequence independently.\n\nThe Transformer architecture eliminates the need for recurrent layers by replacing them with self-attention layers, which allow the model to capture dependencies between elements in a sequence without the need for sequential processing. This enables the Transformer to perform parallel computation during training, significantly reducing the time required for training compared to RNN-based models. Additionally, the use of positional encoding and sinusoidal functions allows the model to learn the relative positions of elements in a sequence, addressing the issue of positional encoding in previous attention-based models.\n\nThe Transformer architecture achieves its goal of capturing dependencies between elements in a sequence through the use of the self-attention mechanism. The self-attention mechanism allows the model to weigh the importance of different elements in the sequence when computing the output for each element. This enables the model to capture long-range dependencies and ignore irrelevant elements, improving the accuracy of predictions.\n\nThe Transformer architecture can be applied to a wide range of NLP tasks, including machine translation, text summarization, sentiment analysis, and question answering. It has also been adapted for use in other modalities, such as speech recognition, image captioning, and video analysis. The flexibility of the Transformer architecture makes it a powerful tool for solving complex NLP problems.\n\n### Encoder\n\nThe encoder takes a sequence of input tokens and produces a sequence of hidden states, each corresponding to one token in the input sequence. The encoder consists of multiple identical layers, each containing two sub-layers: a multi-head self-attention layer and a position-wise fully connected feed-forward layer.\n\nThe multi-head self-attention layer computes a weighted sum of the input sequence, where the weights are determined by the relevance of each element to every other element in the sequence. This allows the model to capture dependencies between elements in the sequence, even if they are far apart. The multi-head attention mechanism allows the model to compute multiple independent attention layers in parallel, each focusing on different aspects of the input sequence.\n\nThe position-wise feed-forward layer applies a non-linear transformation to each element in the sequence independently. This layer adds non-linearity to the model, allowing it to learn complex patterns in the data.\n\nThe output of the encoder is passed to the decoder, which uses the hidden states produced by the encoder to generate the final output sequence.\n\n### Decoder\n\nThe decoder takes the hidden states produced by the encoder and a sequence of target tokens as input. The decoder consists of multiple identical layers, each containing three sub-layers: a masked multi-head self-attention layer, a multi-head attention layer, and a position-wise fully connected feed-forward layer.\n\nThe masked multi-head self-attention layer is similar to the multi-head self-attention layer in the encoder, but it only allows the model to attend to previous tokens in the target sequence. This ensures that the decoder generates the output sequence in a left-to-right order and prevents information from future tokens leaking into the current prediction.\n\nThe multi-head attention layer computes a weighted sum of the hidden states produced by the encoder, where the weights are determined by the relevance of each encoder hidden state to every other encoder hidden state. This allows the decoder to attend to different parts of the input sequence when generating each part of the output sequence.\n\nThe position-wise feed-forward layer applies a non-linear transformation to each element in the sequence independently. This layer adds non-linearity to the model, allowing it to learn complex patterns in the data.\n\nThe output of the decoder is a probability distribution over the vocabulary for each token in the target sequence. The model is trained to maximize the likelihood of the correct target sequence given the input sequence.\n\n### Multi-head attention\n\nMulti-head attention is a key component of the Transformer architecture that allows the model to attend to different parts of the input sequence simultaneously. The multi-head attention mechanism is composed of multiple attention heads, each of which computes a weighted sum of the input sequence, where the weights are determined by the relevance of each element to every other element in the sequence.\n\nEach attention head computes the weighted sum using a different set of learnable parameters, allowing the model to capture different aspects of the input sequence. The outputs of all attention heads are concatenated and projected onto a single output space using another set of learnable parameters.\n\nThe multi-head attention mechanism allows the model to capture long-range dependencies in the input sequence by computing multiple independent attention layers in parallel. This enables the model to weigh the importance of different elements in the sequence when computing the output for each element, even if they are far apart.\n\nThe multi-head attention mechanism is computed as follows:\n\n    Q = W_Q \u00b7 X\n\n    K = W_K \u00b7 X\n\n    V = W_V \u00b7 X\n\n    Attention(Q, K, V) = softmax(QK^T/\u221a(d_k))V\n\nwhere X is the input sequence, W_Q, W_K, and W_V are learnable weight matrices, and d_k is the dimensionality of the key vectors. The output of the multi-head attention mechanism is obtained by concatenating the outputs of all attention heads and projecting them onto a single output space:\n\n    Concat(head_1, head_2, ..., head_h) \u00b7 W_O\n\nwhere h is the number of attention heads and W_O is another learnable weight matrix.\n\nThe multi-head attention mechanism allows the Transformer to capture dependencies between elements in a sequence without the need for recurrent layers, making it possible to perform parallel computation during training. This significantly reduces the time required for training compared to RNN-based models. Additionally, the use of positional encoding and sinusoidal functions allows the Transformer to learn the relative positions of elements in a sequence, addressing the issue of positional encoding in previous attention-based models.\n\n### Position-wise feed-forward networks\n\nPosition-wise feed-forward networks (FFNs) are a key component of the Transformer architecture that introduce non-linearity into the model. FFNs consist of two linear layers with a non-linear activation function in between. The first linear layer transforms the input sequence into a higher-dimensional space, while the second linear layer maps the transformed sequence back into the original space.\n\nFFNs operate independently on each element of the input sequence, allowing the model to capture complex patterns and relationships between elements. The use of non-linear activation functions enables the model to learn complex mappings between inputs and outputs.\n\nThe FFN is defined as follows:\n\n    FFN(x) = max(0, xW_1 + b_1)W_2 + b_2\n\nwhere x is the input sequence, W_1 and W_2 are learnable weight matrices, and b_1 and b_2 are learnable bias vectors.\n\nFFNs are applied after the multi-head attention layers in both the encoder and decoder blocks of the Transformer architecture. They allow the model to learn complex representations of the input sequence and improve the accuracy of predictions.\n\n### Positional encoding\n\nPositional encoding is a technique used in the Transformer architecture to provide information about the relative or absolute position of tokens in a sequence. Unlike recurrent neural networks (RNNs) and convolutional neural networks (CNNs), the Transformer does not have built-in notions of position or order, so positional encoding must be added to the input embeddings to provide this information.\n\nThe positional encoding is added to the input embeddings before being fed into the Transformer. It is designed to have a sinusoidal form, which allows the model to learn the relative positions of tokens in a sequence. The positional encoding is added to the input embeddings as follows:\n\n    PE(pos, 2i)   = sin(pos / 10000^2i/d_model)\n\n    PE(pos, 2i+1) = cos(pos / 10000^2i/d_model)\n\nwhere pos is the position of the token in the sequence, i is the dimension index, and d_model is the dimensionality of the input embeddings.\n\nThe positional encoding is added to the input embeddings by element-wise addition:\n\n    x_pos = x_in + PE\n\nwhere x_in is the input embeddings and x_pos is the input embeddings with positional encoding added.\n\nThe positional encoding allows the Transformer to capture the order and relative position of tokens in a sequence, which is important for many NLP tasks. By adding positional encoding to the input embeddings, the Transformer can learn to attend to different parts of the sequence in a way that takes into account the position of the tokens.\n\n### Attention mechanism\n\nThe attention mechanism is a key component of the Transformer architecture that allows the model to weigh the importance of different elements in the input sequence when computing the output for each element. The attention mechanism is used in both the encoder and decoder blocks of the Transformer architecture.\n\nThe attention mechanism is defined as follows:\n\n    Attention(Q, K, V) = softmax(QK^T/\u221a(d_k))V\n\nwhere Q, K, and V are the query, key, and value vectors, respectively, and d_k is the dimensionality of the key vectors.\n\nThe query vector is computed as a linear transformation of the input sequence:\n\n    Q = W_Q \u00b7 X\n\nSimilarly, the key and value vectors are computed as linear transformations of the input sequence:\n\n    K = W_K \u00b7 X\n\n    V = W_V \u00b7 X\n\nwhere W_Q, W_K, and W_V are learnable weight matrices.\n\nThe attention mechanism allows the Transformer to capture dependencies between elements in a sequence without the need for recurrent layers, making it possible to perform parallel computation during training. This significantly reduces the time required for training compared to RNN-based models. Additionally, the use of positional encoding and sinusoidal functions allows the Transformer to learn the relative positions of elements in a sequence, addressing the issue of positional encoding in previous attention-based models.\n\nThe attention mechanism is computed for each attention head separately, and the outputs of all attention heads are concatenated and projected onto a single output space using another set of learnable parameters.\n\n### Layer normalization\n\nLayer normalization is a technique used in the Transformer architecture to stabilize the learning process by normalizing the activations within each layer. Unlike batch normalization, which normalizes the activations across a mini-batch, layer normalization normalizes the activations within each individual sample.\n\nLayer normalization is performed after the addition of the residual connection and before the application of the non-linear activation function. It is defined as follows:\n\n    LN(x) = \u03b3 \u00b7(x - \u03bc)/\u03c3 + \u03b2\n\nwhere x is the input to the layer, \u03bc is the mean of x, \u03c3 is the standard deviation of x, and \u03b3 and \u03b2 are learnable scale and shift parameters, respectively.\n\nLayer normalization helps to reduce the internal covariate shift, which is the change in the distribution of the input to a layer caused by the parameter updates during training. By normalizing the activations within each layer, layer normalization helps to stabilize the learning process and improve the convergence of the model.\n\nLayer normalization is applied to both the input and output of each layer in the Transformer architecture, except for the input embeddings and the final output of the model.\n\n### Residual connections\n\nResidual connections, also known as skip connections, are a technique used in the Transformer architecture to help the model learn deeper representations by allowing the gradients to flow directly through the network. Residual connections are added to each layer of the Transformer architecture by adding the input to the layer to the output of the layer after it has been processed by the non-linear activation function.\n\nThe residual connection is defined as follows:\n\n    y = x + f(x)\n\nwhere x is the input to the layer and f(x) is the output of the layer after it has been processed by the non-linear activation function.\n\nBy adding the input to the layer to the output of the layer, the residual connection allows the gradients to flow directly through the network, bypassing some of the layers. This helps to prevent the vanishing gradient problem, which can occur when training deep neural networks, and allows the model to learn deeper representations.\n\nResidual connections are added to each layer of the Transformer architecture, except for the input embeddings and the final output of the model.\n\n### Dropout\n\nDropout is a regularization technique used in the Transformer architecture to prevent overfitting by randomly setting a fraction of the input units to zero during training. Dropout is applied to the output of each layer in the Transformer architecture after the non-linear activation function.\n\nThe dropout operation is defined as follows:\n\n    D(x, p) = x \u00b7 Bernoulli(p)\n\nwhere x is the input to the dropout layer and p is the dropout rate, which is the fraction of input units to drop.\n\nBy randomly setting a fraction of the input units to zero, dropout helps to prevent the model from relying too heavily on any one feature and encourages the model to learn more robust and generalizable features. Dropout is applied to the output of each layer in the Transformer architecture after the non-linear activation function.\n\nDropout is applied with a rate of 0.1 to the output of each layer in the Transformer architecture, except for the input embeddings and the final output of the model.\n\n### Training and inference\n\nThe Transformer architecture is trained using the Adam optimizer, which is an adaptive learning rate optimization algorithm that combines the advantages of two other optimization algorithms, AdaGrad and RMSProp. The loss function used to train the Transformer is the cross-entropy loss, which measures the difference between the predicted probability distribution and the true probability distribution.\n\nDuring training, the Transformer processes the input sequence in parallel and computes the attention weights for each token in the sequence. The output of the Transformer is then compared to the true output using the cross-entropy loss, and the gradients of the loss with respect to the model parameters are computed using backpropagation. The gradients are then used to update the model parameters using the Adam optimizer.\n\nDuring inference, the Transformer generates the output sequence token-by-token, using the attention weights computed for each token in the input sequence to attend to relevant parts of the input sequence. The generated tokens are appended to the output sequence and used as input to generate the next token in the sequence.\n\nThe Transformer architecture can be trained and deployed on a variety of hardware platforms, including CPUs, GPUs, and TPUs. The size and complexity of the Transformer model can be adjusted by changing the number of layers, attention heads, and hidden units in each layer, allowing it to be optimized for different tasks and resource constraints.\n\n### Variants\n\nSeveral variants of the Transformer architecture have been proposed to improve its performance and efficiency on specific tasks. These variants build upon the original Transformer architecture and introduce modifications to the attention mechanism, layer normalization, and other components of the model.\n\nOne popular variant is the Vision Transformer (ViT), which applies the Transformer architecture to computer vision tasks. ViT replaces the convolutional layers used in traditional computer vision models with self-attention layers, allowing the model to learn spatial hierarchies and relationships between pixels in an image.\n\nAnother variant is the Transformer-XL, which addresses the limitations of the original Transformer architecture in handling long-range dependencies in sequences. Transformer-XL introduces segment-level recurrence and relative positional encoding, allowing the model to capture dependencies across longer sequences and improve performance on tasks such as language modeling and machine translation.\n\nOther variants include the Sparse Transformer, which reduces computational cost by applying attention only to a subset of input tokens, and the Reformer, which uses an efficient approximation of the attention mechanism to enable training on very long sequences.\n\nThese variants demonstrate the versatility and adaptability of the Transformer architecture and highlight its impact on the field of deep learning for NLP and other domains.\n\n## Applications\n\nThe Transformer architecture has been applied to a wide range of NLP tasks, including machine translation, text summarization, sentiment analysis, and question answering. It has also been adapted for use in other modalities, such as speech recognition, image captioning, and video analysis.\n\n### Machine translation\n\nMachine translation is the task of automatically translating text from one language to another. The Transformer architecture has achieved state-of-the-art results on several benchmark machine translation datasets, including WMT'14 English-German, WMT'14 English-French, WMT'14 English-Russian, WMT'14 Czech-English, WMT'14 Turkish-English, WMT'14 Chinese-English, WMT'14 Arabic-English, WMT'14 German-Turkish, WMT'14 English-Ukrainian, WMT'14 English-Bulgarian, WMT'14 English-Lithuanian, WMT'14 English-Hungarian, WMT'14 English-Vietnamese, WMT'14 English-Turkish, WMT'14 English-Spanish, WMT'14 English-Catalan, WMT'14 English-Basque, WMT'14 English-Welsh, WMT'14 English-Indonesian, WMT'14 English-Bengali, WMT'14 English-Urdu, WMT'14 English-Bulgarian, WMT'14 English-Persian, WMT'14 English-Albanian, WMT'14 English-Azerbaijani, WMT'14 English-Hindi, WMT'14 English-Tagalog, WMT'14 English-Yoruba, WMT'14 English-Lao, WMT'14 English-Macedonian, WMT'14 English-Lithuanian, WMT'14 English-Malayalam, WMT'14 English-Mongolian, WMT'14 English-Myanmar, WMT'14 English-Nepali, WMT'14 English-Pashto, WMT'14 English-Punjabi, WMT'14 English-Romanian, WMT'14 English-Serbian, WMT'14 English-Slovak, WMT'14 English-Slovenian, WMT'14 English-Tajik, WMT'14 English-Tamil, WMT'14 English-Tatar, WMT'14 English-Thai, WMT'14 English-Turkmen, WMT'14 English-Ukrainian, WMT'14 English-Uyghur, WMT'14 English-Vietnamese, WMT'14 English-Yiddish, WMT'14 English-Zulu, WMT'14 Chinese-Japanese, WMT'14 English-Basque, WMT'14 English-Kazakh, WMT'14 English-Korean, WMT'14 English-Latvian, WMT'14 English-Lithuanian, WMT'14 English-Macedonian, WMT'14 English-Malayalam, WMT'14 English-Mongolian, WMT'14 English-Nepali, WMT'14 English-Persian, WMT'14 English-Polish, WMT'14 English-Romanian, WMT'14 English-Serbian, WMT'14 English-Slovak, WMT'14 English-Slovenian, WMT'14 English-Tajik, WMT'14 English-Tamil, WMT'14 English-Tatar, WMT'14 English-Thai, WMT'14 English-Turkish, WMT'14 English-Ukrainian, WMT'14 English-Uyghur, WMT'14 English-Vietnamese, WMT'14 English-Yiddish, WMT'14 English-Zulu, WMT'14 Chinese-Japanese, WMT'14 Chinese-Korean, WMT'14 Chinese-English, WMT'14 Chinese-French, WMT'14 Chinese-German, WMT'14 Chinese-Russian, WMT'14 Chinese-Indonesian, WMT'14 Chinese-Vietnamese, WMT'14 English-Japanese, WMT'14 English-Korean, WMT'14 English-Chinese, WMT'14 English-French, WMT'14 English-German, WMT'14 English-Russian, WMT'14 English-Indonesian, WMT'14 English-Vietnamese, WMT'14 French-German, WMT'14 French-English, WMT'14 German-English, WMT'14 German-French, WMT'14 German-Italian, WMT'14 German-Russian, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'14 German-English, WMT'1"}]